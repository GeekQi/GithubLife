注：为个人学习总结需要，如有侵权，请您指出。[持续更新， 如有错误，请您指出]        

​    

 ##                      **「礼貌的笑了笑，寸步不让」** 

-----

关于「学术论文／研究项目／工程项目／创业项目」：

### 关于「专业书」：

### 关于「练手到熟练项目」：1%

Java基础：参考：

##### **Web项目:**

- Python 爬虫：参考：[Scrapy](http://wiki.jikexueyuan.com/project/scrapy/)、


- Java Web：参考：[Java Web](http://wiki.jikexueyuan.com/project/java-web/)、
- PHP+MySQL搭建网页：参考：[PHP+MySQL](http://wiki.jikexueyuan.com/project/php-and-mysql-web/) 、

##### **数据挖掘／自然语言处理项目：**

- 建模

##### **算法基础：**

- 数据结构：参考：[剑指Offer](http://wiki.jikexueyuan.com/project/for-offer/) 、[LeetCode](http://wiki.jikexueyuan.com/project/leetcode-book/)
- 算法：参考：

##### **大数据项目：**

- Spark、Scala

### 关于「论文」：1%

数量：

##### [基于关键词及问题主题的问题相似度计算](http://chuansong.me/n/1906250351625) ：

- 社区问答系统、判断问题相似度、推荐问题的答案、避免重复提问。
- 问题包括：问题主题及问题描述
- KT-CNN模型包括：
  - 关键词抽取：
    - 对输入问题S和T,先预处理在通过此模块抽取S和T的关键词序列Ks和Kt
      - **需要：**得分排序序列、TextRank算法、无向有全图、图排序以及选取关键词、TF-IDF
  - 基于关键词相似相异的问题建模:
    - 利用Ks和Kt间相似相异信息，对S和T建模得到特征向量Fs和Ft
      - **需要**：基于文本间相似及相异信息的CNN模型、词向量表示、GloVe模型、语义匹配：相似矩阵、余弦相似度、皮尔森相关系数、矩阵分解、CNN模型卷积层、CNN模型最大池层、
  - 计算主题相似度:
    - 对问题S和T的主题Ts和Tt计算相似度Sim_topic
      - **需要**：向量表示、皮尔森相关系数
  - 问题相似度计算:
    - 基于问题S和T的特征向量Fs和Ft及主题相似度Sim_topic计算S和T的相似度Sim_q
      - **需要**：线性模型的加权相加

##### [人机对话系统中基于关键词的回复生成技术](http://chuansong.me/n/1873214451926)：

- 早期回复生成技术：基于规则：文本分析规则。／／ 序列决策问题：马尔可夫决策
- 回复生成问题：Seq2Seq深度学习框架：用户消息和回复被建模成两个序列，通过大规模训练数据训练模型学习两个序列间的映射关系。
  - 极大似然估计方法的交叉熵损失函数、最大互信息损失、随机变量
- 使用关键词增强生成回复的相关性
  - Seq2BF（sequence to backward and forward sequences）模型
  - 分为两阶段：根据用户消息在词表中选取和消息具有最大互信息的词作为关键词，根据关键词预测回复的剩余部分。
  - 简要介绍：Seq2Seq 框架下基本的回复生成模型：
    - 两部分之encoder：负责将消息编码成一个模型的内部表示
    - 两部分之decoder：在encoder输出条件下，从特定字符开始，逐字生成完整的回复内容。

##### [文本生成概述](http://chuansong.me/n/1851499951313)：

- 根据格式化数据或自然语言文本生成新闻等可解释文本、定义、任务、评价指标、实现方法、数据驱动方法
- 文本生成定义：接受非语言形式的信息作为输入，生成刻度的文字表示。数据到文本的生成。
- 文本生成任务：文本到文本、数据到文本、图像到文本
  - 文本到文本：
    - 文本摘要：
      - 抽取式摘要：信息抽取和规划等主要步骤
        - 主题模型、聚类、SVR(Support Vector Regression)、线性回归、抽取名词短语、动词短语、随机森林、
      - 生成式摘要
- 文本生成方法：基于规则、基于规划、数据驱动
  - 基于语言模型的自然语言生成：n-gram
  - 使用深度学习的自然语言生成

##### [事件演化的规律和模型](http://chuansong.me/n/1835019551922)：

- 事件图谱、图结构、马尔可夫逻辑网络（无向图）、贝叶斯网络（有向五环图）。事理图谱（有向有环图）。
- 事理图谱的定义：
  - 事件：用抽象、泛化的谓词短语来表示。：不关注时间、地点等
  - 事件间顺承关系：两个时间在时间上先后发生的关系。
  - 事件因果关系：满足顺承关系时序约束的基础上，两个事件件有很强的因果性，强调前因后果。
  - 事理图谱**Event Evolutionary Graph** ：描述事件之间顺承、因果关系的事理烟花逻辑有向图。以某个事件节点进行广度优先搜索，扩展得到事件演化链条。
    - 事理图谱是一种概率有向图。概率图模型的贝叶斯网络、马尔可夫逻辑网络、贝叶斯用有向无环图表达变量节点间的条件依赖与独立性关系。马尔可夫随机场采用无向图表达变量间的相互作用关系。

##### [阿里自然语言处理部总监：NLP技术的应用及思考](http://chuansong.me/n/1810576651425) ：

郎君博士，哈工大社会计算与信息检索研究中心博士毕业生，目前为阿里巴巴iDST自然语言处理部总监

- 计算平台、业务层、NLP4大经典AI完全难题：问答、复述、文摘、翻译。
- 阿里需要：技术体系及服务、核心业务快速增长、商业机会
  - 内容搜索、内容推荐、评价、问答、文摘、文本理解
  - 商品搜索、推荐、智能交互、翻译、广告、风控、舆情监控、
- 词法分析：分词、词性、实体：
  - Bi-LSTM-CRF、多领域词表
  - 推荐算法、蚂蚁金服、资讯搜索
- 句法分析：依存句法分析、成分句法分析：
  - Shift-reduce、graph-based、Bi-lSTM
  - 资讯搜索、评价情感分析、商品标题、搜索Query
- 情感分析：情感对象、属性、属性关联
  - 情感辞典挖掘、属性级、句子级、篇章级情感分析
  - 商品评价、问答、品牌舆情、互联网舆情
- 句子生成：句子可控改写、句子压缩
  - Beam Search、 Seq2Seq+Attention
  - 商品标题压缩、资讯标题改写、PUSH消息改写
- 句子相似度：浅层相似度、语义相似度
  - Edit Distance、Word2Vec、DSSM
  - 相似问题、商品重发检测
- 文本分类／聚类：垃圾防控、信息聚合
  - ME, SVM, FastText
  - 商品类目预测、问答意图分析、文本垃圾过滤、舆情聚类、名片OCR后语义识别
- 文本表示：词向量、句子向量、篇章向量、Seq2Seq
  - Word2Vec、LSTM、DSSM、Seq2Seq
- 知识库：电商同义词／上下位、通用同义词／上下位。领域词库、情感词库
  - bootstrapping、click-through mining、word2vec、k-means、CRF
  - 语义归一、语义扩展、Query理解、意图理解、情感分析
- 语料库：分词、词性标注数据、依存句法标注数据
- 标题分析：分词、实体打标、热度计算、中心识别
- 评价系统：treelink模型、maxent模型、贝叶斯模型、dbn模型总体融合
- 决策购买问题：产品化：分为四类：
  - 无效问题过滤：
    - 分类采用LR+GBDT：定制特征
  - 相似问题识别：
    - Doc2Vec 计算相似度、人工评测
  - 页面问答排序
    - 内容丰富度、点赞数、过滤词表匹配数等加权求和、CTR提升
  - 智能分发

##### [基于深度多任务学习的自然语言处理技术](http://chuansong.me/n/1768183851425) 

- 统计自然语言处理依赖标注数据
- 多任务学习Multi-task Learning：
  - 有监督学习Supervised Learning， 利用人工标注的训练数据进行学习
  - 归纳迁移机制，基本目标是提高泛化性能。利用并行训练的方法学习多个任务。 多任务学习的基本假设是多个任务之间具有相关性。
  - 词法、句法、语义分析等多任务，之间存在紧密的内在联系
- 深度多任务学习：
  - 深度学习：建立在含有多层非线性变换的神经网络结构之上，对数据进行抽象表示和学习的一系列机器学习算法。

##### [对话系统的Goal Oriented和Task Oriented 概念的异同](http://chuansong.me/n/1758705951350)

- 人机对话系统：
  - 开放域对话系统：闲聊：微软小冰。 百度问答：度秘。
  - 任务型对话系统：设备控制：Siri。公交线路查询、餐厅预订。

##### [自然语言处理中的知识获取](http://chuansong.me/n/1724644951614)

- 各个行业：教育、医疗、法律等知识服务型行业
- 自然语言中任何问题抽象为：如何从形式与意义的多对多映射中，根据语境选择一种正确的映射。
- 自然语言处理中知识获取的三要素：
  - 显性知识：
    - 元知识，如WordNet、知识图谱
  - 数据：
    - 带标注数据、无标注
  - 学习算法：
    - SVM、CRF等浅层学习模型，人工定义的特征模版抽取特征及特征组合，结合
    - RNN、CNN等深度学习模型，自动学习有效特征及特征组合的能力
- 大数据和深度学习相互依赖的：
  - 一方面大数据需要复杂的学习模型。长尾数据、复杂模型、大数据
  - 另一方面深度学习需要大数据。神经网络机器翻译（NMT）已经迅速超越统计机器翻译（SMT）、端到端
    - 信息抽取两种做法：
      - 1先做句法分析，再做信息抽取
      - 2直接信息抽取，也就是“端到端”，也是分层的

##### [迁移学习：基本概念到相关研究](http://chuansong.me/n/1716697451319)

- 什么是迁移学习：

  - 机器学习的监督学习场景中，如果针对任务和域A训练一个模型，我们假设被提供了此标签数据。
  - 迁移学习借用已存在的相关任务或域的标签数据来处理场景。尝试把源域中解决源任务时获得的知识存储下来，应用到目标域的目标任务中。

- 为什么需要迁移学习：

  - Andrew Ng：迁移学习将会是继监督学习之后的下一个机器学习商业成功驱动力。
  - ml在产业界的应用和成功主要：监督学习的驱动。
  - 无监督学习是实现通用人工智能的关键成分。
  - 产业界对ml的应用分2类：
    - 一方面：先进模型
    - 另一方面：大量标签数据使得模型成功
  - 迁移学习可以帮助我们处理全新的场景，是ml在没有大量标签任务和域中规模化应用所必须的。

- 迁移学习定义

  - 给定一个源域 Ds，一个对应的源任务 Ts，还有目标域 Dt，以及目标任务 Tt，现在，迁移学习的目的就是：在 Ds≠Dt，Ts≠Tt 的情况下，让我们在具备来源于 Ds 和 Ts 的信息时，学习得到目标域 Dt 中的条件概率分布 P（Yt|Xt）

- 迁移学习场景

  - 给定源域和目标域 Ds 和 Dt，其中，D={X,P(X)}，并且给定源任务和目标任务 Ts 和 Tt，其中 T={Y,P(Y|X)}。源和目标的情况可以以四种方式变化

- 迁移学习的应用

  - 从模拟中学习：google自动驾驶、机器人等

- 迁移学习方法

  - 使用预训练CNN特征
  - 理解卷积神经网络、全连接层


##### [如何让人工智能学会用数据说话](http://chuansong.me/n/1668814251914)

- 基于结构化数据的的文本生成
  - 机器翻译、文本摘要、诗词生成等都属于文本生成的范畴：
    - 共同点：用户输入非结构化文本，机器根据目标输出相应文本
- 结构化文本生成：特点：基于数据和事实
- 文本生成的典型商业应用：财经、体育类新闻报道的生成、产品描述、商业数据的分析和解释、物联网数据的分析。比如：天气预报自动生成。
- 文本生成的技术发展：
  - 对选定的数据记录，用自然语言描述出来
- 方法：
  - 早期：基于规则：三个独立的模块：
    - 内容规划(Content planning), 即选择描述那些数据记录或数据域
    - 句子规划(Sentence planning), 即决定所选择的数据记录或数据域在句子中的顺序
    - 句子实现(Surface realization), 即基于句子规划生成实际的文本。
  - 基于神经网络的方法：
    - 基于神经语言模型(Neural Language Model)
    - 基于神经机器翻译(Neural Machine Translation)
    - Semantic Controlled LSTM（Long Short-term Memory）模型:用于文本生成：在LSTM基础上引入了控制门读取结构化数据信息。
- 数据：
  - 天气预报、维基百科人物传记、基于对话的人机对话数据集

##### [卷积神经网络在句子分类上的应用](http://chuansong.me/n/1723633)

- 基于预先训练的词向量而训练的卷积神经网络(CNN-Convolutional Neural Networks)、文本分类、语义分析、词向量word vectors 
- 卷积神经网络CNN利用一个卷积层进行特征提取，最初CV，后来在nlp也得到应用，如语义分析、搜索短语检索、句子分类
- 特征向量使用max-over-time池化、word2vec词向量、

##### [自然语言中的Attention Model](http://chuansong.me/n/2215468)

- 先说Encoder-Decoder框架
  - AM模型基本附着在Encoder-Decoder框架下的。但本身并不依赖于Encoder-Decoder模型：适合处理由一个句子生成另一个句子的通用处理模型。
  - 具体使用什么模型自己定：CNN/RNN/BiRNN/GRU/LSTM/Deep LSTM
- AM：以上的还没有体现出“注意力模型”。

##### [深度学习：推动NLP领域发展的新引擎](http://chuansong.me/n/2793709)

- Word Embedding: word2vec能把词变成向量：忽略词之间的关系(句法关系)、词的顺序等
  - 引入词的关系：Dependency Parser：把抽取出来的Relation作为词的Context
  - 改进Bag of Words:
  - 外部资源和知识库：word2vec只用了词的上下文共现， 没有用外部资源如词典知识库
- RNN／LSTM／CNN
  - RNN相关的模型如LSTM基本上算法是解决结构化问题的标准。比普通的FeesForward Network, RNN 有记忆能力
  - 普通的神经网络只会在学习的时候“记忆”，也就是反向传播算法学习参数。然后不“记忆”了。训练好之后，不管什么时候来一个相同的输入，输出一样。对于image classification没问题，但是speech recognition等nlptask，数据有时序或者结构的。
  - RNN具有“记忆”能力。前一个输出会影响后面的判断。比如：前一个He,后面出现is的概率比are高很多。
  - 最简单RNN直接把前一个时间点的输出作为当前出入的一部分，有梯度消失的问题。比较流行的改进如LSTM和GRU等模型通过gate开关，判断是否需要遗忘／记忆之前的状态，以及当前状态是否需要输出到下个时间点。比如语言模型。
  - CNN最早图像。通过卷积发现位置无关的feature，而且这些feature的参数相同，
    - machine translation、语义角色标注Sematic Role labeling
- Multi-model deep learning
- Reasoning, Attention and Memory
  - RNN/LSTM是模拟人类大脑的记忆机制，但除了记忆之外，Attention也是非常有用的机制

##### [深度学习浪潮中的自然语言处理技术](http://chuansong.me/n/472336251048)

- 目前，机器学习技术为自然语言的歧义、动态性等提供了可行的解决方案，成为研究主流，称为统计自然语言处理。
- 一个统计自然语言处理系统通常由2部分构成：
  - 训练数据（样本）
  - 统计模型（算法）
- 但是传统的机器学习方法在数据获取和模型构建等方面存在严重问题：
  - 大规模标注数据难以获得，带来了严重的数据稀疏问题。
  - 需要人工设计模型所需的特征及特征组合。需要深刻理解和丰富经验
- 基于深度学习的自然语言处理
  - 建立在含有多层非线性变换的神将网络结构之上，对数据的表示进行抽象和学习的一系列机器学习算法。
  - 深度学习为自然语言处理的研究主要带来两方面变化：
    - 一方面使用统一的分布式(低维、稠密、连续)向量表示不同粒度的语言单元，如词、短语、篇章
    - 一方面使用循环、卷积、递归等神经网络模型对不同的语言单元向量进行组合，获得更大语言单元的表示
  - 分布式表示：
    - 深度学习最早在nlp的应用是神将网络语言模型：基本假设低维、稠密、连续的向量表示词汇，又称为分布式词表示(Distributed Word Representation)或词嵌入(Word Embedding)。 可以将相似的词汇表示为相似的向量。
    - 理论上，将原有高维、稀疏、离散的词汇表示方法（One-hot表示）映射为分布式表示是一种降维方法，可有效克服机器学习的维度灾难（Curse of Dimensionality）问题，从而获得更好的学习效果。
  - 语义组合(Semantic Composition)
    - 分布式词表示的思想可以进一步扩展，可通过组合（Composition）的方式来表示短语、句子甚至是篇章等更大粒度的语言单元。
    - 三种神经网络结构实现不同的组合方式
      - 循环神经网络（顺序组合）RNN，Recurrent Neural Network
        - 从左至右顺序的对句子中的单元进行两两组合：“我”和“喜欢”组合，生成隐层h1。将h1和“红”组合，生成h2.类推。传统的RNN存在严重梯度消失（Vanishing Gradient）或者梯度爆炸（Exploding Gradient）问题。
        - 深度学习中一些常用的技术，如使用ReLU激活函数、正则化以及恰当的初始化权重参数等都可以部分解决这一问题
        - 另一类更好的解决方案是减小网络的层数，以LSTM和GRU等为代表的带门循环神经网络（Gated RNN）都是这种思路，即通过对网络中门的控制，来强调或忘记某些输入，从而缩短了其前序输入到输出的网络层数，从而减小了由于层数较多而引起的梯度消失或者爆炸问题
      - 卷积神经网络（局部组合）CNN，Convolutional Neural Network
        - 隐含层神经元只与部分输入层神经元连接，同时不同隐含层神经元的局部连接权值共享。如评论文本分类，最终褒贬性由局部短语决定，且与顺序无关。
        - 由于存在局部接收域性质，各个隐含神经元的计算可以并行的进行，这就可以充分利用现代的硬件设备（如GPU），加速卷积神经网络的计算，这一点在循环神经网络中是较难实现的
      - 递归神经网络（句法结构组合）RecNN，Recursive Neural Network
        - 首先对句子进行语法分析，将结构转化为树状结构，构建深度神经网络。
  - 很多自然语言任务如对话生成，有赖于更大的上下文或语境，传统的基于人工定义特征的方式很难对其进行建模，深度学习模型则提供了一种对语境进行建模的有效方式 
  - 无论何种神经网络模型，都是基于固定的网络结构进行组合，传统的有监督学习框架很难实现该目标，而强化学习（Reinforcement Learning）框架为我们提供了一种自动学习动态网络结构的途径。

[基于深度学习的关系抽取](http://chuansong.me/n/831970551568)

- 信息抽取旨在从大规模非结构或半结构的自然语言文本中抽取结构化信息。关系抽取是其中的重要子任务之一，主要目的是从文本中识别实体并抽取实体之间的语义关系。
- 现有主流的关系抽取技术分为有监督的学习方法、半监督的学习方法和无监督的学习方法三种
  - 有监督的学习方法将关系抽取任务当做分类问题，根据训练数据设计有效的特征，从而学习各种分类模型，然后使用训练好的分类器预测关系。该方法的问题在于需要大量的人工标注训练语料，而语料标注工作通常非常耗时耗力
  - 半监督的学习方法主要采用Bootstrapping进行关系抽取。对于要抽取的关系，该方法首先手工设定若干种子实例，然后迭代地从数据从抽取关系对应的关系模板和更多的实例
  - 无监督学习方法假设拥有相同语义关系的实体对拥有相似的上下文信息。因此可以利用每个实体对对应上下文信息来代表该实体对的语义关系，并对所有实体对的语义关系进行聚类
  - 有监督的学习方法能够抽取更有效的特征，其准确率和召回率都更高。因此有监督的学习方法受到了越来越多学者的关注
- 基于有监督学习的关系抽取
  - 需要大量人工标注的训练数据，从村里安数据中自动学习关系对应的抽取模式。
  - 远程监督
- 基于深度学习的关系抽取
  - 有监督的依赖标注等分类特征，并且存在错误
  - 递归神经网络解决关系抽取问题
  - 卷积神经网络关系抽取
  - 基于端到端神经网络的关系抽取模型：双向LSTM(Long-Short-Term-Memory)
- 总结及未来趋势
  - 基于句法树的树形LSTM神经网络模型在关系抽取上取得了不错的效果
  - 目前的神经网络关系抽取主要用于预先设定好的关系集合。而面向开放领域的关系抽取，仍然是基于模板等比较传统的方法


##### [基于深度学习的依存句法分析](http://chuansong.me/n/710400151780)  

- 句法分析：句子从词语的序列形式按照语法体系转化为图结构（树结构）。以刻画句子内部的句法关系（主谓宾）。使用依存户连接句子中两个具有一定句法关系的词语，最终形成一颗句法依存树。
- 主流依存句法分析方法：
  - 基于图（Graph-based）
    - 将依存句法分析看成从完全有向图中寻找最大生成树的问题，图的两边表示两个词之间存在某种句法关系的可能性
  - 基于转移（Transition-based）
    - 通过规约等转移动作构建一棵依存句法树，学习目标是寻找最优动作序列
- 深度学习技术：
  - 建立在含有多层非线性变换的神经网络结构之上，对数据抽象和学习的一系列机器学习算法。
- 基于转移的依存句法分析方法：
  - 一系列由初始到终止的状态（State或Configuration）表示句法分析的过程。一个状态由栈（Stack）、缓存（Buffer)以及部分分析好的依存弧构成。栈存储已经分析的词，缓存表示待分析的词
  - 学习一个分类器，输入状态，输出该状态下最可能动作：贪心解码算法
  - 抽取出特征后，传统方法：采用线性分类器，即将特征进行线性加权组合，结合系数为分类器学习获得的权重，选取分数最高的类别作为采取的动作
- 基于深度学习的依存句法分析：
  - 贪心解码算法：先从一个状态提取一些重要核心特征，与传统高维、稀疏、离散向量One-hot表示不同，使用低维、稠密、连续的分布式向量来表示特征。
    - 相似的词可用相似的向量表示：克服数据稀疏问题
    - 分布式表示是一种降维方法，克服维度灾难（Curse of Dimensionality）
  - 全局解码算法：
    - 用柱搜索（Beam Search）等近似全局搜索／解码算法总和考虑多个状态之间的依赖关系。
- 目前还需要利用传统方法构造转移系统，同时全局解码算法有助于系统性能的进步提升
  - 序列到序列（Sequence to Sequence），也称编码解码（EncoderDecoder）方法在多个自然语言处理任务中广泛应用：机器翻译、阅读理解等。
  - 依存句法是典型的结构话学习问题，在nlp中，很多结构话学习任务：分词、词性标注等，都可以借鉴上述基于转移的算法框架解决。
  - 很多上层应用依赖于句法分析。尤其是深度学习中递归神经网络方法就是依赖句法分析结果进行语义的递归组合。

##### [深度学习的五个挑战和其解决方案](http://chuansong.me/n/1664863851518)

- 机器学习的各个主要方向，从底层的深度学习分布式机器学习平台(AI的Infrastructure)到中层的深度学习、强化学习、符号学习算法以及再上面的机器学习理论。
- 新的基于CNN的深度模型叫做残差网络，这个残差网络深度高达152层，取得了当时图象识别比赛上面最好的成绩
- 深度学习里最经典的模型：
  - 全连接的神经网络，就是每相临的两层之间节点之间是通过边全连接；
  - 再就是卷积神经网络，这个在计算机视觉里面用得非常多；
  - 再就是循环神经网络RNN，这个在对系列进行建模，例如自然语言处理或者语音信号里面用得很多，这些都是非常成功的深度神经网络的模型。
  - 还有一个非常重要的技术就是深度强化学习技术，这是深度学习和强化学习的结合，也是AlphaGo系统所采用的技术
- 当前深度学习的一个前沿就是如何从无标注的数据里面进行学习。现在已经有相关的研究工作，包括最近比较火的生成式对抗网络，以及我们自己提出的对偶学习。
  - 生成式对抗网络的主要目的是学到一个生成模型
  - 它是同时学习两个神经网络：一个神经网络生成图像，另外一个神经网络给图像进行分类，区分真实的图像和生成的图像。在生成式对抗网络里面，第一个神经网络也就是生成式神经网络，它的目的是希望生成的图像非常像自然界的真实图像，这样的话，那后面的第二个网络，也就是那个分类器没办法区分真实世界的图像和生成的图像；而第二个神经网络，也就是分类器，它的目的是希望能够正确的把生成的图像也就是假的图像和真实的自然界图像能够区分开。大家可以看到，这两个神经网络的目的其实是不一样的，他们一起进行训练，就可以得到一个很好的生成式神经网络
  - 针对如何从无标注的数据进行学习，我们组里面提出了一个新思路，叫做对偶学习。对偶学习的思路和前面生成式对抗学习会非常不一样。对偶学习的提出是受到一个现象的启发：我们发现很多人工智能的任务在结构上有对偶属性
    - 搜索引擎最主要的任务是针对用户提交的检索词匹配一些文档，返回最相关的文档；当广告商提交一个广告之后，广告平台需要给他推荐一些关健词使得他的广告在用户搜索这些词能够展现出来被用户点击
- 深度学习面临的第二个挑战就是如何把大模型变成小模型
  - CNN模型，也就是卷积神经网络，做模型压缩；
    - 剪枝：边权重小的去掉
    - 权值共享：上百万权值聚类，用均值代替这一类权值
    - 量化：降低浮点型精度
    - 二进制神经网络：原来32bit权值现在1bit
  - 针对一些序列模型或者类似自然语言处理的RNN模型如何做一个更巧妙的算法，使得它模型变小，并且同时精度没有损失。
    - 新的循环神经网络LightRNN：不是模型压缩降低模型大小，而是算法
    - 每个词要做词嵌入（word embedding）语义相似或相近的词在向量空间里的向量也比较接近，这样可以表达词之间语义信息或相似性。
    - 我们不用一个向量表示一个词，而是两个向量表达一个词。行／列
- 今年的人工智能国际顶级会议AAAI 2017的最佳论文奖，颁给了一个利用物理或者是一些领域的专业知识来帮助深度神经网络做无标注数据学习的项目。论文里的具体例子是上面这张图里面一个人扔枕头的过程，论文想解决的问题是从视频里检测这个枕头，并且跟踪这个枕头的运动轨迹。如果我们没有一些领域的知识，就需要大量的人工标注的数据，比如说把枕头标注出来，每帧图像的哪块区域是枕头，它的轨迹是什么样子的。实际上因为我们知道，枕头的运动轨迹应该是抛物线，二次型，结合这种物理知识，我们就不需要标注的数据，能够把这个枕头给检测出来，并且把它的轨迹准确的预测出来。这篇论文之所以获得了最佳论文奖，也是因为它把知识和数据结合起来，实现了从无标注数据进行学习的可能
- AI技术来分析股票：动态决策性问题：难点：时变性
- 决策第二点：各种因素相互影响：静态认知性任务我们的预测结果不会对问题

##### [理解LSTM网络](http://chuansong.me/n/1756021)

- Recurrent Neural Networks
  - 传统神经网络不能处理的问题：使用先前的事件推断后续的事件
  - RNN解决了这个问题：包含循环的网络：允许信息的持久化
  - 同一神经网络的多次复制，每个神经网络模块把消息传递给下一个
  - 链式特征揭示了RNN本质上与序列和列表相关的
  - RNN的语音识别、建模、翻译、图片描述等
  - 应用成功的关键之处：LSTM的使用，是一种特别的RNN，比标准的RNN在很多任务都表现的更好。
- 长期依赖问题：
  - RNN的关键：用来连接先前的信息到当前的任务上，还有很多依赖因素
  - 有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如：语言模型基于当前词语来预测下一个词。如果预测“the clouds are in the sky”最后的词，我们并不需要任何其他的上下文--因此下一个词应该是sky。RNN可以学会使用先前的信息。
  - 但是更加复杂的场景下：假设我们试着去预测“I grew up in France… I speak fluent French”最后的词，相关信息和当前预测位置之间的间隔变得相当大。当间隔大的时候，RNN会丧失学习到连接如此远的信息的能力。
- LSTM网络-Long Short Term 网络
  - RNN特殊的类型。可以学习长期依赖
  - 通过刻意的设计避免长期依赖问题。记住长期的信息在实践中是LSTM的默认行为
  - 所有RNN都具有一种重复神经网络模块的链式形式，标准RNN中，重复的模块只有一个简单结构：例如tanh层。
  - LSTM同样如此：但是重复的模块有不同的结构
- LSTM的核心思想：
  - 关键：细胞状态：水平线在图上方贯穿运行。
  - LSTM有通过精心设计的称为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。包含一个sigmoid神经网络层和一个pointwise 乘法操作。Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”！
- 逐步理解LSTM：
  - 第一步决定丢弃什么信息。通过忘记门层完成。
  - 下一步决定什么样的新信息被存在细胞状态中。
    - 第一，sigmoid层称“输入门层”决定什么值我们将要更新。
    - 根据信息产生对状态的更新
    - 最后确定输出什么值。这个输出基于我们的细胞状态。
- LSTM的变体
  - GRU

##### [浅谈基于张量分解的隐变量模型参数学习](http://chuansong.me/n/1805856)

- 很多工作使用张量分解学习隐变量模型的参数，可以获得参数的全局最优解
- 隐变量模型本质上一个定义在两类变量上的概率分布：隐含变量和观测变量：隐变量模型的两个最基本任务：
  - 给定观测变量，推断隐含变量，在概率图模型：infernce
  - 估计模型中参数：learning
- 学习概率分布的参数：通常方法；最大似然估计、矩估计。
  - 隐变量模型的参数学习使用最多的是最大似然估计：给定数据，写出似然函数，优化似然函数，估计参数。：参数估计的主流方法。 缺点：隐变量模型的似然函数基本上都是非凸函数，很难获得全局最优解。优化似然函数的常用方法：EM、variational EM通常只能获得局部最优解。
  - 为了获得参数的全局最优解：矩估计：基于张量分解的方法就属于此类：基本思想：求样本的低阶矩，通过解方程得到参数的值。求解一阶矩和二阶矩。但是方程不一定能写出来。张量方法目前只适用于某些隐变量模型，隐马尔可夫模型，话题模型，高斯混合模型，并不像最大似然方法普遍适用于任何模型
  - 张量分解本质上：优化问题：非凸优化问题。如果这个非凸优化中得到的是局部最优解。但是如果张量是对称的。尽管非凸优化，也能获取全局最优解。

##### [人工智能之争](http://chuansong.me/n/1792248)

- 传统人工智能：自然语言翻译、符号推理（symbolicreasoning）、博弈论（game playing）等问题：专家系统：运用规则记录专家的经验：医生诊断经验的模型
- 人机交互human computer interaction：图形用户界面
- 机器学习：数理统计工具开发不同的识别和分类算法:识别目标、发现数据中模式、用于机器人的策略

##### [Facebook人工智能研究最新进展](http://chuansong.me/n/1897593)

- 最大挑战：无监督学习
  - 因为人类和动物使用对多的就是这种学习方式

##### [知识图谱的应用](http://chuansong.me/n/1999287)

- 什么是知识图谱
  - 本质：语义网络：基于图的数据结构：节点（point）和边（edge）组成
  - 每个节点：表示现实的实体。每条边：实体与实体之间的“关系”
  - 知识图谱：关系的最有效的表示方式，把所有不同种类的信息连接在一起的到的关系网络，知识图谱提供了从“关系”角度去分析问题的能力。
  - 最初：用来优化现有的搜索引擎，不同于关键词搜索的传统搜索引擎，知识图谱可用来更好的查询复杂的官联系信息。
- 知识图谱的表示
  - 描述：事实。
  - 属性图和传统的RDF格式都可以作为知识图谱的表示和存储方式
- 知识图谱的存储
  - 知识图谱是基于图的数据结构。存储方式主要有两种：
    - RDF存储
    - 图数据库
  - 如果需要设计的知识图谱简单，查询不会涉及到1度以上的关联查询。可以选择用关系型数据存储格式来保存知识图谱。对于复杂的关系网络知识图谱的的优点还是明显的。
  - 把实体和关系存储在图数据结构是一种复合整个故事逻辑的好方式
- 应用
  - 反欺诈：风控：基于大数据的反欺诈难点：不同来源的数据（结构化、非结构化）整合在一起，构建反欺诈引擎，并有效识别出欺诈。涉及到复杂的关系网络。知识图谱，作为关系的直接表示方式，可以很好的解决这两个问题。
  - 构建多数据源的知识图谱
  - 不一致性验证设计到知识推理。可以理解成链路预测，也就是从已有的关系图谱里推导出新的关系或连接。
  - 异常分析：基于图
    - 静态分析：给定图结构和某个点，从中发现一些异常点
    - 动态分析：分析其结构随时间变化的趋势
  - 除了贷前的风险控制，也可以在贷后发挥其强大的作用。
  - 智能搜索
  - 精准营销：结合多种数据源分析实体之间的关系。对用户的行为有更好的理解。
- 数据的噪声
  - 数据本身错误：不一致性验证
  - 数据冗余：涉及“消歧分析”
- 非结构化数据处理能力
  - 数据挖掘、nlp、ml
- 知识推理
  - 人类智能的重要特征：需要规则的支持。常用的推理算法：基于逻辑的推理（Logic）和基于分布式表示方法(Distributed Representation)的推理
- 大数据、小样本、构建有效的生态闭环是关键
  - 面临的依然是小样本问题，也就是样本数量少。实际上，我们能拿到的欺诈样本数量不多，即便有几百万个贷款申请，最后被我们标记为欺诈的样本很可能也就几万个而已
  - 所谓的生态闭环，指的是构建有效的自反馈系统使其能够实时地反馈给我们的模型，并使得模型不断地自优化从而提升准确率。

##### [AlphaGo原理解析](http://chuansong.me/n/2658183)

- 围棋棋盘是19x19路，所以一共是361个交叉点，每个交叉点有三种状态，可以用1表示黑子，-1表示白字，0表示无子，考虑到每个位置还可能有落子的时间、这个位置的气等其他信息，我们可以用一个361 * n维的向量来表示一个棋盘的状态。我们把一个棋盘状态向量记为s
- 当状态s下，我们暂时不考虑无法落子的地方，可供下一步落子的空间也是361个。我们把下一步的落子的行动也用361维的向量来表示，记为a。
- 这样，设计一个围棋人工智能的程序，就转换成为了，任意给定一个s状态，寻找最好的应对策略a，让你的程序按照这个策略走，最后获得棋盘上最大的地盘
- 第一招：深度卷积神经网络：
- 第二：MCTS：蒙塔卡洛搜索树Monte-Carlo Tree Search：
  - 没有任何人工的feature，完全依靠规则本身。靠一种类似遗传算法的自我进化
  - 可以连续运行
- 第三：自我进化，强化学习

[神经网络做唐诗](http://chuansong.me/n/2247902)

- [github地址](https://github.com/GeekQi/tangshi-rnn)
- 神将网络在cv和nlp方面效果非常好，原因有如下：
  - 传统的机器学习建立在统计基础上，但是当数据与数据之间的关系难以用统计描述，传统方法不行
  - 传统ml需要专家知识挑选特征。特征好坏与学习成果有很大关系。
- 神经网络的厉害之处就是克服了以上两点：
  - 一是每个神经元都有非线性的公式，可以得到复杂的难以用数学公式描述的关系
  - 二是语音识别的网络，训练时都是原始数据输入，到结构输出模型（End2End）
- 简单的神经网络有局限：RNN递归神经网络（Recurrent Neural Network）本身限制
  - 每次的输出作为下一次的输入返回到神经网络中。训练神经网络就是把这个过程逆向，从最后的输出开始往之前的输出推，每一次推的时候要用到当时的输入和输出计算梯度。
    - 当把第1000个字符的梯度退回到第1个时间状态有两个问题：
      - 存储999个中间值，吃光内存
      - Vanishing/Exploding Gradient, 太远的gradient可能就太大爆掉，或者太小对神经网络完全没有影响
- 但是近体诗：五言和七言的比较适合这个神经网络
  - 字数少、句式固定、规律明确。
    - 大概$150的AWS的GPU 节点使用费，和一些其他忽略不计的数据处理的各种CPU节点。
    - 一周收集并清理全唐诗。
    - 然后训练的时候走了很多弯路，最后这个model在一个月之后才弄出来的。
    - 处理平仄花了一周。
- LSTM和RNN的对比
  - RNN有两个著名的衍生姐妹，Gated Recurrent Unit和Long-Short-Term Memory。
  - LSTM训练时间长，最后效果好不少
- Word Embedding重要性及词典大小
  - 神经网络每个输入输出都是向量或矩阵。为了能把所有字都转化成向量，用到的技巧就是embedding：每个字变成独特的向量。里面只有有一个1
  - 神经网络的第一层实现功能就是把这个很长的one hot vector 变成一个独特的长度更低的向量
  - 实际使用取了频率最高的2000个字，用34%的独立字符覆盖94%的总体字数
- 加入起始和终止字符，让结构更加明确。^ 表示开始。 $表示结束
- 利用Dropout增加准确性
  - 训练的时候一定数量内部的神经元被随机设置为0，但是训练完了又回复原状。也就是遮挡住一部分特征。比如小孩识别汽车，每次看一部分，但是给张完整的，也能看出是汽车
- 不同SGD方法效果不明显
- 生成字符表是一个概率分布
  - 这个神经网络的生成结构：每次在做预测的时候，并不是给出某一个字符，而是给出整个词典里所有字可能出现的概率。
- 将平水韵作为神经网络的输出过滤
  - 关键是押韵效果不好，需要平仄的地方进行过滤


##### [神经网络十大误解分析](http://chuansong.me/n/329467651436)

- 机器学习中最流行最强大的一类
  - 计量金融中，常被用作时间序列预测、构建专用指标、算法交易、证券分类、信用风险建模等，但是不可靠
- 神经网络更接近曲线拟合（curve fitting）和回归分析（regression analysis）等统计方法。曲线拟合即函数逼近，逼近复杂的数学函数
- 神经网络由互连节点层组成。单个节点称为感知器（perceptron），类似与一个多元线性回归（multiple linear regression）。
  - 多元线性回归和感知器之间的不同之处在于：感知器将多元线性回归生成的信号送给可能线性可能非线性的激活函数中。在多层感知器（MLP）中，感知器按层级排布，层层之间互相连接。
  - MLP中有三种类型的层：输入层、隐藏层、输出层。
    - 输入层接收输入模式而输出层可以包含一个分类列表
    - 隐藏层调整输入的权重，直到将NN的误差降低到最小（另一种解释：隐藏层提取输入数据中的显著特征，这些特征有关于输出的预测能力）
  - 映射输入：输出
    - 感知器接收输入向量。通过感知器的权重向量进行加权。在多元线性回归的背景中，这些可被认为是回归系数或beta系数。使用该总和产物得到净值的神经元被称为求和单元（summation unit）
    - 净输入信号减去偏差theta后被输入一些激活函数f()。激活函数通常是单调递增函数，其值位于 (0,1) 或 (-1,1) 之间，激活函数可以是线性的，也可以是非线性的
    - 最简单的神经网络只有一个映射输入到输出的神经元。对于给定模式 p，该网络的目标是相对一些给定的训练模式 tp 的一些一只的目标值来最小化输出信号 op 的误差。比如，如果该神经元应该映射 p 到 -1，但却将其映射到了 1，那么，根据距离的求和平方测定，神经元的误差为 4，即 (-1-1)^2
  - 分层：
    - 感知器被分层进行组织：第一层：输入层，接受训练集。隐藏层将前一层的输出作为下一层的输入，下一层的输出又作为另一层的输入。隐藏层：提取输入数据中的显著特征。这些特征可以预测输出。这个过程称为特征提取（feature extraction），而且和主成分分析（PCA）等统计技术相似功能。
  - 学习规则：
    - 神经网络的目标是最小化一些错误度量（measure of error）,最常见的错误度量是平方误差和（Sum squared error(SSE)） ε
    - 鉴于该网络的目标是最小化 ε，可以使用优化算法调整该NN中的权重。最常见学习算法是梯度下降法：计算相对于NN中每一层的权重的误差偏导数，然后在提督相反的方向上移动
- NN的不同框架
  - 多层感知器：最简单的NN结构。任何NN的性能，是其结构和权重的一个函数。
  - RNN（递归NN）：一些或所有的连接倒流，意味着反馈环路存在于NN中。
  - Boltzmann NN 最早的全连接NN之一，也就是Boltzmann机。：Hopfield递归NN的蒙特卡洛版。
  - 深度NN最大问题之一：尤其是不稳定金融市场环境下，是过度拟合
  - 自适应NN（AdaptiveNN）能够在学习中同时自适应、并优化自身结构的NN。
  - 径向基函数网络：利用径向基函数作为激活功能。常用的函数为高斯分布。还用于支持向量机的内核
- 如果NN太大／太小，NN会出现过拟合／拟合不够，也就是网络无法顺利泛化样本
- 利用相关性去选择输入变量有两个问题：如果使用线性相关矩阵，不小心排除了有用变量。第二两个相对不相关变量，结合可能产生一个强相关变量。第二种问题应该利用主成分分析去获取有用的特征向量（变量的线性结合）
- 选择变量过程的一个问题：多重共线性：有两个或更多的独立变量是高度相关的。在回归模型，可能引发回归系数根据模型或数据的细微变化而不规律的变化
- 隐藏层越多，过拟合风险越大。
- 神经网络可能用于回归或分类。在回归模型中，简单的输出值被映射到一组真实数字，只需要一个输出神经元。分类系统输出一个神经元。如果类别未知，就要使用无监督NN比如：自组织映射
- 最高的办法遵守奥卡姆剃刀原理。对于两个性能相当的模型，自由参数更少的模型，泛化效果越佳。另一方面，不能牺牲效果来选择过度简化的模型。
- NN的训练算法有很多：必须停止的情况：误差率降低到了可接受水平、验证集的误差率开始变差或资源耗尽。最常见的NN学习算法：反向传播算法（backpropagation），这种算法使用了前文的梯度下降。
- 反向传播包括两个步骤：
  - 前向传播—将训练数据集通过网络，记录下神经网络的输出并计算出网络的误差
  - 反向传播— 将误差信号反向通过网络，使用梯度下降优化神经网络的权重
  - 存在问题：一次调整所有权重会导致权重空间的NN出现明显变化、随机梯度下降算法慢，对局部最小值敏感。对特定NN局部最小值。为了得到所需的全局最优化算法，两种流行的全局优化算法是粒子群优化算法（PSO）和遗传算法（GA）。
- NN可用的三种学习策略
  - 监督学习、	
    - 需要至少两个数据集：训练集由输入数据和预期输出数据组成。测试集只包含输入数据。这两个数据集的数据必须有标记，即数据模式是已知的
  - 无监督、
    - 一般用在没有标记的数据中发现隐藏结构：隐马尔可夫链，类似聚类算法
    - 流行框架：自组织映射（Self Organizing Map）本质上多维量度技术：另一个应用对股票交易的时间序列图标上色
  - 增强学习／强化学习
    - 策略由三部分组成
      - 一个指定NN如何决策的规则：例如技术分析／基本面分析
      - 一个区分好坏的奖赏功能：如赚／亏
      - 一个执行长期目标的价值函数
    - 在金融／游戏环境，强化学习策略特别有用，应为NN可以学习对特定量化指标进行优化。例如对风险调整收益的合适度量


##### [深度学习中的最小风险训练](http://chuansong.me/n/652376051971)

- 有监督学习指：训练样本不仅包含输入，同时包含对应标准答案输出。有监督学习的标准训练准则是极大似然估计，基本思想是一个好的模型应该尽可能使观测到的训练样本概率最大。
- 最小风险训练：使用损失函数来描述模型预测与标准答案之间的差异程度（损失）。并试图 寻找一组参数使得模型在训练集上损失的期望值（风险）最小。
- 在深度学习中使用最小风险训练有以下两大优点：
  - 适用于任意评价指标：任意定义在单个样本级的评价指标都可以作为最小风险训练中的损失函数。让训练出来的模型尽可能贴近用户需求
  - 适用于任意NN：最小风险训练不对模型框架做任何假设，无论是卷积NN还是CNN等

##### [解读GAN及进展](http://chuansong.me/n/1500459351517)

- GAN:Generative Adversarial Nets 生成式对抗网络：一方面将产生式模型拉回到了一直由判别式模型主场。也将对抗训练从常规游戏技能引到更一般领域。
- GAN基本框架：当判别器（Discriminator）不能区分真实数据x和生成数据G(z) 时，就认为生成器G达到了最优

##### [机器学习中贝叶斯基本理论、模型和算法](http://chuansong.me/n/1649815651232)

- 基本公式：我们用θ描述模型的参数，这个模型可以是神经网络、线性模型、或者SVM，参数都用θ来描述；大D是我们的训练集；π(θ)是先验分布，是我们看到数据之前对模型本身分布的描述；p(D|θ)是似然函数，给定一个模型θ的情况下描述这个数据的似然；我们的目标是想获得这个后验分布，是看到数据之后我们再重新看模型本身的分布情况。
- 机器学习中贝叶斯法则可以做什么
  - 预测问题：大M来描述model class，比如线性模型、非线性模型，model class里面有多个具体的模型，我们还是用参数θ表示
  - 还可以做不同模型的比较、模型的选择。比如说我们要做分类问题，到底是要选线性的模型还是深度学习的非线性模型，这是在做模型选择的问题。这个问题可以描述成这样：我们用M1表示一个model class，可能是一个线性模型，我们用M2表示另外一个model class，是非线性模型，我们在同样数据集D的情况下，我们可以去比较这两个量哪个大，这个量是描述的在M1下我们观察到训练集的一个似然，另外一个是在M2的情况下观察的数据集似然，可以通过这个比较看我应该选择哪一种模型，这是一个用贝叶斯方法做模型选择的一个基本的规则。
- 贝叶斯推理时，我们的输入是一个先验分布和一个似然函数，输出是一个后验分布
- 损失函数优化问题。比如说，我要做分类，我要训练神经网络，第一项是一个损失函数，它度量了在训练集的错误率；第二项是我们想加的正则化项，目的是保护这个模型避免过拟合、避免发散，这是一个基本框架。这些东西在机器学习里基本上和贝叶斯是不搭边的
- 增强学习/在线学习，它的目标是优化Regret/reward，也有一个目标函数来度量。
- 传统的SVM是找一个决策面，按照一定的最优准则来找。贝叶斯的思想是：我们可以有无穷多个决策面，但是每个决策面有一定的概率。

##### [基于社会媒体的预测技术(上)](http://chuansong.me/n/1703436)

- 社会媒体为预测技术提供了新的数据源
- 基于社会媒体的预测技术
  - 两方面作用：社会信号的采集／大众预测的融合
- 基于消费意图的挖掘的预测
  - 基于社会媒体的消费意图挖掘
    - 对于消费意图挖掘任务的领域相关等问题，提出了：**基于领域自适应卷积神经网络的社会媒体用户消费意图挖掘方法**。卷积神经网络对于解决该任务有以下两方面的优势：
    - 卷积神经网络中的卷积层可以以滑动窗口的方式捕捉词汇级语义特征，而马克斯池(max pooling)层则可以很好地将词汇级特征整合成句子级语义特征；
    - 卷积神经网络可以学习不同层次的特征表示，而一些特征表示则可以在不同领域间迁移。
  - 基于消费意图挖掘的电影票房预测
    - 推荐系统、产品销售预测等
    - 电影票房预测主流模型：线性预测模型／非线性预测模型：都有一个前提：收入与预测影响因素之间存在线性或非线性关系。首周预测：线性回归比非线性好，总票房，非线性高于线性。说明：上映前一周的数据与首周线性关系比较明显，此时线性预测好。随着时间推移，新因素加入以及偶然情况发生，使得线性关系不明显。线性回归模型的预测能力不如非线性。
- 基于事件抽取的预测
  - 不同于消费是从人的主观角度。事件从客观事实角度出发
  - 金融市场的预测研究分成：时间序列交易数据驱动和文本驱动两个不同方向。
    - 时间序列交易数据：最早用于建立预测模型的数据：包括股票历史价格数据等
    - 文本驱动：挖掘新闻报道中客观事实及大众情感等。
- 基于因果分析的预测
  - 与相关性相比，因果的确定性更强，如稀有事件等

##### [评论对象抽取综述](http://chuansong.me/n/2032451)

- 主流的提取技术：名词短语的频繁项挖掘、评价词的映射、监督学习方法及主题模型放方法。
- 评价对象抽取属于信息抽取的范畴：将非结构文本转换为结构化数据的技术。主要用于网络文本的意见挖掘。
- 如果挖掘在文本中已经出现的评论对象：主流方法4种：
  - 名词挖掘：从频繁的名词开始
    - 评论对象大都是名词或名词短语:词性标记得到语料中的名词
  - 评价词与对象的关系
- 监督学习方法
  - 信息抽取的研究提出了很多监督学习算法。其中主流的方法根植于序列学习（Sequential Learning，或者Sequential Labeling）
  - 目前最好的序列学习算法是隐马尔可夫模型（Hidden Markov Model，HMM）和条件随机场（Conditional Random Field，CRF）
- 主题模型（Topic Model）
  - 统计主题模型逐渐成为海量文档主题发现的主流方法。主题建模是一种非监督学习方法，它假设每个文档都由若干个主题构成，每个主题都是在词上的概率分布，最后输出词簇的集合，每个词簇代表一个主题，是文档集合中词的概率分布。
  - 目前主流的主题模型有两种：概率潜在语义模型（Probabilistic Latent Semantic Analysis，PLSA）和潜在狄利克雷分配（Latent Dirichlet Allocation，LDA）
  - 主题模型是基于贝叶斯网络的图模型


##### [用户画像User Profile之用户能力标签](http://chuansong.me/n/1980238)

- 微博作为最大的中文社交媒体，拥有数以“PB”（1024 TB）计的用户信息，从海量的用户信息中发掘每个用户的社交特性、潜在能力及兴趣等信息，是微博为用户提供更加人性化服务的基础。
- 用户画像体系。该体系涵盖**能力标签、兴趣标签、关系及亲密度、信用质量和自然属性**五大部分
- 每一个用户都是网络中的一个具备发布、传播、消费信息功能的节点。其中一部分节点具备发布优质原创信息的功能，并通过社交网络将信息快速传播，即**能力节点**；而其他大部分节点则偏重于消费信息，同时传播其感兴趣的信息，即**消费节点**。
- **用户标签体系、能力标签的应用场景、能力标签挖掘框架、关键技术点**四个方面对用户能力标签的整体挖掘框架和挖掘算法
- **用户标签体系**
  - 某个话题下的相关信息中聚合出一个或者多个具有代表性的词语作为标签，能够方便对用户与内容的查找与分析。
  - **在当前的三层用户标签体系中，共存在50多个一级标签，1000多个二级标签和近30万的三级标签**
- **能力标签的应用场景**
  - 其中两个典型的业务场景是“微博找人”和“热门微博”
  - 找人业务场景中，用户可以直接发现各垂直领域的专家账号，通过关注专家账号可以直接获取各垂直领域的优质内容。
  - 在热门微博业务场景中，内容流都出自于垂直领域的专家账号：一个账号通过发布某个领域的优质内容形成初步影响力，大数据计算出其所属领域后，热门微博会在对应领域进行内容推荐，使该账户逐步成长为专家账号，从而形成一个产品闭环。
  - 能力标签的主要作用是构建各种优质语料的重要基础数据源，通过能力标签圈定专家用户群体，提取出优质语料等相关信息；在大部分情况下，能力标签不直接在业务场景中展示
- **能力标签挖掘框架**
  - 首先通过**用户关系数据**(主要是分组，用于体现粉丝对于用户能力的认可度)、**用户内容数据**(主要是原创博文，用于体现用户自身的专业能力)、**用户行为数据**(主要是转、评、赞等互动信息，用于体现该用户在相关领域内的影响力)挖掘出用户的能力标签及其基础权重；其次通过引入**用户的自填信息、认证信息**作为能力标签权重的调权因子参与计算；接下来通过多个维度的定向挖掘系统和运营反馈系统进行能力标签的校正和增加能力标签的覆盖。最后，**将挖掘出来的用户能力标签及权重输出至用户能力标签库，供上层业务调用**。
- **信用质量和自然属性**
  - 标签词汇聚、用户影响力、时间窗口和时间衰减三个关键技术点
  - 标签词汇聚
    - 用户为关注对象打上的标签作为用户关系数据引入到挖掘过程中，由于标签属于UGC，就会造成同一个标签主题有多种不同的表达方式，将多种不同的表达方式聚合起来，形成一个标签集，并且映射到我们的标签体系中，可以有效地提升能力标签的准确率和覆盖率
    - 首先将分组信息通过分类模型划分为**强关系型**(同学、同事等)和**兴趣型**(互联网、财经等)两类，并将兴趣型分组信息作为我们的基础预料
    - 接下来通过**聚类、关联**等相关算法进行标签词(分组信息)的聚合；
    - 最后将聚合的标签集根据**相关程度等因子划分为高相关和低相关**两类
  - 用户影响力
    - **用户在某个特定标签下的影响力**，因此影响力计算的边界(如图5所示)是标签对应的兴趣用户群体（包含该标签的能力用户）
    - 具体地，我们将其它用户对某个用户原创博文的转、评、赞等互动行为作为基础数据，利用**pagerank迭代算法**进行该用户影响力的计算
    - 其中，同领域用户的影响力大小是由其它用户对相关博文的转、评、赞等互动行为按照一定的权重比计算得到的。
  - 时间窗口和时间衰减
    - 考虑到原创博文的消费价值和计算代价，对于用户内容数据，我们选取了用户近一段时期内的原创博文作为基础语料进行计算。
    - 关于时间衰减，我们结合牛顿冷却定律和微博的业务需求推导出相应的衰减公式，并通过衰减效果的对比，确定了相关衰减参数的数值，最终得出了用户能力标签内容权重的时间衰减函数
- 主要从**社交关系、原创内容、影响力**三个维度来识别用户的能力标签以及计算相应的权重，同时通过用户的自填信息、认证信息等其他信息进行调权。

##### [社会媒体挖掘](http://chuansong.me/n/1941613)

- 基于Web 2.0的思想和技术的互联网应用，支持用户创造和交换内容。
- 用户在社会媒体上分享、交流、联系、互动产生的海量数据，比如每天都查看女神的微信并点赞的数据，比如每天都发几张吃吃吃的美食图的数据
- 《微信社会影响力报告》《双十一剁手数据报告》《手游人群消费报告》
- 首先呢，要懂图、网络度量、网络模型和数据挖掘
- 度量和模型指导我们做出什么样的图，用什么标准解释图的含义
- 数据挖掘基本要素是数据获取、数据预处理、数据挖掘算法
- **社区分析**
  - 分析社区是如何形成、演变的，如何知道这个社区的质量？这就需要社区分析知识，比如社区发现算法
- **信息如何扩散**
  - 需要先搞懂信息的传播方式：羊群效应、信息级联、创新扩散和流行病
- **实际应用**
  - 分析社会网络中个人的影响力，典型的问题：微博中哪个大V最有号召力？
  - 在线为用户推荐个人和好友，典型的问题：微博是如何为你推荐好友的？
  - 分析用户个人行为，典型的问题：你今天会不会玩三国杀，玩完还会干什么？

##### **[语言分析技术在社会计算中的应用](http://chuansong.me/n/2294022)**

- **面向社会媒体的自然语言使用分析**
  - 传统的自然语言处理主要面向正式文本，例如新闻、论文等。这些文本遣词造句比较规范，行文符合逻辑，因此比较容易处理。
  - 自然语言处理技术按照处理目标分为几个层次：
    - 1）词汇层。主要是在词汇级别的处理任务，如中文分词、词性标注、命名实体识别等。（2）句法层。主要是在句法级别的处理任务，如针对句子的句法分析、依存分析等。（3）语义层。主要是在语义空间的处理任务，例如语义分析、语义消歧、复述等。（4）篇章层。主要是在篇章级别的处理任务，如指代消解、共指消解等。（5）应用层。主要是指利用自然语言处理分析技术完成的应用任务，如文本分类、信息抽取、问答系统、文档摘要、机器翻译，等等。
- **面向社会媒体的自然语言分析应用**
  - 社会预测
    - 产品销量、体育比赛结果、股市走势、政治选举结果、自然灾害传播趋势
    - 社会媒体中关于候选人的提及率就是很好的预测指标，例如根据Facebook上的支持率就能够成功预测2008年美国总统大选结果

##### **[个性化推荐](http://chuansong.me/n/2823936)**

- 推荐系统的研究和生产实际，基于以上这个关于推荐的肤浅定义，把整个系统的模型简化成了——预测用户对于某个事物的喜爱，也就是人们常说的Rating Prediction的问题[3]。Collaborative Filtering，特别是基于Matrix Factorization[2]和Latent Factor Model[1]的各种方法
- 协同过滤，辅佐以Machine Learning的很多手段（比如Tree-based Models)，常常能够在Rating Prediction这个问题上有不俗的成绩
- 原因很直观，要想优化用户喜好，那就必然强调用户的历史行为。协同过滤或者是机器学习导向的算法，都试图充分挖掘单个用户以及群体用户的喜好，并且加以推崇到极致（Optimization)。

##### **[为学者写的提高生产力的方法技巧](http://chuansong.me/n/1703442)**

- 优化交易成本
  - 保证你生活道路上的阻力最小。因为最小的阻力意味着最大的生产力。

##### **[英文论文写作](http://chuansong.me/n/2833039)**

- 段落就是为读者描述、解释了一个核心思想的一段具有逻辑性的文字
  - A logical “unit” of text which develops/explains an idea to the reader.
- 文章就是由多个具有逻辑性的段落前后关联组成的。
  - An article is a chain of logical units/paragraphs.

##### **[顺滑：让语音识别更流畅](http://chuansong.me/n/1443061951148)**

- 自动语音识别（ASR）得到的文本中，往往含有大量的不流畅现象
- 不流畅现象主要分为两部分，一部分是ASR系统本身识别错误造成的，另一部分是speaker话中自带的
- NLP领域主要关注的是speaker话中自带的不流畅现象，ASR识别错误则属于语音识别研究的范畴。顺滑(disfluency detection)任务的目的就是要识别出speaker话中自带的不流畅现象。
- 对于顺滑任务，研究分4类：序列标注方法、句法和顺滑联合方法、基于RNN的方法、基于seq-to-seq的方法。目前性能最好的是基于RNN的方法和基于seq-to-seq的方法
  - 序列标注方法
    - 这类方法可以分为两大类。一类是基于词的方法。这类方法的做法是利用序列标注模型，给句子中每个词赋一个标签，最后根据标签来判断词的类型。
    - 另一类是基于块（chunk）的方法，这类方法的输出不再是与输入等长的标签序列，而是一个个带标签的短语块，这类方法的一个优点是可以利用块级别的特征
    - 传统的序列标注模型通过设计复杂的离散特征，可以在一定程度上解决长距离依赖问题，但是受限于训练语料的规模，往往会面临稀疏性的问题
  - 句法和顺滑联合方法
    - 联合方法需要同时进行句法和顺滑分析，相对于序列标注等方法，其速度会比较慢。
  - 基于RNN的方法
    - Recurrent Neural Network（RNN）是为了对序列数据进行建模而产生的，其在理论上能很好的解决长距离依赖问题，因此一些研究者尝试将RNN网络应用到顺滑任务中。LSTM(Long Short Term Memory networks)是一种特殊的RNN网络，它可以有效减轻简单RNN容易出现的梯度爆炸和梯度消散问题
    - 直接用双向LSTM的隐层输出来对每个位置的词进行分类，判断其是否为不流畅词，其性能已经超越了之前最好的序列标注方法以及句法和顺滑联合方法。
    - 这种直接分类的方法没有考虑输出标签之间的联系，当输出标签之间存在强依赖性时，这种分类方案可能会导致 标签偏置(label bias)的问题。
    - 为了解决标签偏置的问题，我们尝试采用LSTM-CRF模型。在LSTM-CRF模型中，首先通过双向LSTM学习到每个位置的特征表示，然后将学习到的特征表示直接送到一个线性CRF模型。从实验结果来看，LSTM-CRF模型的F1值要比LSTM高一个点左右
  - 基于seq-to-seq的方法
    - 采用seq-to-seq方法主要有两个动机。一个是seq-to-seq框架在编码阶段会对输入句子学习一个全局的表示，该全局表示有助于解决长距离依赖问题
    - 另一方面，seq-to-seq方法本身可以被看做一个基于条件的语言模型，原始的输入句子相当于语言模型的条件，解码阶段相当于一个语言模型的生成过程，这样就有一定得能力保证生成句子的句法完整性
    - 传统的encode-decode框架显然不能满足顺滑任务的要求，总结起来，其主要有三个局限
      - 一是其每步生成新词的时候，都会在一个固定的词表中去选择一个概率最大的词，这样就可能会生成一个不在原始句子中出现的词，
      - 二是其只能在固定的词表中去选词，如果原始句子中出现了一个不在词表中的词，那么这个词就肯定不会被生成，这明显是不符合顺滑任务要求的
      - 最后一个原因是其无法保证生成词的有序性。

##### **[Dropout分析](http://chuansong.me/n/1519459851816)**

- 过拟合(Overfitting)是深度神经网络（DNN）中的一个重要的问题：该模型学习仅对训练集合进行分类，使其自身适应于训练示例，而不是学习能够对通用实例进行分类的决策边界。
- 已经提出了许多过拟合问题的解决方案，其中，Dropout因为其简明且以经验为基础的良好结果而占据主流
- Dropout的思想是训练DNNs的整体然后平均整体的结果，而不是训练单个DNN。DNNs以概率p丢弃神经元，因此保持其它神经元概率为q=1-p。当一个神经元被丢弃时，无论其输入及相关的学习参数是多少，其输出都会被置为0。丢弃的神经元在训练阶段的前向传播和后向传播阶段**都不起作用**：因为这个原因，每当一个单一的神经元被丢弃时，训练阶段就好像是在一个新的神经网络上完成。
- Dropout在实践中表现良好，是因为它在训练阶段阻止了神经元的共适应。
- Dropout如何工作：
  - Dropout以概率p关闭神经元，相应的，以大小为q=1-p的概率开启其他神经元。**每个单个神经元有同等概率被关闭。**

##### **[基于协同过滤的中文零指代消解方法](http://chuansong.me/n/1389485951863)**

##### **[两种阅读理解模型框架的概要介绍](http://chuansong.me/n/1344812751660)**

##### **[基于事件的金融市场预测研究](http://chuansong.me/n/1281017951054)**

##### **[文本蕴含相关研究简介](http://chuansong.me/n/1238576451130)**



### 关于「当前系统项目」：**40%**

**1.目标明确**

：任务要明确，稍微改一点点，就是另一个目标了。参考自：[王露平博士](http://see.xidian.edu.cn/html/news/7897.html)

**1.1数据获取**

1.1.1工作：**理解**甲方提供Excel版：数据字典／样本数据：(订单明细、POP信息、POP商家评分、商品评论、商品基础信息等19张表格)，并**存储到mysql数据库中**：「数据导入」「数据库建表」等

1.1.2工具：[Linux基础 - get!](http://www.runoob.com/linux/linux-tutorial.html) 、 [sql基础 - get!](http://www.runoob.com/sql/sql-tutorial.html) 、   [mysql基础 - get!](http://www.runoob.com/mysql/mysql-tutorial.html)  

1.1.3参考：

1.2**数据初步分析**

1.2.1工作：原始19张表的数据分布初步探索「数据均值等分布」

1.2.2工具：[python基础 -- get！](http://www.runoob.com/python/python-tutorial.html) 、pandas

1.2.3参考：

**2.业务理解数据**

2.1**特征维度选择**

2.1.1工作：从原始19张表中结合业务理解，选择特征维度及特征列表

2.1.2工具：Hive。

2.1.3参考：

2.2**特征维度融合**

2.2.1工作：特征列表汇总到**核心目标**业务上的特征维度

2.2.2工具：Hive。

2.2.3参考：

**3.图谱构建**

3.1**图谱构建**

3.1.1工作：依据特征维度，构建图谱

3.1.2工具：Hive

3.1.3参考：

3.2图谱分析

3.2.1工作：基于图谱，提取特征

3.2.2工具：[python基础 -- get！](http://www.runoob.com/python/python-tutorial.html)、NetworkX

3.2.3参考：

**4.单模型特征应用**

4.1**回归模型选择**

4.1.1工作：单模型对提取出的特征进行评估

4.1.2工具：scikit-learn、XGB。

4.1.3参考：

4.2附：文本相似度分析过程：

4.2.1工作：匹配SELF与POP的名称

4.2.2工具：gensim、[python基础 -- get！](http://www.runoob.com/python/python-tutorial.html) 

4.2.3参考：

**5.多层级模型特征应用**

5.1**架构搭建**

5.1.1工作：多模型的架构搭建

5.1.2工具：

5.1.3参考：

-----

### 关于「基础技能」：

> 服务端：

[python基础 - get！](http://www.runoob.com/python/python-tutorial.html) 字符串、列表、字典、元组、IO、异常、多线程、面向对象、正则、MySQL、JSON、100实例

[Java基础 - get!](http://www.runoob.com/java/java-tutorial.html)    Java例子

[Linux基础 - get!](http://www.runoob.com/linux/linux-tutorial.html) 登陆、用户、文件、Vim

[Docker基础 - get!](http://www.runoob.com/docker/docker-tutorial.html)  容器引擎、虚拟化、打包镜像部署、沙盒、虚拟化

[PHP基础 - get！](http://www.runoob.com/php/php-tutorial.html)mysql、XML、AJAX

[正则表达式基础 - get!](http://www.runoob.com/regexp/regexp-tutorial.html)  

[JSP基础 - get!](http://www.runoob.com/jsp/jsp-tutorial.html)  

[Scala基础 - get!](http://www.runoob.com/scala/scala-tutorial.html)  

[设计模式基础 - get!](http://www.runoob.com/design-pattern/design-pattern-tutorial.html)

[Django基础 - get!](http://www.runoob.com/django/django-tutorial.html)  

[Servlet基础 - get!](http://www.runoob.com/servlet/servlet-tutorial.html)  

> 数据库：

[sql基础 - get!](http://www.runoob.com/sql/sql-tutorial.html) 查询、筛选、插入、更新、连接、合并、约束、删除、更改、函数

[mysql基础 - get!](http://www.runoob.com/mysql/mysql-tutorial.html) 匹配、索引、临时表、正则、去重、SQL注入

[MongoDB - get!](http://www.runoob.com/mongodb/mongodb-tutorial.html)  分布式文件存储

[Redis - get！](http://www.runoob.com/redis/redis-tutorial.html)  key-value存储系统

> 开发工具：

[git基础 - get!](http://www.runoob.com/git/git-tutorial.html)  分布式版本控制系统、克隆、修改、提交、工作区、版本库、分支、主线、合并、提交历史、标签、[github](http://www.runoob.com/w3cnote/git-guide.html)

> XML教程：

[XML基础 - get!](http://www.runoob.com/xml/xml-tutorial.html)  可扩展标记语言（e**X**tensible **M**arkup **L**anguage）、传输和存储数据

> Web Service:

[RSS基础 - get!](http://www.runoob.com/rss/rss-tutorial.html)    Really Simple Syndication（真正简易联合）

[RDF基础 - get!](http://www.runoob.com/rdf/rdf-intro.html)   RDF 是一个用于描述 Web 上的资源的框架

> HTML / CSS / JavaScript

[HTML基础 - get!](http://www.runoob.com/html/html-tutorial.html)

[CSS基础 - get!](http://www.runoob.com/css3/css3-tutorial.html)

[Boostrap基础 - get!](http://www.runoob.com/bootstrap/bootstrap-tutorial.html)

[JavaScript基础 - get!](http://www.runoob.com/js/js-tutorial.html)  

[jQuery基础 - get!](http://www.runoob.com/jquery/jquery-tutorial.html)

[AJAX基础 - get!](http://www.runoob.com/ajax/ajax-tutorial.html)  

[JSON基础 - get!](http://www.runoob.com/json/json-tutorial.html) 

[Highcharts基础 - get!](http://www.runoob.com/highcharts/highcharts-tutorial.html)  

> 网站建设：

[HTTP基础 - get!](http://www.runoob.com/http/http-tutorial.html)  HyperText Transfer Protocol、超文本传输协议、TCP/IP传输控制协议/因特网互联协议、客户端-服务端架构、C/S、通讯流程、消息结构、客户端请求消息、服务器响应消息、请求方法、HTTP状态码

[TCP/IP基础 - get!](TCP/IP基础 - get!)   TCP/IP 是因特网的通信协议、-**TCP 使用固定的连接、**IP 是无连接的、**IP 路由器**、**TCP/IP**、**IP 地址包含 4 组数字、**32 比特 = 4 字节、**IPV6**、**域名**、**TCP/IP 是不同的通信协议的大集合**

[网站主机基础 - get!](http://www.runoob.com/hosting/hosting-tutorial.html)  网站、web 服务器、ISP( Internet Service Provider ) Internet 服务提供商、每日的备份、流量限制、带宽或内容限制、域名是网站唯一的名称、主机解决方案中应包括域名注册、DNS 、月流量、POP 指的是邮局协议。、IMAP 指的是 Internet 消息访问协议。

[网站建设指南 - get!](http://www.runoob.com/web/web-buildingprimer.html)  World Wide Web（WWW)、信息存储文件：网页、web服务器、客户端、浏览器、获取网页：从服务器请求网页数据、HTTP请求包含网页地址、显示指令HTML、Hyper Text Markup Language（HTML）标记语言、<p>段落、Cascading Style Sheets（CSS）层叠样式表、样式表定义如何显示HTML元素、JavaScript客户端脚本、向HTML添加交互行为、EXtensible Markup Language（XML）可扩展标记语言、传输信息、ASP（Active Server Pages 动态服务器页面） 和 PHP（Hypertext Preprocessor技术，允许在网页中插入服务器可执行脚本） 、服务端脚本、动态改变内容、对HTML表单数据响应、访问数据、SQL、Web 创建：少即是多、导航一致、加载速度、用户反馈、显示器、Web标准：HTML、CSS、XML、XSL、DOM、Web语义化、语义网技术：描述语言和推理逻辑、语义网实现：XML及RDF （Resource Description Framework资源描述框架）、信息资源及其之间关系的描述：RDF、使用URI来标识不同的对象（包括资源节点、属性类或属性值）、可将不同的URI连接起来，清楚表达对象间的关系、**AdSense**、**AJAX (Asynchronous JavaScript and XML)**、**Apache**开源的Web服务器、**API (Application Programming Interface)**、**Browser**、**Client**、**Client/Server**、**Clickthrough Rate**、**Cloud Computing**、**Cookie**、**DB2**、**DBA (Data Base Administrator)**、**DNS (Domain Name Service)**计算机程序运行在Web服务器上域名翻译成IP地址、**DOS (Disk Operating System)**、**HTTP Client**计算机程序，从Web服务器请求服务、**HTTP Server**计算机程序，从Web服务器提供服务、**IIS (Internet Information Server)**适用于Windows操作系统的Web服务器、**IMAP (Internet Message Access Protocol)**电子邮件服务器检索电子邮件标准通信协议、**IP (Internet Protocol)**、**IP Address (Internet Protocol Address)**每一台计算机的一个独特的识别号码（如197.123.22.240）、**JSP (Java Server Pages)**基于Java技术允许在网页中插入服务器可执行的脚本、**LAN (Local Area Network)**局部地区（如建筑物内）的计算机之间的网络、**OS (Operating System)**、**Page Views**、**PDF (Portable Document Format)**、**Ping**、**Search Engine**、**TCP (Transmission Control Protocol)**、**TCP/IP (Transmission Control Protocol / Internet Protocol)**两台计算机之间的互联网通信协议的集合。 TCP协议是两台计算机之间的自由连接，而IP协议负责通过网络发送的数据包。、**URI (Uniform Resource Identifier)**用来确定在互联网上的资源。 URL是一种类型的URI。、**URL (Uniform Resource Locator)**Web地址。标准的办法来解决互联网上的网页文件（页）（如：http://www.w3cschool.cc/）、**VPN (Virtual Private Network)**两个远程站点之间的专用网络，通过一个安全加密的虚拟互联网连接（隧道）、**Web Services**软件组件和Web服务器上运行的应用程序、搜索引擎优化（Search Engine Optimization）SEO、提高一个网站在搜索引擎中的排名（能见度）的过程、百度搜索网站登录口、



--------

### 关于「Life」：

- ##### 礼貌的笑了笑，寸步不让

  > 老板，枉为人师！


- 别人一辈子都达不到的，才是价值的表彰

  > 翟墨，朗读 - 高尔基的《海燕》


- **结果**=**80%**(100%**想法**+100%**实施**)  

  > 打仗预留军，工作多思考；

  > **保留点实力，摸底很被动**；

  >主意跟我走，大家同向前。
  >

  - 有感于：[在职场里，看起来「尽全力」地工作是一件很蠢的事情吗](https://www.zhihu.com/question/60708921/answer/179744908?utm_medium=social&utm_source=wechat_session)  Jun 6

- > **「高考」光环已逝去，   「阶层」流动使命完。**

  > **「专业」紧随生产力，     未料想「已弃」高考。**
  >

  - 有感于：[高考40年，阶层分流的历史使命早就已经结束了](https://mp.weixin.qq.com/s?__biz=MzI0NzA3MTM5NQ%3D%3D&mid=2650557246&idx=1&sn=491c608e588acfdc6eaa05ddc4cccb9d#wechat_redirect) Jun 7




------
### 关于「基础知识」：如下：


#### 1.刷题

- 目的分析：基础知识的掌握牢固， 「 **基础熟练决定你的灵活性！」**
- 解决办法：大量做题，**「 不比别人熟练，必败」**
- 可能方式：**[纽克](https://www.nowcoder.com/7037691)、[柒越](https://www.julyedu.com)**

#### 2.简历

- 目的分析：让人接受你的努力， **「 被认可，被膜拜，这是主流！」**
- 时间节点：**6月底**

#### 3.经验

- 目的分析：系统性质的项目过程， **「 灵活的经验才能被主流朝拜！」**
- 解决办法：**实战**项目、开源项目， **「 片段化不系统，机会抓不住」**
- 可能方式：**[豪洞悉](http://geek.ai100.com.cn)、[艾科科](http://tinyletter.com/fly51fly/archive)、工种薅**、**英语**

---

### 关于「已完成项目／比赛」：

Dell EMC比赛：https://github.com/alare/Hackathon_2017

比赛硬件：树莓派， 虚拟机

比赛需要但未完成：Docker相关

比赛指南：[Introduction](https://github.com/alare/Hackathon_2017/blob/master/documentation/Mars-challenge-instructions.md)

比赛得分：[PointsGet](https://github.com/alare/Hackathon_2017/blob/master/documentation/Mars-challenge-points-table.md)

1.第一部分：传感器获取数据

1.1.工作：后台运行多个传感器go文件并集成。并在浏览器查看传感器的传入数据。

1.2.工具：后台运行go命令：`nohup go run flare.go &` 。端口如下：0.0.0.0:9000（localhost）。环境变量env

1.3.参考：[Setting up the Raspberry Pi Sensors](https://github.com/alare/Hackathon_2017/blob/master/documentation/Raspberry-Go-Weather-Simulator-Setup.md)   以及  [SensorSuite](https://github.com/alare/Hackathon_2017/tree/master/sensorsuite) 

2.第二部分：平台策略执行

2.1工作：在浏览器显示出监控数据，需要运行docker

2.2工具：[Game Controller](https://github.com/alare/Hackathon_2017/tree/master/game-controller) 以及[Dashboard](https://github.com/alare/Hackathon_2017/tree/master/dashboard) 以及 [Testing the Command and Control Center](https://github.com/alare/Hackathon_2017/blob/master/documentation/Mars-challenge-instructions.md#testing-the-command-and-control-center)

3.第三部分：各个队伍PK

3.1工作：更改策略，使得最后存活时间更长，注意更改url以进行队伍pk

3.2工具：[Team_Strategy](https://github.com/alare/Hackathon_2017/tree/master/clients) 。[Testing the Command and Control Center](https://github.com/alare/Hackathon_2017/blob/master/documentation/Mars-challenge-instructions.md#testing-the-command-and-control-center)

比赛过程：**「晚上10点的没有的微信」**— 那时候，很辛苦

 ![going](YangQiDaily/going.jpg)

比赛结果：「运气比较好，挑战了清华、北大」

 ![Hackathon](YangQiDaily/Hackathon.jpg) 



![FirstPrize](YangQiDaily/FirstPrize.jpg)


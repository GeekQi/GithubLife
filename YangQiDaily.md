注：为个人学习总结需要，如有侵权，请您指出。[持续更新， 如有错误，请您指出]        

​    

 ##                      **「礼貌的笑了笑，寸步不让」** 

-----

关于「学术论文／研究项目／工程项目／创业项目」：

### 关于「专业书」：

### 关于「练手到熟练项目」：1%

Java基础：参考：

##### **Web项目:**

- Python 爬虫：参考：[Scrapy](http://wiki.jikexueyuan.com/project/scrapy/)、


- Java Web：参考：[Java Web](http://wiki.jikexueyuan.com/project/java-web/)、
- PHP+MySQL搭建网页：参考：[PHP+MySQL](http://wiki.jikexueyuan.com/project/php-and-mysql-web/) 、

##### **数据挖掘／自然语言处理项目：**

- 建模

##### **算法基础：**

- 数据结构：参考：[剑指Offer](http://wiki.jikexueyuan.com/project/for-offer/) 、[LeetCode](http://wiki.jikexueyuan.com/project/leetcode-book/)
- 算法：参考：

##### **大数据项目：**

- Spark、Scala

### 关于「论文」：1%

数量：

##### [基于关键词及问题主题的问题相似度计算](http://chuansong.me/n/1906250351625) ：

- 社区问答系统、判断问题相似度、推荐问题的答案、避免重复提问。
- 问题包括：问题主题及问题描述
- KT-CNN模型包括：
  - 关键词抽取：
    - 对输入问题S和T,先预处理在通过此模块抽取S和T的关键词序列Ks和Kt
      - **需要：**得分排序序列、TextRank算法、无向有全图、图排序以及选取关键词、TF-IDF
  - 基于关键词相似相异的问题建模:
    - 利用Ks和Kt间相似相异信息，对S和T建模得到特征向量Fs和Ft
      - **需要**：基于文本间相似及相异信息的CNN模型、词向量表示、GloVe模型、语义匹配：相似矩阵、余弦相似度、皮尔森相关系数、矩阵分解、CNN模型卷积层、CNN模型最大池层、
  - 计算主题相似度:
    - 对问题S和T的主题Ts和Tt计算相似度Sim_topic
      - **需要**：向量表示、皮尔森相关系数
  - 问题相似度计算:
    - 基于问题S和T的特征向量Fs和Ft及主题相似度Sim_topic计算S和T的相似度Sim_q
      - **需要**：线性模型的加权相加

##### [人机对话系统中基于关键词的回复生成技术](http://chuansong.me/n/1873214451926)：

- 早期回复生成技术：基于规则：文本分析规则。／／ 序列决策问题：马尔可夫决策
- 回复生成问题：Seq2Seq深度学习框架：用户消息和回复被建模成两个序列，通过大规模训练数据训练模型学习两个序列间的映射关系。
  - 极大似然估计方法的交叉熵损失函数、最大互信息损失、随机变量
- 使用关键词增强生成回复的相关性
  - Seq2BF（sequence to backward and forward sequences）模型
  - 分为两阶段：根据用户消息在词表中选取和消息具有最大互信息的词作为关键词，根据关键词预测回复的剩余部分。
  - 简要介绍：Seq2Seq 框架下基本的回复生成模型：
    - 两部分之encoder：负责将消息编码成一个模型的内部表示
    - 两部分之decoder：在encoder输出条件下，从特定字符开始，逐字生成完整的回复内容。

##### [文本生成概述](http://chuansong.me/n/1851499951313)：

- 根据格式化数据或自然语言文本生成新闻等可解释文本、定义、任务、评价指标、实现方法、数据驱动方法
- 文本生成定义：接受非语言形式的信息作为输入，生成刻度的文字表示。数据到文本的生成。
- 文本生成任务：文本到文本、数据到文本、图像到文本
  - 文本到文本：
    - 文本摘要：
      - 抽取式摘要：信息抽取和规划等主要步骤
        - 主题模型、聚类、SVR(Support Vector Regression)、线性回归、抽取名词短语、动词短语、随机森林、
      - 生成式摘要
- 文本生成方法：基于规则、基于规划、数据驱动
  - 基于语言模型的自然语言生成：n-gram
  - 使用深度学习的自然语言生成

##### [事件演化的规律和模型](http://chuansong.me/n/1835019551922)：

- 事件图谱、图结构、马尔可夫逻辑网络（无向图）、贝叶斯网络（有向五环图）。事理图谱（有向有环图）。
- 事理图谱的定义：
  - 事件：用抽象、泛化的谓词短语来表示。：不关注时间、地点等
  - 事件间顺承关系：两个时间在时间上先后发生的关系。
  - 事件因果关系：满足顺承关系时序约束的基础上，两个事件件有很强的因果性，强调前因后果。
  - 事理图谱**Event Evolutionary Graph** ：描述事件之间顺承、因果关系的事理烟花逻辑有向图。以某个事件节点进行广度优先搜索，扩展得到事件演化链条。
    - 事理图谱是一种概率有向图。概率图模型的贝叶斯网络、马尔可夫逻辑网络、贝叶斯用有向无环图表达变量节点间的条件依赖与独立性关系。马尔可夫随机场采用无向图表达变量间的相互作用关系。

##### [阿里自然语言处理部总监：NLP技术的应用及思考](http://chuansong.me/n/1810576651425) ：

郎君博士，哈工大社会计算与信息检索研究中心博士毕业生，目前为阿里巴巴iDST自然语言处理部总监

- 计算平台、业务层、NLP4大经典AI完全难题：问答、复述、文摘、翻译。
- 阿里需要：技术体系及服务、核心业务快速增长、商业机会
  - 内容搜索、内容推荐、评价、问答、文摘、文本理解
  - 商品搜索、推荐、智能交互、翻译、广告、风控、舆情监控、
- 词法分析：分词、词性、实体：
  - Bi-LSTM-CRF、多领域词表
  - 推荐算法、蚂蚁金服、资讯搜索
- 句法分析：依存句法分析、成分句法分析：
  - Shift-reduce、graph-based、Bi-lSTM
  - 资讯搜索、评价情感分析、商品标题、搜索Query
- 情感分析：情感对象、属性、属性关联
  - 情感辞典挖掘、属性级、句子级、篇章级情感分析
  - 商品评价、问答、品牌舆情、互联网舆情
- 句子生成：句子可控改写、句子压缩
  - Beam Search、 Seq2Seq+Attention
  - 商品标题压缩、资讯标题改写、PUSH消息改写
- 句子相似度：浅层相似度、语义相似度
  - Edit Distance、Word2Vec、DSSM
  - 相似问题、商品重发检测
- 文本分类／聚类：垃圾防控、信息聚合
  - ME, SVM, FastText
  - 商品类目预测、问答意图分析、文本垃圾过滤、舆情聚类、名片OCR后语义识别
- 文本表示：词向量、句子向量、篇章向量、Seq2Seq
  - Word2Vec、LSTM、DSSM、Seq2Seq
- 知识库：电商同义词／上下位、通用同义词／上下位。领域词库、情感词库
  - bootstrapping、click-through mining、word2vec、k-means、CRF
  - 语义归一、语义扩展、Query理解、意图理解、情感分析
- 语料库：分词、词性标注数据、依存句法标注数据
- 标题分析：分词、实体打标、热度计算、中心识别
- 评价系统：treelink模型、maxent模型、贝叶斯模型、dbn模型总体融合
- 决策购买问题：产品化：分为四类：
  - 无效问题过滤：
    - 分类采用LR+GBDT：定制特征
  - 相似问题识别：
    - Doc2Vec 计算相似度、人工评测
  - 页面问答排序
    - 内容丰富度、点赞数、过滤词表匹配数等加权求和、CTR提升
  - 智能分发

##### [基于深度多任务学习的自然语言处理技术](http://chuansong.me/n/1768183851425) 

- 统计自然语言处理依赖标注数据
- 多任务学习Multi-task Learning：
  - 有监督学习Supervised Learning， 利用人工标注的训练数据进行学习
  - 归纳迁移机制，基本目标是提高泛化性能。利用并行训练的方法学习多个任务。 多任务学习的基本假设是多个任务之间具有相关性。
  - 词法、句法、语义分析等多任务，之间存在紧密的内在联系
- 深度多任务学习：
  - 深度学习：建立在含有多层非线性变换的神经网络结构之上，对数据进行抽象表示和学习的一系列机器学习算法。

##### [对话系统的Goal Oriented和Task Oriented 概念的异同](http://chuansong.me/n/1758705951350)

- 人机对话系统：
  - 开放域对话系统：闲聊：微软小冰。 百度问答：度秘。
  - 任务型对话系统：设备控制：Siri。公交线路查询、餐厅预订。

##### [自然语言处理中的知识获取](http://chuansong.me/n/1724644951614)

- 各个行业：教育、医疗、法律等知识服务型行业
- 自然语言中任何问题抽象为：如何从形式与意义的多对多映射中，根据语境选择一种正确的映射。
- 自然语言处理中知识获取的三要素：
  - 显性知识：
    - 元知识，如WordNet、知识图谱
  - 数据：
    - 带标注数据、无标注
  - 学习算法：
    - SVM、CRF等浅层学习模型，人工定义的特征模版抽取特征及特征组合，结合
    - RNN、CNN等深度学习模型，自动学习有效特征及特征组合的能力
- 大数据和深度学习相互依赖的：
  - 一方面大数据需要复杂的学习模型。长尾数据、复杂模型、大数据
  - 另一方面深度学习需要大数据。神经网络机器翻译（NMT）已经迅速超越统计机器翻译（SMT）、端到端
    - 信息抽取两种做法：
      - 1先做句法分析，再做信息抽取
      - 2直接信息抽取，也就是“端到端”，也是分层的

##### [迁移学习：基本概念到相关研究](http://chuansong.me/n/1716697451319)

- 什么是迁移学习：

  - 机器学习的监督学习场景中，如果针对任务和域A训练一个模型，我们假设被提供了此标签数据。
  - 迁移学习借用已存在的相关任务或域的标签数据来处理场景。尝试把源域中解决源任务时获得的知识存储下来，应用到目标域的目标任务中。

- 为什么需要迁移学习：

  - Andrew Ng：迁移学习将会是继监督学习之后的下一个机器学习商业成功驱动力。
  - ml在产业界的应用和成功主要：监督学习的驱动。
  - 无监督学习是实现通用人工智能的关键成分。
  - 产业界对ml的应用分2类：
    - 一方面：先进模型
    - 另一方面：大量标签数据使得模型成功
  - 迁移学习可以帮助我们处理全新的场景，是ml在没有大量标签任务和域中规模化应用所必须的。

- 迁移学习定义

  - 给定一个源域 Ds，一个对应的源任务 Ts，还有目标域 Dt，以及目标任务 Tt，现在，迁移学习的目的就是：在 Ds≠Dt，Ts≠Tt 的情况下，让我们在具备来源于 Ds 和 Ts 的信息时，学习得到目标域 Dt 中的条件概率分布 P（Yt|Xt）

- 迁移学习场景

  - 给定源域和目标域 Ds 和 Dt，其中，D={X,P(X)}，并且给定源任务和目标任务 Ts 和 Tt，其中 T={Y,P(Y|X)}。源和目标的情况可以以四种方式变化

- 迁移学习的应用

  - 从模拟中学习：google自动驾驶、机器人等

- 迁移学习方法

  - 使用预训练CNN特征
  - 理解卷积神经网络、全连接层


##### [如何让人工智能学会用数据说话](http://chuansong.me/n/1668814251914)

- 基于结构化数据的的文本生成
  - 机器翻译、文本摘要、诗词生成等都属于文本生成的范畴：
    - 共同点：用户输入非结构化文本，机器根据目标输出相应文本
- 结构化文本生成：特点：基于数据和事实
- 文本生成的典型商业应用：财经、体育类新闻报道的生成、产品描述、商业数据的分析和解释、物联网数据的分析。比如：天气预报自动生成。
- 文本生成的技术发展：
  - 对选定的数据记录，用自然语言描述出来
- 方法：
  - 早期：基于规则：三个独立的模块：
    - 内容规划(Content planning), 即选择描述那些数据记录或数据域
    - 句子规划(Sentence planning), 即决定所选择的数据记录或数据域在句子中的顺序
    - 句子实现(Surface realization), 即基于句子规划生成实际的文本。
  - 基于神经网络的方法：
    - 基于神经语言模型(Neural Language Model)
    - 基于神经机器翻译(Neural Machine Translation)
    - Semantic Controlled LSTM（Long Short-term Memory）模型:用于文本生成：在LSTM基础上引入了控制门读取结构化数据信息。
- 数据：
  - 天气预报、维基百科人物传记、基于对话的人机对话数据集

##### [卷积神经网络在句子分类上的应用](http://chuansong.me/n/1723633)

- 基于预先训练的词向量而训练的卷积神经网络(CNN-Convolutional Neural Networks)、文本分类、语义分析、词向量word vectors 
- 卷积神经网络CNN利用一个卷积层进行特征提取，最初CV，后来在nlp也得到应用，如语义分析、搜索短语检索、句子分类
- 特征向量使用max-over-time池化、word2vec词向量、

##### [自然语言中的Attention Model](http://chuansong.me/n/2215468)

- 先说Encoder-Decoder框架
  - AM模型基本附着在Encoder-Decoder框架下的。但本身并不依赖于Encoder-Decoder模型：适合处理由一个句子生成另一个句子的通用处理模型。
  - 具体使用什么模型自己定：CNN/RNN/BiRNN/GRU/LSTM/Deep LSTM
- AM：以上的还没有体现出“注意力模型”。

##### [深度学习：推动NLP领域发展的新引擎](http://chuansong.me/n/2793709)

- Word Embedding: word2vec能把词变成向量：忽略词之间的关系(句法关系)、词的顺序等
  - 引入词的关系：Dependency Parser：把抽取出来的Relation作为词的Context
  - 改进Bag of Words:
  - 外部资源和知识库：word2vec只用了词的上下文共现， 没有用外部资源如词典知识库
- RNN／LSTM／CNN
  - RNN相关的模型如LSTM基本上算法是解决结构化问题的标准。比普通的FeesForward Network, RNN 有记忆能力
  - 普通的神经网络只会在学习的时候“记忆”，也就是反向传播算法学习参数。然后不“记忆”了。训练好之后，不管什么时候来一个相同的输入，输出一样。对于image classification没问题，但是speech recognition等nlptask，数据有时序或者结构的。
  - RNN具有“记忆”能力。前一个输出会影响后面的判断。比如：前一个He,后面出现is的概率比are高很多。
  - 最简单RNN直接把前一个时间点的输出作为当前出入的一部分，有梯度消失的问题。比较流行的改进如LSTM和GRU等模型通过gate开关，判断是否需要遗忘／记忆之前的状态，以及当前状态是否需要输出到下个时间点。比如语言模型。
  - CNN最早图像。通过卷积发现位置无关的feature，而且这些feature的参数相同，
    - machine translation、语义角色标注Sematic Role labeling
- Multi-model deep learning
- Reasoning, Attention and Memory
  - RNN/LSTM是模拟人类大脑的记忆机制，但除了记忆之外，Attention也是非常有用的机制

##### [深度学习浪潮中的自然语言处理技术](http://chuansong.me/n/472336251048)

- 目前，机器学习技术为自然语言的歧义、动态性等提供了可行的解决方案，成为研究主流，称为统计自然语言处理。
- 一个统计自然语言处理系统通常由2部分构成：
  - 训练数据（样本）
  - 统计模型（算法）
- 但是传统的机器学习方法在数据获取和模型构建等方面存在严重问题：
  - 大规模标注数据难以获得，带来了严重的数据稀疏问题。
  - 需要人工设计模型所需的特征及特征组合。需要深刻理解和丰富经验
- 基于深度学习的自然语言处理
  - 建立在含有多层非线性变换的神将网络结构之上，对数据的表示进行抽象和学习的一系列机器学习算法。
  - 深度学习为自然语言处理的研究主要带来两方面变化：
    - 一方面使用统一的分布式(低维、稠密、连续)向量表示不同粒度的语言单元，如词、短语、篇章
    - 一方面使用循环、卷积、递归等神经网络模型对不同的语言单元向量进行组合，获得更大语言单元的表示
  - 分布式表示：
    - 深度学习最早在nlp的应用是神将网络语言模型：基本假设低维、稠密、连续的向量表示词汇，又称为分布式词表示(Distributed Word Representation)或词嵌入(Word Embedding)。 可以将相似的词汇表示为相似的向量。
    - 理论上，将原有高维、稀疏、离散的词汇表示方法（One-hot表示）映射为分布式表示是一种降维方法，可有效克服机器学习的维度灾难（Curse of Dimensionality）问题，从而获得更好的学习效果。
  - 语义组合(Semantic Composition)
    - 分布式词表示的思想可以进一步扩展，可通过组合（Composition）的方式来表示短语、句子甚至是篇章等更大粒度的语言单元。
    - 三种神经网络结构实现不同的组合方式
      - 循环神经网络（顺序组合）RNN，Recurrent Neural Network
        - 从左至右顺序的对句子中的单元进行两两组合：“我”和“喜欢”组合，生成隐层h1。将h1和“红”组合，生成h2.类推。传统的RNN存在严重梯度消失（Vanishing Gradient）或者梯度爆炸（Exploding Gradient）问题。
        - 深度学习中一些常用的技术，如使用ReLU激活函数、正则化以及恰当的初始化权重参数等都可以部分解决这一问题
        - 另一类更好的解决方案是减小网络的层数，以LSTM和GRU等为代表的带门循环神经网络（Gated RNN）都是这种思路，即通过对网络中门的控制，来强调或忘记某些输入，从而缩短了其前序输入到输出的网络层数，从而减小了由于层数较多而引起的梯度消失或者爆炸问题
      - 卷积神经网络（局部组合）CNN，Convolutional Neural Network
        - 隐含层神经元只与部分输入层神经元连接，同时不同隐含层神经元的局部连接权值共享。如评论文本分类，最终褒贬性由局部短语决定，且与顺序无关。
        - 由于存在局部接收域性质，各个隐含神经元的计算可以并行的进行，这就可以充分利用现代的硬件设备（如GPU），加速卷积神经网络的计算，这一点在循环神经网络中是较难实现的
      - 递归神经网络（句法结构组合）RecNN，Recursive Neural Network
        - 首先对句子进行语法分析，将结构转化为树状结构，构建深度神经网络。
  - 很多自然语言任务如对话生成，有赖于更大的上下文或语境，传统的基于人工定义特征的方式很难对其进行建模，深度学习模型则提供了一种对语境进行建模的有效方式 
  - 无论何种神经网络模型，都是基于固定的网络结构进行组合，传统的有监督学习框架很难实现该目标，而强化学习（Reinforcement Learning）框架为我们提供了一种自动学习动态网络结构的途径。

[基于深度学习的关系抽取](http://chuansong.me/n/831970551568)

- 信息抽取旨在从大规模非结构或半结构的自然语言文本中抽取结构化信息。关系抽取是其中的重要子任务之一，主要目的是从文本中识别实体并抽取实体之间的语义关系。
- 现有主流的关系抽取技术分为有监督的学习方法、半监督的学习方法和无监督的学习方法三种
  - 有监督的学习方法将关系抽取任务当做分类问题，根据训练数据设计有效的特征，从而学习各种分类模型，然后使用训练好的分类器预测关系。该方法的问题在于需要大量的人工标注训练语料，而语料标注工作通常非常耗时耗力
  - 半监督的学习方法主要采用Bootstrapping进行关系抽取。对于要抽取的关系，该方法首先手工设定若干种子实例，然后迭代地从数据从抽取关系对应的关系模板和更多的实例
  - 无监督学习方法假设拥有相同语义关系的实体对拥有相似的上下文信息。因此可以利用每个实体对对应上下文信息来代表该实体对的语义关系，并对所有实体对的语义关系进行聚类
  - 有监督的学习方法能够抽取更有效的特征，其准确率和召回率都更高。因此有监督的学习方法受到了越来越多学者的关注
- 基于有监督学习的关系抽取
  - 需要大量人工标注的训练数据，从村里安数据中自动学习关系对应的抽取模式。
  - 远程监督
- 基于深度学习的关系抽取
  - 有监督的依赖标注等分类特征，并且存在错误
  - 递归神经网络解决关系抽取问题
  - 卷积神经网络关系抽取
  - 基于端到端神经网络的关系抽取模型：双向LSTM(Long-Short-Term-Memory)
- 总结及未来趋势
  - 基于句法树的树形LSTM神经网络模型在关系抽取上取得了不错的效果
  - 目前的神经网络关系抽取主要用于预先设定好的关系集合。而面向开放领域的关系抽取，仍然是基于模板等比较传统的方法


##### [基于深度学习的依存句法分析](http://chuansong.me/n/710400151780)  

- 句法分析：句子从词语的序列形式按照语法体系转化为图结构（树结构）。以刻画句子内部的句法关系（主谓宾）。使用依存户连接句子中两个具有一定句法关系的词语，最终形成一颗句法依存树。
- 主流依存句法分析方法：
  - 基于图（Graph-based）
    - 将依存句法分析看成从完全有向图中寻找最大生成树的问题，图的两边表示两个词之间存在某种句法关系的可能性
  - 基于转移（Transition-based）
    - 通过规约等转移动作构建一棵依存句法树，学习目标是寻找最优动作序列
- 深度学习技术：
  - 建立在含有多层非线性变换的神经网络结构之上，对数据抽象和学习的一系列机器学习算法。
- 基于转移的依存句法分析方法：
  - 一系列由初始到终止的状态（State或Configuration）表示句法分析的过程。一个状态由栈（Stack）、缓存（Buffer)以及部分分析好的依存弧构成。栈存储已经分析的词，缓存表示待分析的词
  - 学习一个分类器，输入状态，输出该状态下最可能动作：贪心解码算法
  - 抽取出特征后，传统方法：采用线性分类器，即将特征进行线性加权组合，结合系数为分类器学习获得的权重，选取分数最高的类别作为采取的动作
- 基于深度学习的依存句法分析：
  - 贪心解码算法：先从一个状态提取一些重要核心特征，与传统高维、稀疏、离散向量One-hot表示不同，使用低维、稠密、连续的分布式向量来表示特征。
    - 相似的词可用相似的向量表示：克服数据稀疏问题
    - 分布式表示是一种降维方法，克服维度灾难（Curse of Dimensionality）
  - 全局解码算法：
    - 用柱搜索（Beam Search）等近似全局搜索／解码算法总和考虑多个状态之间的依赖关系。
- 目前还需要利用传统方法构造转移系统，同时全局解码算法有助于系统性能的进步提升
  - 序列到序列（Sequence to Sequence），也称编码解码（EncoderDecoder）方法在多个自然语言处理任务中广泛应用：机器翻译、阅读理解等。
  - 依存句法是典型的结构话学习问题，在nlp中，很多结构话学习任务：分词、词性标注等，都可以借鉴上述基于转移的算法框架解决。
  - 很多上层应用依赖于句法分析。尤其是深度学习中递归神经网络方法就是依赖句法分析结果进行语义的递归组合。

##### [深度学习的五个挑战和其解决方案](http://chuansong.me/n/1664863851518)

- 机器学习的各个主要方向，从底层的深度学习分布式机器学习平台(AI的Infrastructure)到中层的深度学习、强化学习、符号学习算法以及再上面的机器学习理论。
- 新的基于CNN的深度模型叫做残差网络，这个残差网络深度高达152层，取得了当时图象识别比赛上面最好的成绩
- 深度学习里最经典的模型：
  - 全连接的神经网络，就是每相临的两层之间节点之间是通过边全连接；
  - 再就是卷积神经网络，这个在计算机视觉里面用得非常多；
  - 再就是循环神经网络RNN，这个在对系列进行建模，例如自然语言处理或者语音信号里面用得很多，这些都是非常成功的深度神经网络的模型。
  - 还有一个非常重要的技术就是深度强化学习技术，这是深度学习和强化学习的结合，也是AlphaGo系统所采用的技术
- 当前深度学习的一个前沿就是如何从无标注的数据里面进行学习。现在已经有相关的研究工作，包括最近比较火的生成式对抗网络，以及我们自己提出的对偶学习。
  - 生成式对抗网络的主要目的是学到一个生成模型
  - 它是同时学习两个神经网络：一个神经网络生成图像，另外一个神经网络给图像进行分类，区分真实的图像和生成的图像。在生成式对抗网络里面，第一个神经网络也就是生成式神经网络，它的目的是希望生成的图像非常像自然界的真实图像，这样的话，那后面的第二个网络，也就是那个分类器没办法区分真实世界的图像和生成的图像；而第二个神经网络，也就是分类器，它的目的是希望能够正确的把生成的图像也就是假的图像和真实的自然界图像能够区分开。大家可以看到，这两个神经网络的目的其实是不一样的，他们一起进行训练，就可以得到一个很好的生成式神经网络
  - 针对如何从无标注的数据进行学习，我们组里面提出了一个新思路，叫做对偶学习。对偶学习的思路和前面生成式对抗学习会非常不一样。对偶学习的提出是受到一个现象的启发：我们发现很多人工智能的任务在结构上有对偶属性
    - 搜索引擎最主要的任务是针对用户提交的检索词匹配一些文档，返回最相关的文档；当广告商提交一个广告之后，广告平台需要给他推荐一些关健词使得他的广告在用户搜索这些词能够展现出来被用户点击
- 深度学习面临的第二个挑战就是如何把大模型变成小模型
  - CNN模型，也就是卷积神经网络，做模型压缩；
    - 剪枝：边权重小的去掉
    - 权值共享：上百万权值聚类，用均值代替这一类权值
    - 量化：降低浮点型精度
    - 二进制神经网络：原来32bit权值现在1bit
  - 针对一些序列模型或者类似自然语言处理的RNN模型如何做一个更巧妙的算法，使得它模型变小，并且同时精度没有损失。
    - 新的循环神经网络LightRNN：不是模型压缩降低模型大小，而是算法
    - 每个词要做词嵌入（word embedding）语义相似或相近的词在向量空间里的向量也比较接近，这样可以表达词之间语义信息或相似性。
    - 我们不用一个向量表示一个词，而是两个向量表达一个词。行／列
- 今年的人工智能国际顶级会议AAAI 2017的最佳论文奖，颁给了一个利用物理或者是一些领域的专业知识来帮助深度神经网络做无标注数据学习的项目。论文里的具体例子是上面这张图里面一个人扔枕头的过程，论文想解决的问题是从视频里检测这个枕头，并且跟踪这个枕头的运动轨迹。如果我们没有一些领域的知识，就需要大量的人工标注的数据，比如说把枕头标注出来，每帧图像的哪块区域是枕头，它的轨迹是什么样子的。实际上因为我们知道，枕头的运动轨迹应该是抛物线，二次型，结合这种物理知识，我们就不需要标注的数据，能够把这个枕头给检测出来，并且把它的轨迹准确的预测出来。这篇论文之所以获得了最佳论文奖，也是因为它把知识和数据结合起来，实现了从无标注数据进行学习的可能
- AI技术来分析股票：动态决策性问题：难点：时变性
- 决策第二点：各种因素相互影响：静态认知性任务我们的预测结果不会对问题

##### [理解LSTM网络](http://chuansong.me/n/1756021)

- Recurrent Neural Networks
  - 传统神经网络不能处理的问题：使用先前的事件推断后续的事件
  - RNN解决了这个问题：包含循环的网络：允许信息的持久化
  - 同一神经网络的多次复制，每个神经网络模块把消息传递给下一个
  - 链式特征揭示了RNN本质上与序列和列表相关的
  - RNN的语音识别、建模、翻译、图片描述等
  - 应用成功的关键之处：LSTM的使用，是一种特别的RNN，比标准的RNN在很多任务都表现的更好。
- 长期依赖问题：
  - RNN的关键：用来连接先前的信息到当前的任务上，还有很多依赖因素
  - 有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如：语言模型基于当前词语来预测下一个词。如果预测“the clouds are in the sky”最后的词，我们并不需要任何其他的上下文--因此下一个词应该是sky。RNN可以学会使用先前的信息。
  - 但是更加复杂的场景下：假设我们试着去预测“I grew up in France… I speak fluent French”最后的词，相关信息和当前预测位置之间的间隔变得相当大。当间隔大的时候，RNN会丧失学习到连接如此远的信息的能力。
- LSTM网络-Long Short Term 网络
  - RNN特殊的类型。可以学习长期依赖
  - 通过刻意的设计避免长期依赖问题。记住长期的信息在实践中是LSTM的默认行为
  - 所有RNN都具有一种重复神经网络模块的链式形式，标准RNN中，重复的模块只有一个简单结构：例如tanh层。
  - LSTM同样如此：但是重复的模块有不同的结构
- LSTM的核心思想：
  - 关键：细胞状态：水平线在图上方贯穿运行。
  - LSTM有通过精心设计的称为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。包含一个sigmoid神经网络层和一个pointwise 乘法操作。Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”！
- 逐步理解LSTM：
  - 第一步决定丢弃什么信息。通过忘记门层完成。
  - 下一步决定什么样的新信息被存在细胞状态中。
    - 第一，sigmoid层称“输入门层”决定什么值我们将要更新。
    - 根据信息产生对状态的更新
    - 最后确定输出什么值。这个输出基于我们的细胞状态。
- LSTM的变体
  - GRU

##### [浅谈基于张量分解的隐变量模型参数学习](http://chuansong.me/n/1805856)

- 很多工作使用张量分解学习隐变量模型的参数，可以获得参数的全局最优解
- 隐变量模型本质上一个定义在两类变量上的概率分布：隐含变量和观测变量：隐变量模型的两个最基本任务：
  - 给定观测变量，推断隐含变量，在概率图模型：infernce
  - 估计模型中参数：learning
- 学习概率分布的参数：通常方法；最大似然估计、矩估计。
  - 隐变量模型的参数学习使用最多的是最大似然估计：给定数据，写出似然函数，优化似然函数，估计参数。：参数估计的主流方法。 缺点：隐变量模型的似然函数基本上都是非凸函数，很难获得全局最优解。优化似然函数的常用方法：EM、variational EM通常只能获得局部最优解。
  - 为了获得参数的全局最优解：矩估计：基于张量分解的方法就属于此类：基本思想：求样本的低阶矩，通过解方程得到参数的值。求解一阶矩和二阶矩。但是方程不一定能写出来。张量方法目前只适用于某些隐变量模型，隐马尔可夫模型，话题模型，高斯混合模型，并不像最大似然方法普遍适用于任何模型
  - 张量分解本质上：优化问题：非凸优化问题。如果这个非凸优化中得到的是局部最优解。但是如果张量是对称的。尽管非凸优化，也能获取全局最优解。

##### [人工智能之争](http://chuansong.me/n/1792248)

- 传统人工智能：自然语言翻译、符号推理（symbolicreasoning）、博弈论（game playing）等问题：专家系统：运用规则记录专家的经验：医生诊断经验的模型
- 人机交互human computer interaction：图形用户界面
- 机器学习：数理统计工具开发不同的识别和分类算法:识别目标、发现数据中模式、用于机器人的策略

##### [Facebook人工智能研究最新进展](http://chuansong.me/n/1897593)

- 最大挑战：无监督学习
  - 因为人类和动物使用对多的就是这种学习方式

##### [知识图谱的应用](http://chuansong.me/n/1999287)

- 什么是知识图谱
  - 本质：语义网络：基于图的数据结构：节点（point）和边（edge）组成
  - 每个节点：表示现实的实体。每条边：实体与实体之间的“关系”
  - 知识图谱：关系的最有效的表示方式，把所有不同种类的信息连接在一起的到的关系网络，知识图谱提供了从“关系”角度去分析问题的能力。
  - 最初：用来优化现有的搜索引擎，不同于关键词搜索的传统搜索引擎，知识图谱可用来更好的查询复杂的官联系信息。
- 知识图谱的表示
  - 描述：事实。
  - 属性图和传统的RDF格式都可以作为知识图谱的表示和存储方式
- 知识图谱的存储
  - 知识图谱是基于图的数据结构。存储方式主要有两种：
    - RDF存储
    - 图数据库
  - 如果需要设计的知识图谱简单，查询不会涉及到1度以上的关联查询。可以选择用关系型数据存储格式来保存知识图谱。对于复杂的关系网络知识图谱的的优点还是明显的。
  - 把实体和关系存储在图数据结构是一种复合整个故事逻辑的好方式
- 应用
  - 反欺诈：风控：基于大数据的反欺诈难点：不同来源的数据（结构化、非结构化）整合在一起，构建反欺诈引擎，并有效识别出欺诈。涉及到复杂的关系网络。知识图谱，作为关系的直接表示方式，可以很好的解决这两个问题。
  - 构建多数据源的知识图谱
  - 不一致性验证设计到知识推理。可以理解成链路预测，也就是从已有的关系图谱里推导出新的关系或连接。
  - 异常分析：基于图
    - 静态分析：给定图结构和某个点，从中发现一些异常点
    - 动态分析：分析其结构随时间变化的趋势
  - 除了贷前的风险控制，也可以在贷后发挥其强大的作用。
  - 智能搜索
  - 精准营销：结合多种数据源分析实体之间的关系。对用户的行为有更好的理解。
- 数据的噪声
  - 数据本身错误：不一致性验证
  - 数据冗余：涉及“消歧分析”
- 非结构化数据处理能力
  - 数据挖掘、nlp、ml
- 知识推理
  - 人类智能的重要特征：需要规则的支持。常用的推理算法：基于逻辑的推理（Logic）和基于分布式表示方法(Distributed Representation)的推理
- 大数据、小样本、构建有效的生态闭环是关键
  - 面临的依然是小样本问题，也就是样本数量少。实际上，我们能拿到的欺诈样本数量不多，即便有几百万个贷款申请，最后被我们标记为欺诈的样本很可能也就几万个而已
  - 所谓的生态闭环，指的是构建有效的自反馈系统使其能够实时地反馈给我们的模型，并使得模型不断地自优化从而提升准确率。

##### [AlphaGo原理解析](http://chuansong.me/n/2658183)

- 围棋棋盘是19x19路，所以一共是361个交叉点，每个交叉点有三种状态，可以用1表示黑子，-1表示白字，0表示无子，考虑到每个位置还可能有落子的时间、这个位置的气等其他信息，我们可以用一个361 * n维的向量来表示一个棋盘的状态。我们把一个棋盘状态向量记为s
- 当状态s下，我们暂时不考虑无法落子的地方，可供下一步落子的空间也是361个。我们把下一步的落子的行动也用361维的向量来表示，记为a。
- 这样，设计一个围棋人工智能的程序，就转换成为了，任意给定一个s状态，寻找最好的应对策略a，让你的程序按照这个策略走，最后获得棋盘上最大的地盘
- 第一招：深度卷积神经网络：
- 第二：MCTS：蒙塔卡洛搜索树Monte-Carlo Tree Search：
  - 没有任何人工的feature，完全依靠规则本身。靠一种类似遗传算法的自我进化
  - 可以连续运行
- 第三：自我进化，强化学习

[神经网络做唐诗](http://chuansong.me/n/2247902)

- [github地址](https://github.com/GeekQi/tangshi-rnn)
- 神将网络在cv和nlp方面效果非常好，原因有如下：
  - 传统的机器学习建立在统计基础上，但是当数据与数据之间的关系难以用统计描述，传统方法不行
  - 传统ml需要专家知识挑选特征。特征好坏与学习成果有很大关系。
- 神经网络的厉害之处就是克服了以上两点：
  - 一是每个神经元都有非线性的公式，可以得到复杂的难以用数学公式描述的关系
  - 二是语音识别的网络，训练时都是原始数据输入，到结构输出模型（End2End）
- 简单的神经网络有局限：RNN递归神经网络（Recurrent Neural Network）本身限制
  - 每次的输出作为下一次的输入返回到神经网络中。训练神经网络就是把这个过程逆向，从最后的输出开始往之前的输出推，每一次推的时候要用到当时的输入和输出计算梯度。
    - 当把第1000个字符的梯度退回到第1个时间状态有两个问题：
      - 存储999个中间值，吃光内存
      - Vanishing/Exploding Gradient, 太远的gradient可能就太大爆掉，或者太小对神经网络完全没有影响
- 但是近体诗：五言和七言的比较适合这个神经网络
  - 字数少、句式固定、规律明确。
    - 大概$150的AWS的GPU 节点使用费，和一些其他忽略不计的数据处理的各种CPU节点。
    - 一周收集并清理全唐诗。
    - 然后训练的时候走了很多弯路，最后这个model在一个月之后才弄出来的。
    - 处理平仄花了一周。
- LSTM和RNN的对比
  - RNN有两个著名的衍生姐妹，Gated Recurrent Unit和Long-Short-Term Memory。
  - LSTM训练时间长，最后效果好不少
- Word Embedding重要性及词典大小
  - 神经网络每个输入输出都是向量或矩阵。为了能把所有字都转化成向量，用到的技巧就是embedding：每个字变成独特的向量。里面只有有一个1
  - 神经网络的第一层实现功能就是把这个很长的one hot vector 变成一个独特的长度更低的向量
  - 实际使用取了频率最高的2000个字，用34%的独立字符覆盖94%的总体字数
- 加入起始和终止字符，让结构更加明确。^ 表示开始。 $表示结束
- 利用Dropout增加准确性
  - 训练的时候一定数量内部的神经元被随机设置为0，但是训练完了又回复原状。也就是遮挡住一部分特征。比如小孩识别汽车，每次看一部分，但是给张完整的，也能看出是汽车
- 不同SGD方法效果不明显
- 生成字符表是一个概率分布
  - 这个神经网络的生成结构：每次在做预测的时候，并不是给出某一个字符，而是给出整个词典里所有字可能出现的概率。
- 将平水韵作为神经网络的输出过滤
  - 关键是押韵效果不好，需要平仄的地方进行过滤


##### [神经网络十大误解分析](http://chuansong.me/n/329467651436)

- 机器学习中最流行最强大的一类
  - 计量金融中，常被用作时间序列预测、构建专用指标、算法交易、证券分类、信用风险建模等，但是不可靠
- 神经网络更接近曲线拟合（curve fitting）和回归分析（regression analysis）等统计方法。曲线拟合即函数逼近，逼近复杂的数学函数
- 神经网络由互连节点层组成。单个节点称为感知器（perceptron），类似与一个多元线性回归（multiple linear regression）。
  - 多元线性回归和感知器之间的不同之处在于：感知器将多元线性回归生成的信号送给可能线性可能非线性的激活函数中。在多层感知器（MLP）中，感知器按层级排布，层层之间互相连接。
  - MLP中有三种类型的层：输入层、隐藏层、输出层。
    - 输入层接收输入模式而输出层可以包含一个分类列表
    - 隐藏层调整输入的权重，直到将NN的误差降低到最小（另一种解释：隐藏层提取输入数据中的显著特征，这些特征有关于输出的预测能力）
  - 映射输入：输出
    - 感知器接收输入向量。通过感知器的权重向量进行加权。在多元线性回归的背景中，这些可被认为是回归系数或beta系数。使用该总和产物得到净值的神经元被称为求和单元（summation unit）
    - 净输入信号减去偏差theta后被输入一些激活函数f()。激活函数通常是单调递增函数，其值位于 (0,1) 或 (-1,1) 之间，激活函数可以是线性的，也可以是非线性的
    - 最简单的神经网络只有一个映射输入到输出的神经元。对于给定模式 p，该网络的目标是相对一些给定的训练模式 tp 的一些一只的目标值来最小化输出信号 op 的误差。比如，如果该神经元应该映射 p 到 -1，但却将其映射到了 1，那么，根据距离的求和平方测定，神经元的误差为 4，即 (-1-1)^2
  - 分层：
    - 感知器被分层进行组织：第一层：输入层，接受训练集。隐藏层将前一层的输出作为下一层的输入，下一层的输出又作为另一层的输入。隐藏层：提取输入数据中的显著特征。这些特征可以预测输出。这个过程称为特征提取（feature extraction），而且和主成分分析（PCA）等统计技术相似功能。









### 关于「当前系统项目」：**40%**

**1.目标明确**

：任务要明确，稍微改一点点，就是另一个目标了。参考自：[王露平博士](http://see.xidian.edu.cn/html/news/7897.html)

**1.1数据获取**

1.1.1工作：**理解**甲方提供Excel版：数据字典／样本数据：(订单明细、POP信息、POP商家评分、商品评论、商品基础信息等19张表格)，并**存储到mysql数据库中**：「数据导入」「数据库建表」等

1.1.2工具：[Linux基础 - get!](http://www.runoob.com/linux/linux-tutorial.html) 、 [sql基础 - get!](http://www.runoob.com/sql/sql-tutorial.html) 、   [mysql基础 - get!](http://www.runoob.com/mysql/mysql-tutorial.html)  

1.1.3参考：

1.2**数据初步分析**

1.2.1工作：原始19张表的数据分布初步探索「数据均值等分布」

1.2.2工具：[python基础 -- get！](http://www.runoob.com/python/python-tutorial.html) 、pandas

1.2.3参考：

**2.业务理解数据**

2.1**特征维度选择**

2.1.1工作：从原始19张表中结合业务理解，选择特征维度及特征列表

2.1.2工具：Hive。

2.1.3参考：

2.2**特征维度融合**

2.2.1工作：特征列表汇总到**核心目标**业务上的特征维度

2.2.2工具：Hive。

2.2.3参考：

**3.图谱构建**

3.1**图谱构建**

3.1.1工作：依据特征维度，构建图谱

3.1.2工具：Hive

3.1.3参考：

3.2图谱分析

3.2.1工作：基于图谱，提取特征

3.2.2工具：[python基础 -- get！](http://www.runoob.com/python/python-tutorial.html)、NetworkX

3.2.3参考：

**4.单模型特征应用**

4.1**回归模型选择**

4.1.1工作：单模型对提取出的特征进行评估

4.1.2工具：scikit-learn、XGB。

4.1.3参考：

4.2附：文本相似度分析过程：

4.2.1工作：匹配SELF与POP的名称

4.2.2工具：gensim、[python基础 -- get！](http://www.runoob.com/python/python-tutorial.html) 

4.2.3参考：

**5.多层级模型特征应用**

5.1**架构搭建**

5.1.1工作：多模型的架构搭建

5.1.2工具：

5.1.3参考：

-----

### 关于「基础技能」：

> 服务端：

[python基础 - get！](http://www.runoob.com/python/python-tutorial.html) 字符串、列表、字典、元组、IO、异常、多线程、面向对象、正则、MySQL、JSON、100实例

[Java基础 - get!](http://www.runoob.com/java/java-tutorial.html)    Java例子

[Linux基础 - get!](http://www.runoob.com/linux/linux-tutorial.html) 登陆、用户、文件、Vim

[Docker基础 - get!](http://www.runoob.com/docker/docker-tutorial.html)  容器引擎、虚拟化、打包镜像部署、沙盒、虚拟化

[PHP基础 - get！](http://www.runoob.com/php/php-tutorial.html)mysql、XML、AJAX

[正则表达式基础 - get!](http://www.runoob.com/regexp/regexp-tutorial.html)  

[JSP基础 - get!](http://www.runoob.com/jsp/jsp-tutorial.html)  

[Scala基础 - get!](http://www.runoob.com/scala/scala-tutorial.html)  

[设计模式基础 - get!](http://www.runoob.com/design-pattern/design-pattern-tutorial.html)

[Django基础 - get!](http://www.runoob.com/django/django-tutorial.html)  

[Servlet基础 - get!](http://www.runoob.com/servlet/servlet-tutorial.html)  

> 数据库：

[sql基础 - get!](http://www.runoob.com/sql/sql-tutorial.html) 查询、筛选、插入、更新、连接、合并、约束、删除、更改、函数

[mysql基础 - get!](http://www.runoob.com/mysql/mysql-tutorial.html) 匹配、索引、临时表、正则、去重、SQL注入

[MongoDB - get!](http://www.runoob.com/mongodb/mongodb-tutorial.html)  分布式文件存储

[Redis - get！](http://www.runoob.com/redis/redis-tutorial.html)  key-value存储系统

> 开发工具：

[git基础 - get!](http://www.runoob.com/git/git-tutorial.html)  分布式版本控制系统、克隆、修改、提交、工作区、版本库、分支、主线、合并、提交历史、标签、[github](http://www.runoob.com/w3cnote/git-guide.html)

> XML教程：

[XML基础 - get!](http://www.runoob.com/xml/xml-tutorial.html)  可扩展标记语言（e**X**tensible **M**arkup **L**anguage）、传输和存储数据

> Web Service:

[RSS基础 - get!](http://www.runoob.com/rss/rss-tutorial.html)    Really Simple Syndication（真正简易联合）

[RDF基础 - get!](http://www.runoob.com/rdf/rdf-intro.html)   RDF 是一个用于描述 Web 上的资源的框架

> HTML / CSS / JavaScript

[HTML基础 - get!](http://www.runoob.com/html/html-tutorial.html)

[CSS基础 - get!](http://www.runoob.com/css3/css3-tutorial.html)

[Boostrap基础 - get!](http://www.runoob.com/bootstrap/bootstrap-tutorial.html)

[JavaScript基础 - get!](http://www.runoob.com/js/js-tutorial.html)  

[jQuery基础 - get!](http://www.runoob.com/jquery/jquery-tutorial.html)

[AJAX基础 - get!](http://www.runoob.com/ajax/ajax-tutorial.html)  

[JSON基础 - get!](http://www.runoob.com/json/json-tutorial.html) 

[Highcharts基础 - get!](http://www.runoob.com/highcharts/highcharts-tutorial.html)  

> 网站建设：

[HTTP基础 - get!](http://www.runoob.com/http/http-tutorial.html)  HyperText Transfer Protocol、超文本传输协议、TCP/IP传输控制协议/因特网互联协议、客户端-服务端架构、C/S、通讯流程、消息结构、客户端请求消息、服务器响应消息、请求方法、HTTP状态码

[TCP/IP基础 - get!](TCP/IP基础 - get!)   TCP/IP 是因特网的通信协议、-**TCP 使用固定的连接、**IP 是无连接的、**IP 路由器**、**TCP/IP**、**IP 地址包含 4 组数字、**32 比特 = 4 字节、**IPV6**、**域名**、**TCP/IP 是不同的通信协议的大集合**

[网站主机基础 - get!](http://www.runoob.com/hosting/hosting-tutorial.html)  网站、web 服务器、ISP( Internet Service Provider ) Internet 服务提供商、每日的备份、流量限制、带宽或内容限制、域名是网站唯一的名称、主机解决方案中应包括域名注册、DNS 、月流量、POP 指的是邮局协议。、IMAP 指的是 Internet 消息访问协议。

[网站建设指南 - get!](http://www.runoob.com/web/web-buildingprimer.html)  World Wide Web（WWW)、信息存储文件：网页、web服务器、客户端、浏览器、获取网页：从服务器请求网页数据、HTTP请求包含网页地址、显示指令HTML、Hyper Text Markup Language（HTML）标记语言、<p>段落、Cascading Style Sheets（CSS）层叠样式表、样式表定义如何显示HTML元素、JavaScript客户端脚本、向HTML添加交互行为、EXtensible Markup Language（XML）可扩展标记语言、传输信息、ASP（Active Server Pages 动态服务器页面） 和 PHP（Hypertext Preprocessor技术，允许在网页中插入服务器可执行脚本） 、服务端脚本、动态改变内容、对HTML表单数据响应、访问数据、SQL、Web 创建：少即是多、导航一致、加载速度、用户反馈、显示器、Web标准：HTML、CSS、XML、XSL、DOM、Web语义化、语义网技术：描述语言和推理逻辑、语义网实现：XML及RDF （Resource Description Framework资源描述框架）、信息资源及其之间关系的描述：RDF、使用URI来标识不同的对象（包括资源节点、属性类或属性值）、可将不同的URI连接起来，清楚表达对象间的关系、**AdSense**、**AJAX (Asynchronous JavaScript and XML)**、**Apache**开源的Web服务器、**API (Application Programming Interface)**、**Browser**、**Client**、**Client/Server**、**Clickthrough Rate**、**Cloud Computing**、**Cookie**、**DB2**、**DBA (Data Base Administrator)**、**DNS (Domain Name Service)**计算机程序运行在Web服务器上域名翻译成IP地址、**DOS (Disk Operating System)**、**HTTP Client**计算机程序，从Web服务器请求服务、**HTTP Server**计算机程序，从Web服务器提供服务、**IIS (Internet Information Server)**适用于Windows操作系统的Web服务器、**IMAP (Internet Message Access Protocol)**电子邮件服务器检索电子邮件标准通信协议、**IP (Internet Protocol)**、**IP Address (Internet Protocol Address)**每一台计算机的一个独特的识别号码（如197.123.22.240）、**JSP (Java Server Pages)**基于Java技术允许在网页中插入服务器可执行的脚本、**LAN (Local Area Network)**局部地区（如建筑物内）的计算机之间的网络、**OS (Operating System)**、**Page Views**、**PDF (Portable Document Format)**、**Ping**、**Search Engine**、**TCP (Transmission Control Protocol)**、**TCP/IP (Transmission Control Protocol / Internet Protocol)**两台计算机之间的互联网通信协议的集合。 TCP协议是两台计算机之间的自由连接，而IP协议负责通过网络发送的数据包。、**URI (Uniform Resource Identifier)**用来确定在互联网上的资源。 URL是一种类型的URI。、**URL (Uniform Resource Locator)**Web地址。标准的办法来解决互联网上的网页文件（页）（如：http://www.w3cschool.cc/）、**VPN (Virtual Private Network)**两个远程站点之间的专用网络，通过一个安全加密的虚拟互联网连接（隧道）、**Web Services**软件组件和Web服务器上运行的应用程序、搜索引擎优化（Search Engine Optimization）SEO、提高一个网站在搜索引擎中的排名（能见度）的过程、百度搜索网站登录口、



--------

### 关于「Life」：

- ##### 礼貌的笑了笑，寸步不让

  > 老板，枉为人师！


- 别人一辈子都达不到的，才是价值的表彰

  > 翟墨，朗读 - 高尔基的《海燕》


- **结果**=**80%**(100%**想法**+100%**实施**)  

  > 打仗预留军，工作多思考；

  > **保留点实力，摸底很被动**；

  >主意跟我走，大家同向前。
  >

  - 有感于：[在职场里，看起来「尽全力」地工作是一件很蠢的事情吗](https://www.zhihu.com/question/60708921/answer/179744908?utm_medium=social&utm_source=wechat_session)  Jun 6

- > **「高考」光环已逝去，   「阶层」流动使命完。**

  > **「专业」紧随生产力，     未料想「已弃」高考。**
  >

  - 有感于：[高考40年，阶层分流的历史使命早就已经结束了](https://mp.weixin.qq.com/s?__biz=MzI0NzA3MTM5NQ%3D%3D&mid=2650557246&idx=1&sn=491c608e588acfdc6eaa05ddc4cccb9d#wechat_redirect) Jun 7




------
### 关于「基础知识」：如下：


#### 1.刷题

- 目的分析：基础知识的掌握牢固， 「 **基础熟练决定你的灵活性！」**
- 解决办法：大量做题，**「 不比别人熟练，必败」**
- 可能方式：**[纽克](https://www.nowcoder.com/7037691)、[柒越](https://www.julyedu.com)**

#### 2.简历

- 目的分析：让人接受你的努力， **「 被认可，被膜拜，这是主流！」**
- 时间节点：**6月底**

#### 3.经验

- 目的分析：系统性质的项目过程， **「 灵活的经验才能被主流朝拜！」**
- 解决办法：**实战**项目、开源项目， **「 片段化不系统，机会抓不住」**
- 可能方式：**[豪洞悉](http://geek.ai100.com.cn)、[艾科科](http://tinyletter.com/fly51fly/archive)、工种薅**、**英语**

---

### 关于「已完成项目／比赛」：

Dell EMC比赛：https://github.com/alare/Hackathon_2017

比赛硬件：树莓派， 虚拟机

比赛需要但未完成：Docker相关

比赛指南：[Introduction](https://github.com/alare/Hackathon_2017/blob/master/documentation/Mars-challenge-instructions.md)

比赛得分：[PointsGet](https://github.com/alare/Hackathon_2017/blob/master/documentation/Mars-challenge-points-table.md)

1.第一部分：传感器获取数据

1.1.工作：后台运行多个传感器go文件并集成。并在浏览器查看传感器的传入数据。

1.2.工具：后台运行go命令：`nohup go run flare.go &` 。端口如下：0.0.0.0:9000（localhost）。环境变量env

1.3.参考：[Setting up the Raspberry Pi Sensors](https://github.com/alare/Hackathon_2017/blob/master/documentation/Raspberry-Go-Weather-Simulator-Setup.md)   以及  [SensorSuite](https://github.com/alare/Hackathon_2017/tree/master/sensorsuite) 

2.第二部分：平台策略执行

2.1工作：在浏览器显示出监控数据，需要运行docker

2.2工具：[Game Controller](https://github.com/alare/Hackathon_2017/tree/master/game-controller) 以及[Dashboard](https://github.com/alare/Hackathon_2017/tree/master/dashboard) 以及 [Testing the Command and Control Center](https://github.com/alare/Hackathon_2017/blob/master/documentation/Mars-challenge-instructions.md#testing-the-command-and-control-center)

3.第三部分：各个队伍PK

3.1工作：更改策略，使得最后存活时间更长，注意更改url以进行队伍pk

3.2工具：[Team_Strategy](https://github.com/alare/Hackathon_2017/tree/master/clients) 。[Testing the Command and Control Center](https://github.com/alare/Hackathon_2017/blob/master/documentation/Mars-challenge-instructions.md#testing-the-command-and-control-center)

比赛过程：**「晚上10点的没有的微信」**— 那时候，很辛苦

 ![going](YangQiDaily/going.jpg)

比赛结果：「运气比较好，挑战了清华、北大」

 ![Hackathon](YangQiDaily/Hackathon.jpg) 



![FirstPrize](YangQiDaily/FirstPrize.jpg)


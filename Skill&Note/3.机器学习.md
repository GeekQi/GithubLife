注：为个人学习总结需要，如有侵权，请您指出。[持续更新， 如有错误，请您指出]        

> 邮箱：*<u>yangqiokay@foxmai.com</u>*      微信号：<u>*18810578662*</u>

​    

## **「当下唯一一件事：秋招」**

------

### A.关于「秋招」「技能&专业书」：3／num

### **[3.机器学习]()**



| 三次总结&忌盲目乐观 | 二次总结&能讲出来            | 背诵条目&一次总结                                | 参考理解                                     |
| ---------- | -------------------- | ---------------------------------------- | ---------------------------------------- |
|            | **第一章开始：绪论**         |                                          |                                          |
|            |                      | **1. 没有免费的午餐：**：总误差率与学习算法无关；学习算法要与问题相匹配；  **2.大数据时代三大关键技术：**机器学习：数据分析能力； 云计算：数据处理能力；众包：数据标注能力  **3.数据挖掘：**机器学习和数据库是数据挖掘的两大支柱 | ![1](3.机器学习/1.png)![2](3.机器学习/2.png)![3](3.机器学习/3.png)![4](3.机器学习/4.png)![5](3.机器学习/5.png) |
|            | **第一章故事**            | **1. IBM提出机器学习：**西洋跳棋程序；用到了**强化学习**      | ![6](3.机器学习/6.png)                       |
|            | **第二场：模型评估与选择**      |                                          |                                          |
|            |                      | **1.评估方法：** 得到训练集S、测试集T:  **留出法**：(hold-out)直接讲数据集D划分为两个互斥的集合，一个做训练集，一个做测试机；并随机划分多次；留出法返回的是多个结果的平均。**交叉验证法：** 数据集划分k个大小相似互斥子集：每个子集中分层抽样：用k-1个训练，剩下测试。可进行k次训练测试，返回k次均值。 若k=m则留一法。 **自助法bootstrapping：** 在留出法和交叉验证中保留了一部分用作测试，实际用的训练集合小。 自助法(bootstrapping)可重复采样，有回采样：采样产生数据集D‘，每次从D中挑选一个样本，复制到D'中，再返回数据集D。重复m次，得到包含m个样本的数据集。在数据集小、很难划分时有用。 |                                          |
|            |                      | **1.性能度量：** 衡量模型泛化能力的评价指标；反映了任务需求：**回归任务：**均方误差MSE：更一般数据分布D及概率密度函数p，均方误差：表示称期望形式。 **分类任务：**查准率：预测是正的中原来是正的比例 ；查全率：原来是正的，预测是正的比例。 我们可根据学习器的预测结果对样例进行排序，排在前面是最可能正例的样本，每次计算当前查全率、查准率：得到P-R曲线。F1度量；Fb度量；**ROC与AUC** : ROC：横纵坐标的工作特征；随机猜想；有限测试样例不光滑；判断比较：ROC下面积AUC：**代价敏感错误率与代价曲线**：不容损失的大小不同。ROC不能直接反应学习器的期望总体代价，代价函数可以。 | ![7](3.机器学习/7.png)---![8](3.机器学习/8.png)![9](3.机器学习/9.png)---![10](3.机器学习/10.png)![11](3.机器学习/11.png)![12](3.机器学习/12.png)---![13](3.机器学习/13.png)![14](3.机器学习/14.png)![15](3.机器学习/15.png) |
|            |                      | **1.比较检验：** 性能比较：泛化性能；测试集训练集选择；随机性： **统计假设检验**：假设检验：交叉验证t检验：McNemar检验；Friedman检验； | ![16](3.机器学习/16.png)                     |
|            |                      | **1.偏差与方差：**泛化误差可分解为：偏差、方差、噪声之和。**偏差**度量了学习算法的期望预测与真实结果的偏离程度：算法本身的拟合能力。 **方差**：同样大小训练集变动导致学习性能的变化：数据扰动造成的影响；**噪声**表示当前任务上学习算法达到的期望泛化误差下界：问题本身的难度。**泛化性能**由学习算法的能力、数据的充分性、学习任务的难度共同决定。   **给定学习任务，为了取得好的泛化性能：**需要偏差较小即能充分拟合数据，并方差较小：即数据扰动产生的影响小。；**偏差方差窘境** | ![17](3.机器学习/17.png) ![18](3.机器学习/18.png) ![19](3.机器学习/19.png) |
|            | **第三章开始：线性模型**       |                                          |                                          |
|            |                      | 1.线性组合 2.线性回归：均方误差，最小二乘法；多元线性回归；对数线性回归；对数几率；极大似然法；**线性判别分析LDA:**经典线性学习方法，又称Fisher判别分析：给定训练样例，设法将样例投射到一条直线上，使得同类样例的投影点尽可能接近、异类样例投影点尽可能远离： 可以让同类样例投影点的协方差尽可能小，让异类样例投影点尽可能远离，让类中心距离尽可能大：定义类内散度矩阵。即是LDA最大化的目标。LDA经典监督降维技术。 | ![20](3.机器学习/20.png)---![21](3.机器学习/21.png)![22](3.机器学习/22.png)![23](3.机器学习/23.png)![24](3.机器学习/24.png) |
|            |                      | **1.多分类学习：**基本思路：**拆解法**：将多分类任务拆为若干个二分类任务求解：先对问题拆分，为拆出的每个二分类任务训练一个分类器；在测试时，对这些分类器的预测结果进行集成获得最终的多分类结果。关键是如何对多分类任务拆分，以及多个分类器集成。**经典拆分策略3种**：一对一：一对其余；多对多：给定数据集：一对一将N个类别两两配对：产生Cn2个二分类任务： 一对其余：一个类的样例作为正例，其他作为反例，训练N个分类器。可知OVO的存储开销和测试时间开销更大。在类别很多时OVO训练时间更小。 MVM是每次若干了类为正类，其他反类：常用技术ECOC | ![25](3.机器学习/25.png)![26](3.机器学习/26.png)![27](3.机器学习/27.png)![28](3.机器学习/28.png) |
|            |                      | **1.类别不平衡问题**： 分类中不同类别的训练样例数目差别很大；类别不平衡学习的一个基本策略：“再放缩”； 3类做法：第一类：直接对训练集反类样例进行“欠采样“即去除一些反例使得正、负数目接近，然后再进行学习；时间开销小，因为训练集远小于初值训练集。第二：对训练集的正类样例多采样：即增加正例使得正负例数量接近。 训练集合大雨初始训练集。第三：直接基于原始训练集学习但是进行阈值移动。即：分类器预测几率高于观测几率 | ![29](3.机器学习/29.png) ![30](3.机器学习/30.png) |
|            | **第三章总结**            |                                          | ![31](3.机器学习/31.png)                     |
|            | **第四章开始：决策树**        |                                          |                                          |
|            |                      | **1.决策树基本流程：**分而治之策略 **2.剪枝：基本策略**：预剪枝，后剪枝：**3.如何判定决策树泛化性能如何提升**：性能评估方法  **4.连续与缺失值**：4.1**连续值处理**：不能直接根据连续属性的可取值对节点进行划分：连续属性离散化技术：二分法对连续属性处理 C4.5  4.2**缺失值处理** ：如果在属性值缺失情况下划分属性选择 ；给定划分属性，若样本在该属性缺失，怎样对该样本划分：问题一：D'为D在属性上没有缺失值的样本子集：根据D'判断属性的优劣。问题二：若划分已知，将x划入对应子节点。若划分为之：划入所有子节点：调整不同权重：无缺失值样本中在属性a上样本所占比例。 | ![32](3.机器学习/32.png)--- ![33](3.机器学习/33.png) ![34](3.机器学习/34.png)--- ![35](3.机器学习/35.png)![36](3.机器学习/36.png)![37](3.机器学习/37.png)![38](3.机器学习/38.png) |
|            | **第四章总结**            |                                          | ![39](3.机器学习/39.png)                     |
|            | **第五章开始：神经网络**       |                                          |                                          |
|            |                      | **1.神经网络：**具有适应性的简单单元组成的广泛并行互联的网络，它的组织能模拟生物神经系统对真实世界物体所做出的交互反应。神经元模型M-P神经元模型。**感知机与多层网络**：由2层神经元组成：输入层接收输入信号后传递给输出层，输出层是M-P神经元或阈值逻辑单元：能实现逻辑与、或、非运算。解决非线性可分：多层功能神经元；多层前馈神经网络；前馈：不是信号不能往后传，而是网络拓扑结构上不存在环或回路。 | ![40](3.机器学习/40.png)![41](3.机器学习/41.png)![42](3.机器学习/42.png)![43](3.机器学习/43.png)![44](3.机器学习/44.png) |
|            |                      | **1.反向传播算法**：给定训练集：对训练例，假设输出y,得到均方误差e；BP是一个迭代学习算法，迭代每一轮采用广义感知机学习规则对参数估计。基于梯度下降，调整w,影响最后均方误差，得到更新权重即更新偏执。 **BP算法**：现将输入示例提供给输入层神经元，逐层信号前传，直到产生输出层的结果；然后计算输出层的误差。再将误差逆向传播至阴层神经元，根据阴层神经元的误差对连接权和阈值进行调整；迭代循环进行，直到达到停止条件。Bp目标：最小化训练集上累计误差；**累计误差逆传播**：标准BP每次针对一个样例，参数更新频繁。累计BP直接针对累计误差最小化，在整个训练集一遍后才更新参数。。在很多问题中，累计误差下降到一定程度，进一步下降缓慢，标准BP更快获得较好的解，尤其是训练机D非常大；如果设置隐层神经元个数：试错法调整。 **缓解BP网络过拟合：**： 一：早停：将数据分成训练集、验证集：训练机用来计算梯度、更新权重阈值。验证机用来估计误差。若训练机误差降低但验证集误差升高，则停止训练，返回具有最小验证集误差的连接权重和阈值。二：正则化：在误差目标函数中增加用于描述网络复杂度的部分利于连接权重与阈值的平方和。lamda用于对经验误差与网络复杂度进行折中。交叉验证来估计。 | ![45](3.机器学习/45.png)![46](3.机器学习/46.png)![47](3.机器学习/47.png)![48](3.机器学习/48.png)![49](3.机器学习/49.png)![50](3.机器学习/50.png) |
|            |                      | **1.全局最小与局部极小:** ：E是NN在训练集上误差，是关于连接权重w和阈值b的函数。 NN的训练过程：参数寻优过程，在参数空间寻找一组最优参数使得E最小。 两种最优：局部极小、全局最小。基于梯度的搜索是最广泛的参数寻优方法：从初始解出法，迭代寻找最优参数值。每次迭代中，先计算误差函数在当前点点的梯度，然后根据梯度确定搜索方向，容易参数寻优陷入局部极小。：采用以下策略试图“跳出”局部极小，接近全局最小：**一：**以多组不同参数值初始化多个神经网络，按标准方法训练后，取其中误差最小的解作为最终参数。相当于不同初始点开始搜索，可能陷入不同局部极小，从中选择有可能获得全局最小的结果。**二：** 使用模拟退火技术：在每一步都以一定的概率接受比当前解更差的结果，从而有助于跳出局部极小。每次迭代过程中，接受“次优解”的概率要随着时间的推移逐渐降低，保证算法稳定。 **三：** 使用随机梯度下降，与标准梯度下降法准确计算梯度不同，随机梯度下降法在计算梯度时，加入了随机因素。于是即使陷入局部极小点，计算出的梯度可能不为0，可跳出局部极小继续搜索。  遗传算法也可用来训练神经网络以更好逼近全局最小。(多是启发式，理论缺乏保障) | ![51](3.机器学习/51.png)![52](3.机器学习/52.png) |
|            |                      | **1.其他常见神经网络**：**1.1RBF网络：**：径向基函数网络：单隐层前馈神经网络：使用径向基函数作为隐含层神经元激活函数，而输出层则是对隐含层神经元输出的线性组合。假定输入、输出，则RBF网络可表示为：径向基函数：某种沿径向对称的标量函数，定义为样本x到数据中心c之间欧氏距离的单调函数。通常采用两部来训练RBF网络：第一步确定神经元中心c，常用方式包括随机采样、聚类等，第二步，利用BP算法确定参数w／b ；**1.2 ART网络**：竞争型学习是神经网络中一种常用的无监督学习策略，在使用该策略时，网络输出神经元相互经侦，优胜激活，胜者通吃。 ART是竞争型学习代表：**1.3SOM网络**  **1.4级联相关网络** ；**1.5Elman网络** **1.6Boltzmann机** NN中有一类模型是为网络状态定义一个能量。能量最小化的网络达到理想状态，网络的训练就是最小化能量函数。递归神经网络；神经元分为显层、隐层。显层用来表述数据的输入与输出，隐含表述数据的额内在表达。Blotzmann机中的神经元都是布尔形，只取0/1. Boltzmann机的训练过程就是每个训练样本作为一个状态向量。使其出现的概率尽可能大。标准Boltzmann机是全连接图，复杂度高，难以解决现实任务。现实中常采用受限Boltzmann机：仅仅保留显层与隐层的连接，从完全图简化为二部图。常用对比散度进行训练。 | ![53](3.机器学习/53.png)![54](3.机器学习/54.png)![55](3.机器学习/55.png)![56](3.机器学习/56.png)![57](3.机器学习/57.png) |
|            |                      | **1.深度学习** 参数越多模型越复杂，容易过拟合；**一：**无监督逐层训练是多隐层网络训练的有效手段，每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，本层隐节点的输出作为下一层输入，称为预训练，全部完成后，再对整个网络微调训练。 深度信念网络DBN中每一层都是受限Boltzmann机，使用无监督逐层训练时，先训练第一层，是关于训练样本的RBM模型。然后逐层预训练，再利用BP算法对整个网络进行训练。  **二：** 节省训练开销策略：权共享：让一组神经元有相同的连接权重，在CNN卷积NN中很重要。CNN可用BP算法训练 | ![58](3.机器学习/58.png)![59](3.机器学习/59.png)![60](3.机器学习/60.png) |
|            | **第五章总结**            |                                          | ![61](3.机器学习/61.png)                     |
|            | **第六章开始支持向量机**       |                                          |                                          |
|            |                      | **1.最大间隔；**等价最小化问题；凸二次规划问题；对偶问题：另对偶问题的偏导数为0得到a,得到w,b。KKT条件；求解对偶函数：二次规划问题，但问题规模正比于训练样本数量，**采用SMO算法：** 先固定a之外的所有参数，然后求a上的极值，由于存在约束，固定a之外的变量，则a可由其他变量导出，于是SMO每次选择两个变量aiaj并固定其他参数，在参数初始化后，SMO不断执行如下两个步骤直到收敛。  -选取一对需要更新的变量aiaj，固定aiaj以外的参数，求解得到更新后的aiaj。 SMO采用启发式：使得选取的两变量所对应样本之间的间隔最大。这样的两个变量有很大差别，会给目标函数值更大的变化 | ![62](3.机器学习/62.png)![63](3.机器学习/63.png)![64](3.机器学习/64.png) |
|            |                      | **1.核函数：** 可讲样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分，就能找到一个合适的划分超平面。 如果原始空间有限维，即属性有限，一定存在一个高危特征空间是样本可分。可知对偶问题涉及到x映射到特征空间之后的内积，由于特征空间维数可能很高、可能无穷，直接计算很困难，可以将x在特征空间中内积等于它们在原始样本中通过函数k()计算的结果。这样就不必去计算高维甚至无穷维特征空间中的内积。显然，若已知合适的映射函数具体形式，则可写出核函数，但现实任务中不知道是什么形式，那么合适的核函数是否一定存在呢？ **核函数：** 另x为输入空间，k为对称函数。可知只要有一个对称函数对应核矩阵半正定，就能作为核函数使用。核函数选择是SVM的最大变数，很可能性能不佳 | ![65](3.机器学习/65.png)![66](3.机器学习/66.png)![67](3.机器学习/67.png)![68](3.机器学习/68.png) |
|            |                      | **1.软间隔与正则化**： 我们假定训练样本空间或特征空间中是线性可分的。但是现实任务中很难准确确定合适的核函数使得训练集在特征空间中线性可分。或者即便找到了某个核函数使得训练集在特征空间中线性可分，也很难断定这个结果不是过拟合造成。缓解该问题的办法：允许SVM在一些样本上出错：**软间隔** ：允许某些样本不满足约束，当然也是尽可能少，优化目标。又0-1损失函数非凸、非连续，数学性质不太好，不易求解。于是用其他函数代替：通常是凸的连续函数且是0-1损失函数的上界。 三种常用替代损失函数：hinge、指数、对数。也就是**软间隔支持向量机** ，可知与硬间隔下的对偶问题唯一不同的是对偶变量的约束不同。 | ![69](3.机器学习/69.png)![70](3.机器学习/70.png)![71](3.机器学习/71.png)![72](3.机器学习/72.png)![73](3.机器学习/73.png) |
|            |                      | **1.支持向量回归** 回归问题：使得f(x)与y近可能接近。传统回归是f(x)与y完全相同时，损失才为0. 于此不同，SVR假设能容忍最多有e的偏差，即当差的绝对值大于e才计算损失。  **2.核方法：**： 不论SVM还是SVR 学得到的模型总是能表示成核函数的线性组合。表示定理；优化问题都可以表示称核函数的线性组合。 | ![74](3.机器学习/74.png)![75](3.机器学习/75.png)![76](3.机器学习/76.png)![77](3.机器学习/77.png)![78](3.机器学习/78.png)![79](3.机器学习/79.png)![80](3.机器学习/80.png) |
|            | **第六章总结**            |                                          | ![81](3.机器学习/81.png)![82](3.机器学习/82.png)![83](3.机器学习/83.png) |
|            | **第7章开始：贝叶斯分类**      |                                          |                                          |
|            |                      | **1.贝叶斯决策：** 概率框架下实施决策的基本方法，对分类任务来说，在所有相关概率都已知的理想情况下，贝叶斯决策论考虑如何给予这些概率核误判损失来选择最优的类别标记。假设有N种可能的类别标记机遇后验概率获得期望损失。 任务是寻找一个判定准则以最小化总体风险。贝叶斯判定准则：为最小化总体风险，需在每个样本上选择能使条件风险最小的类别标记。  可知，想要试用贝叶斯判定准则来最小化决策风险，首先要活的后验概率。机器学习所要实现的是基于有限的训练样本集，尽可能准确的估计出后验概率。主要有两种策略： 给定x，直接建模来预测c：为判别式模型。也可对联合概率分布建模，再求得条件概率。得到生成模型。 | ![84](3.机器学习/84.png)![85](3.机器学习/85.png)![86](3.机器学习/86.png) |
|            |                      | **1.极大似然估计** ： 估计类条件概率的常用策略：先假设具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。  概率模型的训练过程就是参数估计过程： 参数估计两个不同解决方案： **频率主义**认为参数虽然未知，但是客观存在的固定值，因此可优化似然函数等准则来确定参数值。 **贝叶斯学派**认为参数是未观察到的随机变量，本身也是有分布。可假设参数服从先验分布，然后基于观测到的数据计算参数的后验分布。  **极大似然估计**是寻找能最大化似然函数的参数值。 **2.朴素贝叶斯分类器：** 基于贝叶斯估计后验概率的重要困难在于类条件概率是所有属性的概率，难以从有限训练样本直接估计。朴素贝叶斯采用属性条件随机性假设。 为避免其他属性的信息呗训练集中未出现的属性值抹去，在估计概率值时，通常进行平滑处理：拉普拉斯修正。避免了因训练集样本不充分而导致概率估计为0的问题。现实任务中朴素贝叶斯分类器有多种使用方式：例如若任务对预测速度要求高，则给定训练机，则可将朴素贝叶斯分类器所涉及的所有概率估计事先存储起来。在预测时查表即可判别。 **3. 半朴素贝叶斯**  为降低贝叶斯公式估计后验概率的困难，朴素贝叶斯分类器采用属性条件独立性假设，但现实任务中很难成立。半朴素贝叶斯是适当考虑一部分属性间的相互依赖信息，从而不需要进行完全联合概率计算，又不至于彻底忽略了比较强的属性依赖关系。 | ![87](3.机器学习/87.png)![88](3.机器学习/88.png)![89](3.机器学习/89.png)![90](3.机器学习/90.png)![91](3.机器学习/91.png)![92](3.机器学习/92.png)![93](3.机器学习/93.png) |
|            |                      | **1.贝叶斯网**：  信念网：有向无环图来刻画属性之间的依赖关系。斌够用条件概率表描述属性的联合概率分布。  一个贝叶斯网B由结构G和参数theta 两部分构成。 网络结构是有向无环图，每个节点对应一个属性，若两个属性有直接依赖关系，则有一条边连接起来。 参数theta定量描述这种依赖关系。 假设属性x在G中的父节点集为pi。  则theta 包含了每个属性的条件概率表。**1.2结构：**贝叶斯网结构有效表达了属性间的条件独立性：给定父节点集。贝叶斯网假设每个属性与非后裔属性独立。**1.3.学习** 若网络结构已知，属性间的依赖关系已知，则贝叶斯网的学习过程相对简单。只需要通过对训练样本“计数”估计出每个节点的条件概率即可。但在现实应用中并不知道网络结构。于是贝叶斯学习的首要任务是根据训练数据集找出结构最“恰当”的贝叶斯网。定义一个评分函数来评估贝叶斯网与训练数据的契合程度，然后基于评分函数寻找结构最优的贝叶斯网。**1.4.推断** 贝叶斯网训练好之后能用来回答“查询”即通过一些属性变量的观测值来推测其他属性变量的取值。最理想的是根据贝叶斯网定义的联合概率分布来精确计算后验概率。 | ![94](3.机器学习/94.png)![95](3.机器学习/95.png)![96](3.机器学习/96.png)![97](3.机器学习/97.png)![98](3.机器学习/98.png)![99](3.机器学习/99.png)![100](3.机器学习/100.png) |
|            |                      | **1.EM算法** 未观测变量：隐变量。对模型参数极大似然估计，应最大化对数似然，由于Z是隐变量，无法直接求解，可通过对Z计算期望来最大化一观测数据的对数“边际似然。  EM算法：迭代算法：若参数已知，则根据训练数据推断最优隐变量z的值（E步）。 反之，若z值已知，则可方便的对参数theta做极大似然估计（M步）。 EM算法：两步骤交替计算：第一步E：利用当前估计的参数值来计算对数似然的期望值。 第二步M：寻找能使E步产生的似然期望最大化的参数值。然后心得到的参数值重新被用于E步，直到收敛到局部最优解。 | ![101](3.机器学习/101.png)![102](3.机器学习/102.png) |
|            | **第7章总结**            |                                          | ![103](3.机器学习/103.png)![104](3.机器学习/104.png) |
|            | **第8章开始：集成学习**       |                                          |                                          |
|            |                      | **1. 集成学习：**构建结合多个学习器来完成任务：多分类器系统；集成学习的一般结构：先产生一组“个体学习器“，再用某种策略将它们结合起来。 个体学习器通常由一个现有的学习算法从训练数据产生：如C4.5、BP神经网络算法等。 此时集成中只包含同种类型的个体学习器：如“决策树集成中全是决策树，神经网络集成中全是神经网络，这样的集成是“同质“的。 同质集成中个体学习器称为”基学习器”，相应学习算法为“基学习算法”。集成也可包括不同类型的个体学习器，例如同时包含决策树和神经网络，集成是“异质”的。异质集成中的个体学习器由不同的学习算法生成， 这时不再有基学习算法。相应地，个体学习器一般不称为基学习器，称为“组件学习器“。集成学习通过多个学习器进行结合，获得比单一学习器显著优越的泛化性能。集成学习的结果通过投票法（voting)产生，即“少数服从多数“ 根据个体学习器的生成方式，目前集成学习方法大致分为2大类：个体学习器间存在强依赖关系、必须串行生成的序列化方法，以及个体学习器间不存在强依赖关系、可同时生成的并行化方法。 前者代表：**Boosting**， 后者代表：**Bagging**和“**随机森林**” | ![105](3.机器学习/105.png)![106](3.机器学习/106.png)![107](3.机器学习/107.png) |
|            |                      | **1.Boosting：** ：一种可将弱学习器提升为强学习器的算法：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续收到更多关注。然后基于调整后的样本分布来训练下一个基学习期；重复，直至基学习器数目达到事先指定的值T，最终将T个基学习器进行加权结合。Boosting算法最著名代表：AdaBoost：”加性模型“： 即基学习器的线性组合，来最小化指数损失函数。 **从偏差-方差分解的角度看**，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。 **2.Bagging与随机森林** 想得到泛化性能强的集成。集成中的个体学习器应尽可能相互独立；可以设法使基学习器尽可能由较大差异。给定一个训练数据集，一种可能的做法是对训练样本进行采样，产生出若干个不同的自己，再从每个数据子集中训练出一个基学习器。**2.1Bagging**：并形式集成学习方法：基于自主采样法bootstrap sampling：我们可采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将基学习器结合，就是Bagging的基本流程。在对预测输出结合时，Bagging通常对分类任务使用简单投票法，对回归任务简单平均大。与标准AdaBoost只适用于二分类任务不同，Bagging能不经修改的用于多分类、回归等任务。：自助采样：给Bagging带另一个优点：由于每个基学习器只使用了初始训练集中约63%的样本，剩下的可用作验证集。 **从偏差-方差角度看** Bagging主要关注降低方差，因此在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更加明显。    **2.2随机森林** RF：Bagging的一个扩展，RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。传统决策树在选择划分属性是在当前结点的属性集合中选择一个最优属性。在RF中，对基决策树的每个节点，先从该节点的属性集合中随机选择一个包含k个属性的自己，然后再从这个子集中选择一个最优属性用于划分。 k控制了随机性的引入程度，若k=d，则基决策树的构建与传统决策树相同。随机森林简单、容易实现，计算开销小。随机森林只对Bagging作了小改动，但是与Bagging中基学习器的“多样性”仅通过样本扰动不同。随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动。使得最终集成的泛化性能可通过个体学习器之间差异度的增加而进步一提升 | ![108](3.机器学习/108.png)![109](3.机器学习/109.png)![110](3.机器学习/110.png)![111](3.机器学习/111.png)![112](3.机器学习/112.png)![113](3.机器学习/113.png)![114](3.机器学习/114.png)![115](3.机器学习/115.png)![116](3.机器学习/116.png)![116](3.机器学习/116.png) |
|            |                      | **1.组合策略** ： 学习器结合肯恩刚从三个方面带来好处：首先从统计方面，由于学习任务假设空间大，可能多个假设在训练集上达到同等性能。此时若使用单学习器可能因误选而导致泛化性能不佳，结合多个学习器会减小这一风险； 第二：学习算法会陷入局部极小。第三：某些学习任务的真实假设可能不在当前学习算法考虑的假设空间中，需要结合多个学习器。  **1.1平均法:**对数值型输出：常见策略：平均大：简单平均或者加权平均。 加权平均是集成学习研究的基本出发点。权重一般从训练数据中学习获得，一般而言，个体学习器性能相差较大时，适合食用加权平均法，性能接近时，适合使用简单平均法。 **1.2投票法** ： 分类任务常见投票法；绝对多数投票法 ，相对多数投票法，加权投票法。**1.3 学习法**： 当训练数据很多时，一种更强大的结合策略是使用“学习法”通过另一个学习法器来结合。Stacking是学习法的典型代表。 这里我们把个体学习器称为初级学习器，用于结合的学习器称为次级学习器或元学习器。 Stacking先从初始数据集训练出初始学习器，然后“生成“一个新数据集，用于训练次级学习器。 在这个新数据集中，初级学习器的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记。在训练阶段，次级训练集是利用初级学习器产生的。 直接用初级学习器的训练集来产生次级训练集，则过拟合。 因此一般通过使用交叉验证法或留一法这样的方式，用训练初级学习器未使用的样本来产生次级学习器的训练样本。 以k折交叉验证为例，初始训练集被随机划分为k个大小相似的集合。给定T个初级学习算法。次级学习器的输入属性表示和次级学习算法对Stacking集成的泛化性能有很大影响。研究表明，将初级学习器的输出类概率作为次级学习器的输入属性，用多响应线性回归作为次级学习算法效果较好。  贝叶斯模型平均基于后验概率来为不同模型赋予权重，可视为加权平均法的一种特殊实现。 | ![117](3.机器学习/117.png)![118](3.机器学习/118.png)![119](3.机器学习/119.png)![120](3.机器学习/120.png)![121](3.机器学习/121.png) |
|            |                      | **1.多样性：**： **1.1误差-分歧分解：**想要构建泛化能力强的集成，个体学习器应好而不同：假定用个体学习器通过加权平均法结合产生的集成完成回归学习任务。表明：个体学习器准确性越高、多样性越好，集成越好  **1.2多样性度量：** 度量集成中个体学习器的多样性，即估算个体学习器的多样化程度。典型做法是考虑个体分类器的两两相似／不相似性。**1.3多样性增强**： 在集成学习中需要有效的生成多样性大的个体学习器。与简单的直接用初始数据训练出个体学习器相比，如何增强多样性： 在学习过程引入随机性：对数据样本、输入属性、输出表示、算法参数进行扰动。 | ![122](3.机器学习/122.png)![123](3.机器学习/123.png)![124](3.机器学习/124.png)![125](3.机器学习/125.png)![126](3.机器学习/126.png)![127](3.机器学习/127.png)![128](3.机器学习/128.png) |
|            | **第8章总结**            |                                          | ![129](3.机器学习/129.png)                   |
|            | **第9章开始：聚类**         |                                          |                                          |
|            |                      | **1.聚类任务**： 无监督学习中，训练样本的标记信息是未知的。目标“通过对无标记训练样本的学习来揭示数据的内在性质及规律，为进一步的数据分析提供基础。  聚类试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇”（cluster）。**2.性能度量：**聚类性能度量：聚类“有效指标” 两大类：一：将聚类结果与某个参考模型比较：外部指标。 二：直接考察聚类结果而不利用任何参考模型：内部指标。 常用聚类性能度量外部指标：Jaccard系数、FM指数、 Rand指数、内部指标：DB指数、Dunn指数   **3.距离计算** 闵可夫斯基距离：P=2:欧式距离，P=1:曼哈顿距离。 属性分为“连续属性”、“离散属性”  **4.原型聚类：** 假设聚类结构能通过一组原型刻画。  算法先对原型初始化，然后对原型迭代更新求解。采用不同原型表示、不同求解方式，产生不同算法。**4.1：k均值算法** 给定样本集，k均值针对聚类所得簇划分最小化平方误差。k均值采用贪心策略，迭代优化近似求解。**4.2学习向量量化：**试图找到一组原型向量来刻画聚类结构，但与一般聚类算法不同的是，LVQ假设数据样本带有类别标记，学习过程利用样本的这些监督信息辅助聚类。 **4.3 高斯混合聚类：** 与k均值、LVQ采用原型向量刻画聚类结构不同，高斯混合聚类采用概率模型来表达聚类原型。高斯分布密度函数；**5.密度聚类** ：基于密度的聚类：假设聚类结构能通过样本分布的紧密程度确定。密度聚类算法从样本密度的角度来考察样本之间的可连续性。并基于可连接样本不断扩展聚类簇已获得最终的聚类结果。 DBSCAN是一种著名的密度聚类算法，基于一组“邻域”参数来刻画样本分布的紧密程度。**6.层次聚类**： 不同层次对数据集划分，形成树形的聚类结构，数据集划分采用自底向上聚合策略，也可自顶向下分拆策略。 | ![130](3.机器学习/130.png)![131](3.机器学习/131.png)![132](3.机器学习/132.png)![133](3.机器学习/133.png)![134](3.机器学习/134.png)![135](3.机器学习/135.png)![136](3.机器学习/136.png)![137](3.机器学习/137.png)![138](3.机器学习/138.png)![139](3.机器学习/139.png)![140](3.机器学习/140.png)![141](3.机器学习/141.png)![142](3.机器学习/142.png)![143](3.机器学习/143.png)![144](3.机器学习/144.png)![145](3.机器学习/145.png)![146](3.机器学习/146.png)![147](3.机器学习/147.png)![148](3.机器学习/148.png)![149](3.机器学习/149.png)![150](3.机器学习/150.png) |
|            | **第9章总结**            |                                          | ![151](3.机器学习/151.png)![152](3.机器学习/152.png)![153](3.机器学习/153.png) |
|            | **第10章开始：降维与度量学习**   |                                          |                                          |
|            |                      | **1.k近邻学习：**监督学习方法。分类任务：投票法；回归任务：平均法。没有明显训练过程；最近邻分类器的泛化错误率不超过贝叶斯最优分类器的错误率的两倍。 **2.低维嵌入**  ： 现实应用中属性维度经常成千上万，要满足采样条件所需的样本数目极大。当维度很高时，计算内积都不容易。 在高维情况下出现的数据样本稀疏、距离计算困难等问题：维度灾难。 缓解维度灾难的重要途径：降维：维度约简： 通过某种数学变换将原始高维属性空间转变为一个低纬“子空间”。  若要求原始空间中样本之间的距离在低纬空间保持。即得到**多维缩放MDS：**： 假定m个样本在原始空间距离矩阵为D， 目标是获得在d维空间的表示： 且任意两个样本在d维空间的欧式距离等于原始空间中的距离。 基于线性变换降维的方法称为线性降维方法。 对降维效果的评估，通常比较降维前后学习器的性能。若性能有所提高，则认为降维起到了作用，若降低到2维、3维，则可通过可视化技术直观判断降维效果。  **3.主成分分析**： PCA： 常用降维方法：对正交属性空间中的样本点，如何用超平面对所有样本表示：最近重构性、最大可分性。原样本点与基于投影重构的样本点之间距离。 考虑重构性：距离应该被最小化。则为主成分分析的优化目标。 投影后的样本点的方差最大化。 需要对协方差矩阵进行特征值分解，将求得的特征值排序，再取前d个特征值对应的特征向量构成W，就是主成分分析的解。。 降维后的维数d是是用户指定的。 通过在d不同的低纬空间度对kNN分类器交叉验证选取较好的d值。 PCA：逐一选取方差最大方向，先对协方差矩阵做特征值分解。取最大特征值对应的特征向量w。低维空间和原始空间不同，因为对应于最小的特征值的特征向量被舍弃了。这时降维导致的结果，但是舍弃是必要的：一方面能让样本的采样密度增大，是降维的重要原因。另一方面。当数据受到噪声影响的时候，最小的特征值所对应的特征向量往往与噪声有关，舍弃能进行去噪声。**4.核化线性降维**： 假设从高维空间到低维空间的函数映射是线性的。 现实任务中，需要非线性映射才能找到恰当的低纬度嵌入。非线性降维的一种常用方法，基于核技巧对线性降维方法进行“核化”。 核主成分分析 。**6.度量学习**： 度i 高维数据降维度是希望找到一个合适的低纬空间，在此空间中学习能比原始空间性能好。也就是合适的距离度量。 | ![154](3.机器学习/154.png)![155](3.机器学习/155.png)![156](3.机器学习/156.png)![157](3.机器学习/157.png)![158](3.机器学习/158.png)![159](3.机器学习/159.png)![160](3.机器学习/160.png)![161](3.机器学习/161.png)![162](3.机器学习/162.png)![163](3.机器学习/163.png)![164](3.机器学习/164.png)![165](3.机器学习/165.png)![166](3.机器学习/166.png)![167](3.机器学习/167.png)![168](3.机器学习/168.png)![169](3.机器学习/169.png)![170](3.机器学习/170.png) |
|            | **第11章总结**           |                                          | ![171](3.机器学习/171.png)![172](3.机器学习/172.png)![173](3.机器学习/173.png) |
|            | **第12章开始：特征选择与稀疏学习** |                                          |                                          |
|            |                      | **1.特征选择是数据预处理过程：**两个原因：一：维度灾难。 二：去除不相关特征降低学习难度。 去除冗余特征。想要从初始特征集合中选取一个包含了所有重要信息的特征，如果没有领域先验知识，就只能遍历所有可能的子集。**2.两个关键环节：**一：如果根据评价结果获取下一候选特征子集 二：如何评价候选特征子集的好坏。 第一：“子集搜索”： 给定特征集合，将每个特征看作一个候选子集，对de个候选单特征子集评价：前向搜索， 双向搜索。 第二：子集评价：信息增益。**3.常见的特征选择方法**：过滤式、包裹式、嵌入式： **过滤式：**先对数据集进行特征选择，再训练学习器。 特征选择与后续学习器无关。相当于先用特征选择过程对初始特征过滤，再用过滤后的特征训练模型。**包裹式选择：**与过滤式不考虑后续学习器不同，包裹式特征选择直接把最终将学习的学习器的性能作为特征子集的评价标准：目的是为给定学习器选择最有利于性能的特征子集。**4.嵌入式与L1正则化** ： 过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明显的分别。嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，在同一个优化过程中完成。在学习器训练过程中自动进行了特征选择。**5.稀疏表示与字典表示** 数据集D中很多列与当前学习任务无关。**6.压缩感知** | ![174](3.机器学习/174.png)![175](3.机器学习/175.png)![176](3.机器学习/176.png)![177](3.机器学习/177.png)![178](3.机器学习/178.png)![179](3.机器学习/179.png)![180](3.机器学习/180.png)![181](3.机器学习/181.png)![182](3.机器学习/182.png)![183](3.机器学习/183.png)![184](3.机器学习/184.png)![185](3.机器学习/185.png)![186](3.机器学习/186.png)![187](3.机器学习/187.png) |
|            | **第12章总结**           |                                          | ![188](3.机器学习/188.png)![189](3.机器学习/189.png)![190](3.机器学习/190.png) |

注：为个人学习总结需要，如有侵权，请您指出。[持续更新， 如有错误，请您指出]        

> 邮箱：*<u>yangqiokay@foxmai.com</u>*      微信号：<u>*18810578662*</u>

​    

## **「当下唯一一件事：CV」**

------

### A.关于「CV」「技能&专业书」：2／num

### **[2.统计学习方法]()**



| 三次总结&忌盲目乐观 | 二次总结&能讲出来           | 背诵条目&一次总结                                | 参考理解                                     |
| ---------- | ------------------- | ---------------------------------------- | ---------------------------------------- |
|            | **第一章开始：统计学习方法概述**  |                                          |                                          |
|            |                     | **1.统计学习定义**：基于数据构建统计模型，并对数据进行预测分析 **2.统计学习方法分类：**监督／非监督／半监督／强化学习**3.统计学习三要素：**模型／策略／算法**4.统计学习实现步骤**：训练数据-假设空间-学习策略-学习算法-选择最优模型-预测分析 | ![1](2.统计学习方法/1.png)---  ![2](2.统计学习方法/2.png)---![3](2.统计学习方法/3.png)---![4](2.统计学习方法/4.png) |
|            |                     | **1.监督学习问题分类：**输入输出连续：回归；输出离散：分类；输入输出为序列：标注 **2.监督学习假设：**XY联合概率分布。**3监督学习目的：**输入输出映射 **4 监督学习模型**：概率模型／非概率模型。**5.1监督学习学习过程**：学习系统利用D学习模型（输入输出的映射） **5.2监督学习预测过程**：预测系统输入x,由模型给出输出。 | ![5](2.统计学习方法/5.png)---![6](2.统计学习方法/6.png)---![7](2.统计学习方法/7.png)--- ![8](2.统计学习方法/8.png) |
|            |                     | **1.统计学习第二要素策略**：选择最优模型：损失函数(常用01/平方／绝对值／对数)越小，模型越好； 期望损失／风险函数越小的模型；经验风险／经验损失逼近，越小，模型越好；为避免过拟合：结构风险最小化，模型越好**2.监督学习问题转化**：经验风险／结构风险的最优化问题，经验／结构函数是最优化的目标函数 | ![9](2.统计学习方法/9.png)---![10](2.统计学习方法/10.png)---![11](2.统计学习方法/11.png)---![12](2.统计学习方法/12.png) |
|            |                     | **1.模型选择：**训练误差／测试误差／复杂度关系，过拟合产生。最优模型选择以达到测试误差最小。**2方法一正则化：**结构风险最小化策略的实现：复杂度递增函数：L1/L2范数 ：奥卡姆剃刀原理。 **3方法二交叉验证：**(数据集大。)随机切分三部分。否则交叉验证：重复使用数据：**3.1简单交叉：**随机分2部分：不同条件训练模型，择优 **3.2S折交叉：**随机S个子集。可重复S次。**3.3留一交叉验证**S = N | ![13](2.统计学习方法/13.png)--- ![14](2.统计学习方法/14.png)--- ![15](2.统计学习方法/15.png)---![16](2.统计学习方法/16.png)---![17](2.统计学习方法/17.png) |
|            |                     | **1.泛化能力：**对未知数据的预测能力；通过泛化误差／期望风险反映      | ![18](2.统计学习方法/18.png)                   |
|            |                     | **1.监督学习任务：**决策函数／条件概率分布 **2.生成模型：**先学习联合概率分布，再求条件概率模型。因为模型表示了输入X产生Y输出的生成关系**3判别模型：**直接决策函数／条件概率分布。关心对输入X应该预测什么Y | ![19](2.统计学习方法/19.png)![20](2.统计学习方法/20.png) |
|            |                     | **1.分类问题：**输出离散值； 分类决策函数P(Y\|X)或Y= f(X)：分类器；对新输入x分类预测其输出类y。**2 评价指标：**分类准确率**A**：正确分分类数与总样本比值／精确率**P**：预测为正类，原来就是正类的占比／召回率**R**：原来是正类，预测为正类的占比／**F1**。**3分类应用：**银行客户分类／网络安全日志非法入侵检测／图像人脸识别／手写数字是识别／网页分类／文本分类：文本特征频率。 | ![21](2.统计学习方法/21.png)![22](2.统计学习方法/22.png)---![23](2.统计学习方法/23.png)---![24](2.统计学习方法/24.png)--- |
|            |                     | **1.标注tagging问题：**分类的推广：输入观测序列，输出标记序列／状态序列。 学习系统：模型为条件概率分布。输出使得条件概率最大的序列标记序列y。**2.标注应用：**信息抽取／词性标注 | ![25](2.统计学习方法/25.png)![26](2.统计学习方法/26.png) |
|            |                     | **1.回归问题：**表述输入到输出映射的函数模型，等价于函数拟合。分为一元回归／多元回归；分为线性回归／非线性回归；  回归常用损失函数：平方损失函数-最小二乘法。 **2.回归应用：**商务市场趋势预测、产品质量管理、客户满意度调查、投资风险分析、股票预测 | ![27](2.统计学习方法/27.png) ![28](2.统计学习方法/28.png) |
|            | **第一章总结**           |                                          | ![29](2.统计学习方法/29.png)                   |
|            | **第二章开始：感知机**       |                                          |                                          |
|            |                     | **1.感知机：**二分类的线性分类模型；对应化分正负的分离超平面；判别模型；导入损失函数，梯度下降得到感知机模型；NN/SVM的基础； **2感知机模型：**输入x,输出+1/-1；函数f(x)为感知机。 | ![30](2.统计学习方法/30.png)![31](2.统计学习方法/31.png) |
|            |                     | **1.感知机学习策略：**假设线性可分，感知机目标学习分离超平面，即确定w/b。定义经验损失函数，并极小化  **2损失函数选择：**误分类点到超平面S的总距离：经验风险函数。 | ![32](2.统计学习方法/32.png)![33](2.统计学习方法/33.png) |
|            |                     | **1.感知机学习算法**：求策略中损失函数的最优化问题：梯度下降。  **2感知机学习算法原始形式：**minL(w,b),梯度下降更新w/b。算法步骤4步：**2.1**初始w/b-**2.2**训练集选择数据-**2.3**更新w/b-**2.4**再选择数据，直到没有误分类点.   **3感知机例子：** | ![34](2.统计学习方法/34.png) ![35](2.统计学习方法/35.png)![36](2.统计学习方法/36.png)![37](2.统计学习方法/37.png) |
|            |                     | 1.**感知机收敛**：存在多解，依赖初值，需要约束条件才能唯一，也就是线性支持向量机； 感知机线性不可分则不收敛。  2.**感知机学习算法对偶形式：**将w/b表示为x和y的线性组合，通过系数得到w/b。 算法步骤4步：**2.1**a = 0,b = 0 **2.2**训练集选数据 **2.3**更新a/b **2.4**直到没有误分类数据。  **3对偶形式例子** | ![38](2.统计学习方法/38.png) ![39](2.统计学习方法/39.png) |
|            | **第二章总结**           |                                          | ![40](2.统计学习方法/40.png)                   |
|            | **第三张开始：K近邻算法**     |                                          |                                          |
|            |                     | **1.K近邻(K-nearest neighbor k-NN)：**基本分类回归法。分类时，对新实例，根据其k个最近邻的训练实例的类别，多数表决来进行预测。不具有显式学习过程。 实际上利用训练数据进行特征空间划分作为分类模型； **三要素：**k值选择、距离度量、分类决策规则。 **2.k近邻算法：**2步：**2.1**根据给定距离度量，在训练集T中找出与x最邻近的k个点，涵盖这k个点的邻域为Nk(x) **2.2**在Nk(x)中根据分类决策规则(所属表决)决定y。当k = 1:最近邻算法。**3K近邻模型：**特征空间的划分。**3.1要素一：距离度量：**Lp距离确定 。**3.2要素二：k值选择：**较小k,较小邻域，近似误差减小，估计误差增加，模型复杂，易过拟合。k较大相反。一般先取比较小值，交叉验证择优k。**3.3要素三：分类决策规则**：多数表决；如果分类损失函数0/1 则误分类概率P,则误分类率为；要是其最小即经验风险最小。 | ![41](2.统计学习方法/41.png)![42](2.统计学习方法/42.png)![43](2.统计学习方法/43.png)![44](2.统计学习方法/44.png) |
|            |                     | **1.k近邻实现：kd树：**实现主要是如何对训练数据快速k近邻搜索。**2.构造kd树：** 对k维空间的实例点存储检索的树形数据结构、二叉树；k维空间的划分 构造算法3步：**2.1**:构造根节点，对应于包含T的k维空间超矩形区域：选择中位数切分，左右子节点划分 **2.2**重复，中位数切分 **2.3**直到两个空间没有实例点。停止，形成区域划分 **3例子**   **4.搜索kd树**：可知利用kd树可以减少大部分点的搜索，减少搜索计算量，搜索最近邻算法 共4步：**4.1**在kd树中找到包含目标点的叶节点：方法：从根节点递归向下访问kd树，如果当前维坐标小于切分点坐标，移动左子节点，否则，直到子节点为叶节点 **4.2**此叶节点为“当前最近点” **4.3**递归往上退。每个节点如下操作：**4.3.1**:如果更近，则为当前最近点**4.3.2**当前最近在一个子节点区域，检查父节点另一子节点区域是否更近的点，球的相交，不相交接着向上，相交，移动到另一子节点。**4.4**回到根节点，搜索结束当前最近点即x最近邻。平均复杂**O(logN)** **5.例子** | ![45](2.统计学习方法/45.png) ![46](2.统计学习方法/46.png)![47](2.统计学习方法/47.png)![48](2.统计学习方法/48.png) |
|            | **第三章总结**           |                                          | ![49](2.统计学习方法/49.png)![50](2.统计学习方法/50.png) |
|            | **第四章开始：朴素贝叶斯**     |                                          |                                          |
|            |                     | **1.朴素贝叶斯法：**基于贝叶斯定理与特征条件独立假设的分类算法。对数据集，基于特征独立假设，学习输入／输出的联合概率分布，基于此模型，对给定x,利用贝叶斯定理求**后验概率**最大输出y。**2.基本方法：**先验概率分布P(y=c) 条件概率分布(**朴素贝叶斯做了条件独立性假设**)P(X=x\|y=c):得到联合概率分布P(x,y)。属于生成模型； 分类时，对给定输入x,通过学习到的模型根据贝叶斯计算后验概率分布P(Y=c\|X=x) **3.后验概率最大化：**朴素贝叶斯将实例分到后验概率**最大**的类中。等价于期望风险**最小**化。 | ![51](2.统计学习方法/51.png)![52](2.统计学习方法/52.png)![53](2.统计学习方法/53.png) |
|            |                     | **1.朴素贝叶斯参数估计：1.1极大似然估计：** 朴素贝叶斯中学习意味着P(Y=c)和P(X=x\|Y=c).可用极大似然估计法去估计相应概率。  先验概率P(Y=c)的极大似然估计P;条件概率的极大似然估计P;  **1.2学习分类算法：** 朴素贝叶斯算法：3步：**1.2.1**:计算先验概率及条件概率 **1.2.2**计算后验概率 **1.2.3**确定实例x的类 **1.3例题**  **1.4贝叶斯估计：** 用极大似然估计可能出现估计概率值为0，采用拉普拉斯平滑。**1.5例题** | ![54](2.统计学习方法/54.png) ![55](2.统计学习方法/55.png) ![56](2.统计学习方法/56.png) ![57](2.统计学习方法/57.png) |
|            | **第四章总结**           |                                          | ![58](2.统计学习方法/58.png) ![59](2.统计学习方法/59.png) |
|            | **第五章开始：决策树**       | **1.决策树：**基本分类回归方法；树形结构；if-then规则的集合(**互斥且完备**)／特征空间与类空间的条件概率分布(**P(Y\|X)**)；根据损失函数最小化原则建立决策树模型；包括**三步骤**：特征选择、决策树生成、决策树剪枝； 由节点和边都成。**2.决策树学习用损失函数的目标最小化：**通常是正则化极大似然函数。递归的选择最优特征。进行分割。**3.决策树剪枝：**避免过拟合，使具有更好泛化能力。 | ![60](2.统计学习方法/60.png) ![61](2.统计学习方法/61.png)![62](2.统计学习方法/62.png) ![63](2.统计学习方法/63.png) |
|            |                     | **1.特征选择：**选取对训练数据具有分类能力的特征，提高决策树学习的效率；通常准则：信息增益／信息增益比。 **2信息增益：** **熵**：随机变量不确定性的度量H(p) 。**条件熵**H(Y\|X)表示在已知随机变量X的条件下随机变量Y的不确定性。**信息增益**g(D,A)-H(D)-H(D\|A)[**互信息**]：得知特征X的信息使得类Y的信息不确定性减少的程度。**信息增益算法**3步：**2.1**计算数据集的经验熵 **2.2**计算特征A对数据集D的经验条件熵H(D\|A) **2.3**计算信息增益 ：**例子**: **3信息增益比**：信息增益相对于训练数据集而言，没有绝对意义。当H(D)偏大，信息增益会偏大。用信息增益比可以矫正：g(D,A) = g(D,A)/H(D) | ![64](2.统计学习方法/64.png) ![65](2.统计学习方法/65.png) ![66](2.统计学习方法/66.png)![67](2.统计学习方法/67.png)![68](2.统计学习方法/68.png) |
|            |                     | **1.决策树生成：ID3算法：**在各个节点用信息增益选择特征，递归构建决策树。算法6步：**1.1**若都属于同一类，T为单节点树 **1.2**若特征集为空，则单节点树。并最大类作为类标记 **1.3** 否则，计算信息增益，选择最大特征A **1.4**如果A小于阈值eta,T为单节点。实力最大类作为节点类标记。**1.5**否则，按照A分割D，**1.6**递归调用1-5。 2. **C4.5生成：用信息增益比**。算法同 | ![69](2.统计学习方法/69.png) ![70](2.统计学习方法/70.png) ![71](2.统计学习方法/71.png) |
|            |                     | **1.决策树剪枝：**决策树生成会过拟合，复杂。需要简化：剪枝(pruning); 剪枝通过极小化决策树整体损失函数来实现：决策树学习的损失函数定义C，a控制模型拟合与模型复杂度之间的影响。较大a选择简单模型。剪枝就是a确定时，选择损失函数最小的模型；当a确定，树越大，拟合越好，复杂度越高。  可知决策树生成是局部的提高信息增益比。决策树剪枝是通过优化损失函数，减小模型复杂度的整体考虑。 损失函数极小等价于正则化的极大似然估计的模型选择。**剪枝算法3步**：**1.1**计算每个节点经验熵 **1.2** 递归往回缩，如果缩回去之后，损失函数变小了，就剪枝。父节点变为新叶节点。**1.3**返回2直到不能继续得到损失函数最小的树 | ![72](2.统计学习方法/72.png) ![73](2.统计学习方法/73.png)![74](2.统计学习方法/74.png) |
|            |                     | **1.分类与回归(classification and regression tree CART): CART生成：** 回归树：平方误差最小准则。分类时：基尼指数最小化准则进行特征选择 。**1.1回归树生成：** 一个回归树对应输入空间的划分及划分单元的输出值。假设将输入空间划分为M个单元，并且每个单元有固定输出值，回归树模型f(x)。用平方误差最小准则求每个单元的最优输出。 输入空间划分：选择变量切分，寻找最优切分点。依次重复。**最小二乘回归树生成算法：** 分4步：**1.1.1**：选择最优切分变量与切分点似的最优。**1.1.2**:选定的对划分区域并绝对相应的输出值。**1.1.3**:步骤1、2重复。**1.1.4**划分成M个区域。生成决策树。**1.2 分类树的生成**：用基尼系数选择最优特征，同时决定该特征的最优二值切分点。 **1.3CART生成算法**：共4步：**1.3.1**:计算现有特征基尼系数 **1.3.2**选择基尼系数最小的特征及切分线作为最优特征及切分 **1.3.3**递归12 **1.3.4**生成CART决策树 **1.4例子**  **1.5CART剪枝：** 由2步组成：首先从生成算法的决策树底端剪枝，直到根节点，形成子树序列；然后交叉验证集测试，选择最优子树。 **1.5.1剪枝形成子树序列：**损失函数C；固定a一定有C最小的树。a较大最优子树Ta偏小;可以递归的对树进行剪枝，将a从小到大。得到n个最优子树序列。 **具体的：**从T0整体树开始剪枝，对任意节点t的损失函数C.以t为根节点的损失函数Ct；在某一a,C=Ct.而t的节点少，则进行剪枝。则对T0每一个t都计算g(t)，减去，得到子树。    **1.5.1在剪枝得到的子树序列中交叉验证选择最优子树：**用平方误差货基尼指数最小判定。 **1.6CART剪枝算法：**7步：1.6.1k=0,T=T0 1.6.2设a=+00 1.6.3自下而上对个节点计算C及g(t) 1.6.4自上而下访问内部节点，如果gt=a则剪枝，并多数表决决定类。1.6.5a=a1.6.6交叉验证得到最优子树 | ![75](2.统计学习方法/75.png)  ![76](2.统计学习方法/76.png)![77](2.统计学习方法/77.png)![78](2.统计学习方法/78.png)![79](2.统计学习方法/79.png)![80](2.统计学习方法/80.png)![81](2.统计学习方法/81.png) |
|            | **第五章总结**           |                                          | ![82](2.统计学习方法/82.png)![83](2.统计学习方法/83.png) |
|            | **第六章开始逻辑回归与最大熵模型** |                                          |                                          |
|            |                     | **1.逻辑回归**：分类方法 ；**最大熵**：概率模型学习准则，推广到分类问题得到最大熵模型。 都是对数线性模型。  **2.逻辑斯特分布：**分布函数属于逻辑斯特函数／密度函数 **3.二项逻辑斯特回归模型：**分类模型；条件概率分布P(Y\|X)表示。对给定x求得P(Y=1\|X) 和P(Y=0\|X)，比较两个条件概率的大小，将实例x分到概率值较大的那一类。  **4.模型参数估计：** 对训练集T,用极大似然估计估计模型参数；设P(Y=1\|X) 和P(Y=0\\ | X)似然函数为：**相乘**(联合概率最大，又相互独立)，对数似然，求极大值，得到w的估计值；问题转为**对数似然函数的目标函数的最优化：**梯度下降／拟牛顿法。**5.多项逻辑回归：** 二项为二分类，多项为多分类。 |
|            |                     | **1.最大熵模型**：最大熵原理推导实现。 **2.最大熵原理**：概率模型学习准则：学习概率模型时，所有可能概率模型中，熵最大模型是最好的：通常用约束条件确定模型的集合；**3.最大熵模型定义：** 最大熵原理应用到分类中得到最大熵模型。：假设满足所有约束条件的模型集合；定义在条件概率分布P(Y\|X)上的条件熵为H(P)则模型集合C中条件熵最大的模型为最大熵模型。  **4.最大熵模型的学习过程：**就是求解最大熵模型的过程；约束最优化问题； 按照最优化习惯，将最大值问题改写为等价求最小值问题。  **5.极大似然估计：** | ![88](2.统计学习方法/88.png)![89](2.统计学习方法/89.png)![90](2.统计学习方法/90.png)![91](2.统计学习方法/91.png)![92](2.统计学习方法/92.png)![93](2.统计学习方法/93.png)![94](2.统计学习方法/94.png)![95](2.统计学习方法/95.png)![96](2.统计学习方法/96.png) |
|            |                     | **1.逻辑回归／最大熵模型学习：归结为**似然函数为目标函数的最优化问题；通过迭代算法求解。 目标函数是凸函数，多种优化方法都能找到全局最优解：常用：迭代尺度法、梯度下降法、牛顿法／拟牛顿法  **2.改进的迭代尺度法：** (improved iterative scaling IIS) 最大熵模型学习的最优化算法：假设最大熵模型当前参数向量w.找到新参数向量w+eta。使得模型对数似然函数增大。则w = w+eta.直到都收敛。 | ![97](2.统计学习方法/97.png)![98](2.统计学习方法/98.png)![99](2.统计学习方法/99.png)![100](2.统计学习方法/100.png)![101](2.统计学习方法/101.png) |
|            | 附录                  | **梯度下降法：**：求解无约束最优化问题的常用方法；迭代算法；每一步要求目标函数梯度向量；选取初值，不断迭代，更新x,目标函数最小化，直到收敛。**牛顿法／拟牛顿法**：牛顿法利用极小点的必要条件导数为0； 拟牛顿法：矩阵代替：BFGS流行拟合牛顿法：用B逼近矩阵H **拉格朗日对偶性**：拉格朗日函数 |                                          |
|            | **第六章总结**           |                                          | ![102](2.统计学习方法/102.png)![103](2.统计学习方法/103.png) |
|            | **第七章开始支持向量机**      |                                          |                                          |
|            |                     | **1.支持向量机**：二类分类模型：最大间隔线性分类器；还包括核技巧：非线性分类器；求解凸二次规划问题；正则化合页损失函数最小化问题；   线性可分／线性支持(训练数据近似线性可分)／非线性SVM; 核函数表示将输入从输入空间映射到特征空间，得到的特征向量之间的内积；核方法比SVM更一般 。 **2.线性可分支持向量机与硬间隔最大化**：解唯一：函数间隔y(wx+b)表示分类预测正确性机确信度。 几何间隔：避免成比例w/b使函数间隔变化：y(wx+b)/\|w\|。  **3.间隔最大化：** 几何间隔最大的分离超平面唯一：转化为约束最优化问题：是凸二次规划问题；  **4.线性可分支持向量机学习算法-最大间隔法**：4.1构造并求解约束最优化问题 得到w/b 4.2得到分离超平面。  **5.支持向量：** 使得约束成立的点：线性可分下，训练数据的样本点中与分离超平面距离最近的样本点。  间隔依赖于分离超平面法向量：间隔边界。决定分离超平面只有支持向量起作用，即很少的重要训练样本决定。  **6.学习对偶算法：** 拉格朗日函数；对偶问题：极大极小问题；6.1求min得到w; 6.2求max得到a; 可得到w / b的表示：可知分类决策函数只依赖于输入x和训练样本输入的内积。 **7.线性可分支持向量机学习算法：** 7.1构造并求解约束最优化问题,得到a 7.2计算w/b.得到分离超平面。 | ![104](2.统计学习方法/104.png) ![105](2.统计学习方法/105.png) ![106](2.统计学习方法/106.png) ![107](2.统计学习方法/107.png) ![108](2.统计学习方法/108.png) ![109](2.统计学习方法/109.png) ![110](2.统计学习方法/110.png) ![111](2.统计学习方法/111.png) ![112](2.统计学习方法/112.png) ![113](2.统计学习方法/113.png) ![114](2.统计学习方法/114.png) ![115](2.统计学习方法/115.png) ![116](2.统计学习方法/116.png) ![117](2.统计学习方法/117.png) |
|            |                     | **1.线性支持向量机与软间隔最大化** ：线性可分SVM对不可分训练数据是不适用的。扩展到线性不可分问题：某些样本点不能满足函数间隔大于等于1的约束条件：引入松弛变量eta：约束条件：y(wx+b)>=1-eta； 目标函数也变了。惩罚参数C大，则对误分类惩罚增大。C小，对误分类惩罚减小；**2.学习算法**。**3.支持向量** ：分离超平面／间隔边界／支持向量 **4合页损失函数** | ![118](2.统计学习方法/118.png)![119](2.统计学习方法/119.png)![120](2.统计学习方法/120.png)![121](2.统计学习方法/121.png)![122](2.统计学习方法/122.png)![123](2.统计学习方法/123.png)![124](2.统计学习方法/124.png)![125](2.统计学习方法/125.png) |
|            |                     | **1.非线性支持向量机与核函数** ：核技巧：非线性问题：利用非线性模型才能很好分类：如果能用一个超曲面将正负例正确分开，称为非线性可分问题。非线性变化可将非线性问题变为线性问题，转化为线性分类问题。**核技巧：**先转换将原空间中数据映射到新空间，在新空间用线性分类学习方法从训练数据中学习分裂模型。 **2.核函数：** 核技巧：只定义核函数K(x,z)不显式定义映射函数。核技巧在SVM的对偶问题中，涉及内积的可用核函数代替。 **3.正定核：** 通常说的核函数就是正定核函数 **4常用核函数**：多项式／高斯／字符串 **5非线性支持向量机**：只需要将线性支向量机中对偶形式中内积换成核函数 | ![126](2.统计学习方法/126.png)![127](2.统计学习方法/127.png)![128](2.统计学习方法/128.png)![129](2.统计学习方法/129.png)![130](2.统计学习方法/130.png)![131](2.统计学习方法/131.png)![132](2.统计学习方法/132.png)![133](2.统计学习方法/133.png)![134](2.统计学习方法/134.png)![135](2.统计学习方法/135.png) |
|            |                     | **1.序列最小最优算法：**SVM学习的实现：图二次规划问题：全局最优解； **2.两个变量二次规划的求解方法：**选择两个变量，固定其他变量。针对两个变量构建二次规划问题。这个问题关于这两个变量的解更接近原始二次规划问题的解，因为会使得院士二次规划问题的目标函数值变更小，可以通过解析方法求解。**3.变量的选择方法：**  **4.SMO算法：** | ![136](2.统计学习方法/136.png)![137](2.统计学习方法/137.png)![138](2.统计学习方法/138.png)![139](2.统计学习方法/139.png)![140](2.统计学习方法/140.png)![141](2.统计学习方法/141.png)![142](2.统计学习方法/142.png)![143](2.统计学习方法/143.png) |
|            | **第7章总结**           |                                          | ![144](2.统计学习方法/144.png)![145](2.统计学习方法/145.png)![146](2.统计学习方法/146.png) |
|            | **第8章开始 提升方法**      |                                          |                                          |
|            |                     | **1.提升(boosting) ：常用的统计学习方法：**通过改变训练样本权重，学习多个分类器，并将分类器线性组合，提高分类性能。 **2.提升方法基本思路：** 对一个复杂任务，多个专家判断进行适当综合得出的判断，比任何一个专家单独判断好。提升方法就是从弱科学习算法出发，反复学习，得到一系列弱分类器(基本分类器) 组合得到强分类器。大多数提升方法都是改变训练数据的概率分布(权值分布),针对不同训练数据分布调用弱学习算法学习一些弱分类器。    一：如何改变训练数据的权值或概率分布 二：如何将弱分类器组合成强分类器。   **3.问题解决：AdaBoost:** 解决一：提高被前一轮弱分类器错误分类样本的权值，降低正确分类样本权值，于是分类问题被一系列弱分类器分而治之。  解决二：加权多数表决的方法：加大分类误差率小的弱分类器的权值。使其在表决中起较大作用。**4. AdaBoost算法**  4.1 初始化权值分布 4.2 对m=1-M： 4.2.1 使用权值分布D的训练数据集学习，得到基本分类器Gm。4.2.2计算Gm在训练数据集上的分类误差率em 4.2.3计算Gm的系数am. 4.2.4更新权值分布Dm+1。4.3构建组合分类器的线性组合。 **5.AdaBoost算法的训练误差：** AdaBoost的基本性质是在学习过程不断减少训练误差，及训练数据集上的分类误差率。  **6.AdaBoost:** 可认为是模型为加法模型、损失函数为指数函数、学习算法为前向分布的二类分类学习方法。  前向分布算法： 加法模型； 给定训练数据集损失函数下，学习加法模型称为经验风险极小化及损失函数极小化问题。  前向分布算法求解优化问题：学习的是加法模型，从前向后，每一步只学习一个基函数及系数。逐步不仅优化目标函数。可以简化优化的复杂度。   **AdaBoost算法**是前向分步加法算法的特例：模型由基本分类器组成的加法模型，损失函数是指数函数 | ![147](2.统计学习方法/147.png) ![148](2.统计学习方法/148.png) ![149](2.统计学习方法/149.png) ![150](2.统计学习方法/150.png) ![151](2.统计学习方法/151.png) ![152](2.统计学习方法/152.png) ![153](2.统计学习方法/153.png) ![154](2.统计学习方法/154.png) ![155](2.统计学习方法/155.png) ![156](2.统计学习方法/156.png) |
|            |                     | **1.提升树：**以分类树／回归树为基本分类器的提升方法。统计学习中性能最好的方法之一： **2.提升树模型：** 采用加法模型(基函数线性组合)与前向分布算法，以决策树为基函数的提升方法为提升树。对分类问题决策树：二叉分类树。对回归问题决策树：二叉回归树 **3.提上树算法：** 采用前向分步算法：首先确定初始提升树，经过经验风险极小化确定下一棵决策树的参数。 由于决策树的线性组合可以很好的拟合训练数据，所以提升树是高功能学习算法。  不同问题：平方误差损失函数的回归问题；指数损失函数的分类问题。 二类分类问题：提升树算法只需将AdaBoost算法的基本分类器设置为二类分类树即可。  回归问题的提升树：3.1初始化。3.2计算残差拟合回归树，更新3.3得到回归问题提升树**。  4. 梯度提升：** 提升树利用加法模型与前向分布算法实现学习的优化过程。当损失函数是平方损失和指数函数时候，每一步优化很简单。但是一般损失函数每一步优化不容易：提出梯度提升算法，利用损失函数的负梯度作为回归问题提升树算法中残差近似值，拟合一个回归树。 | ![157](2.统计学习方法/157.png)![158](2.统计学习方法/158.png)![159](2.统计学习方法/159.png)![160](2.统计学习方法/160.png)![161](2.统计学习方法/161.png)![162](2.统计学习方法/162.png) ![163](2.统计学习方法/163.png) |
|            | **第8章总结**           |                                          | ![164](2.统计学习方法/164.png)![165](2.统计学习方法/165.png) |
|            | **第9章开始：EM算法及其推广**  |                                          |                                          |
|            |                     | **1.EM：迭代算法：**用于含有隐变量的概率模型参数的极大似然估计／极大后验概率估计。  E求期望。M求极大。  如果概率模型的变量都是观测变量。那么给定数据，可以直接极大似然估计／贝叶斯估计模型参数。当有隐含变量:EM算法。  EM算法首先选取参数的初始值，然后通过下面步骤计算参数估计值直到收敛： E:计算模型参数pi、p、q下观测数据y来自硬币B的概率：M：计算模型参数的新估计值。 EM算法通过迭代求L的极大似然估计概率。 **EM算法：** 1.1选择初值,开始迭代 ；1.2 E：在下一次迭代的E步，计算Q；1.3 M：使得Q最大的theta.；1.4 重复23直到收敛。    **Q函数**：完全数据的对数似然函数关于在给定观测数据Y和当前参数下对未观测数据z的条件概率分布的期望。  **2.EM算法在非监督学习中的应用** EM算法可用于生成模型的非监督学习，生成模型由联合概率分布P(x,y)表示，可认为非监督学习训练数据是联合概率分布产生的数据。x为观测数据，y为未观测数据。 | ![165](2.统计学习方法/165.png)![166](2.统计学习方法/166.png)![167](2.统计学习方法/167.png)![168](2.统计学习方法/168.png)![169](2.统计学习方法/169.png) ![170](2.统计学习方法/170.png)![171](2.统计学习方法/171.png) |
|            | **第9章总结**           |                                          | ![172](2.统计学习方法/172.png) ![173](2.统计学习方法/173.png) |
|            | **第10章开始：隐马尔可夫模型**  |                                          |                                          |
|            |                     | **1.隐马尔可夫模型：**可用于标注的统计学模型：由隐藏的马尔可夫链随机生成观测序列的过程；属于生成模型。 是关于时序的概率模型。由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生随机序列的过程。隐藏的马尔可夫链随机生成的状态的序列为状态序列；每个状态生成一个观测，而由此产生的观测随机序列称为观测序列；序列的每一个位置又看作一个时刻。  隐马尔可夫模型由初始概率向量pi、状态转移概率矩阵A、观测概率概率矩阵B；  pi和A决定状态序列。 B决定观测序列。   pi和A确定了隐藏的马尔可夫链，生成不可观测的状态序列。 B确定了如何从状态生成观测，与状态序列综合确定了如何产生观测序列。   隐马尔可夫模型可用于标注；状态对应着标记。 标注问题是给定观测的序列预测其对应的标记序列。  可以假设标注问题的数据是由隐马尔可夫模型生成的，这样就可以利用隐马尔可夫模型的学习与预测算法标注。**2. 观测序列的生成过程：** 2.1按照初始化状态pi产生状态i.； 2.2按照状态i的观测概率分布b生成o；2.3 按照 状态的状态转移概率分布a产生状态i+1。 2.4t<T则2.2。否则终止。 **3.隐马尔可夫三个基本问题**：概率计算问题：给定模型和观测序列计算模型下观测序列出现的问题  。学习问题：已知观测序列，估计模型参数，使得模型下观测序列概率最大：用极大似然估计方法估计参数。 预测问题：解码问题：已知模型，观测序列：求对给定观测序列条件概率最大的状态序列。即给定观测序列，求最有可能对应的状态序列。 | ![174](2.统计学习方法/174.png)![175](2.统计学习方法/175.png)![176](2.统计学习方法/176.png)![177](2.统计学习方法/177.png) |
|            |                     | **1.概率计算算法：计算观测序列概率** 直接计算：给定模型，和观测序列，计算观测序列出现的概率：具体是按照概率公式直接计算：通过列举所有可能长度为T的状态，求各个状态序列I与观测序列的联合概率，然后对所有可能的概率序列求和得到条件概率。 **2.前向算法：**给定隐马尔可夫模型，定义**到时刻t部分**观测序列为o，且状态为q的概率为前向概率。可以递推求得前向概率a及观测序列概率 ：前向算法：2.1.初值；2.2递推 2.3终止。**3.后向算法：**：给定隐马尔可夫模型，定义**t-T**部分观测序列为，且状态为q的概率为前向概率。3.1.初值；3.2递推 3.3终止。 | ![178](2.统计学习方法/178.png)![179](2.统计学习方法/179.png)![180](2.统计学习方法/180.png)![181](2.统计学习方法/181.png)![182](2.统计学习方法/182.png)![183](2.统计学习方法/183.png)![184](2.统计学习方法/184.png) |
|            |                     | **1.隐马尔可夫模型的学习：**根据训练数据包括观测序列和对应状态序列还是只有观测序列分为：监督学习／非监督学习。**2.监督学习：** 可用极大似然估计来估计隐马尔可夫模型参数：2.1转移概率a的估计；2.2观测概率b的估计；2.3初始状态概率pi的估计。 **3.Baum-Welch算法** ：假定只有S个长度为T的观测序列，没有对应状态序列。目标是学习隐马尔可夫模型的参数；则是一个含有隐变量的概率模型：由EM算法实现 | ![185](2.统计学习方法/185.png)![186](2.统计学习方法/186.png)![187](2.统计学习方法/187.png)![188](2.统计学习方法/188.png)![189](2.统计学习方法/189.png) |
|            |                     | **1.预测算法：**近似算法／维特比算法。**近似算法：**每个时刻t选择在该时刻最有可能出现的状态i,从而得到一个状态序列I，将它作为预测的结果： 给定隐马尔可夫模型和观测序列。在t时刻处于状态q的概率r：在每一时刻t最有可能的状态i：从而得到状态序列 I。优点计算简单，缺点不能保证预测的状态序列整体最有可能。**维特比算法** 用动态规划解隐马尔可夫模型预测；即用动态规划求概率最大路径(最优路径)；这时一条路径对应一个状态序列。根据动态规划原理，最优路径的特性：如果最优路径在时刻t通过节点i,那么这一路径从节点到终点的部分路径对所有可能路径来说是必须是最优的。导入两个变量eta和w。定义时刻t状态为i的所有单个路径中概率最大值为eta.得到eta+1的递推公式。定义时刻t状态i的所有单个路径中概率最大路径第t-1个节点为w。  算法1.1 初始化eta/w ；1.2递推 ；1.3终止；1.4最优路径回溯 | ![190](2.统计学习方法/190.png) ![191](2.统计学习方法/191.png) ![192](2.统计学习方法/192.png) ![193](2.统计学习方法/193.png) |
|            | **第10章总结**          |                                          | ![194](2.统计学习方法/194.png)![195](2.统计学习方法/195.png) |
|            | **第11章开始：条件随机场**    |                                          |                                          |
|            |                     | **1.条件随机场：**给定一组输入随机变量条件下，另一组输出随机变量的条件概率分布模型。特点：假设输出随机变量构成马尔可夫随机场。   线性链条件随机场：问题变为由输入序列对输出序列预测的判别模型，对数线性模型。 学习方法：极大似然估计或正则化的极大似然估计。  **2概率无向图模型**： 马尔可夫随机场：可以由无向图表示的联合概率分布。  图：节点和边组成的集合G=(V,E)。  概率图模型：图表示的概率分布。设联合概率分布P(Y)： 由无向图表示概率分布P(Y). 即在图G中，节点v表示随机变量Y,边e表示随机变量之间的概率依赖关系。 给定一个联合概率分布P(Y)和表示它的无向图G。首先定义无向图表示的随机变量之间存在的**成对马尔可夫性、局部马尔可夫性、全局马尔可夫性。**  成对马尔可夫性：设u\v是无向图G中任意两个没有边连接的节点。节点u.v分别对应随机变量yu,yv。其他所有节点为O,对应随机变量组Yo.成对马尔可夫性： 给定随机变量组Yo的条件下随机变量yuyv是条件独立的。  局部马尔可夫性：v是无向图G的节点。w是与v有边连接的所有节点。O是v\w以外所有节点。 局部马尔可夫性：给定随机变量yw条件下，随机变量YuYo是独立的。 全局马尔可夫性：设节点集合AB在无向图G中被节点集合C分开的任意节点集合。全局马尔可夫性：制定随机变量组Yc下随机变量YaYb是条件独立。**概率无向图模型** ：设连个概率分布P(Y) 由无向图G = (V,E) 表示，在图G中，节点表示随机变量，边表示随机变量之间的依赖关系。如果P(Y)满足成对、局部、全局马尔可夫性，就称此联合概率分布为概率无向图模型或马尔可夫随机场。 **3.概率无向图模型的因子分解：** 团、最大团； 将概率无向图模型的联合概率分布表示为其最大团上的随机变量的函数的乘积：因子分解。**4.条件随机场的定义与形式：**条件随机场：给定随机变量X,随机变量Y的马尔可夫随机场。 线性链条件随机场可用于标注等问题：条件概率P(Y\|X)中，Y是输出变量，表示标记序列／状态序列。X是输入变量，表示需要标注的观测序列。 学习时，利用训练数据通过极大似然估计得到条件概率。预测时，对给定输入，得到条件概率最大的输出序列y。**条件随机场** 设XY是随机变量；P是给定X条件下条件噶绿分布Y构成一个无向图表示的马尔可夫随机场，则条件概率分布为条件随机场。 **线性链条件随机场**： XY均为线性链表示的随机变量序列；在x条件下，y的条件概率分布构成条件随机场则为线性链条件随机场；对数线性模型 **5.条件随机场的概率计算问题** ：条件随机场的额概率计算法问题是给定条件随机场P，输入序列x输出序列y。计算条件概率p及相应数学期望。**前向后向算法：** **条件随机场的学习算法**：给定训练数据集估计条件随机场 模型参数问题。：改进迭代尺度法；你牛顿法 **条件随机场的预测算法：**：**维特比算法：** | ![196](2.统计学习方法/196.png)![197](2.统计学习方法/197.png)![198](2.统计学习方法/198.png)![199](2.统计学习方法/199.png)![200](2.统计学习方法/200.png)![201](2.统计学习方法/201.png)![202](2.统计学习方法/202.png)![203](2.统计学习方法/203.png)![204](2.统计学习方法/204.png)![205](2.统计学习方法/205.png)![206](2.统计学习方法/206.png)![207](2.统计学习方法/207.png)![208](2.统计学习方法/208.png)![209](2.统计学习方法/209.png)![210](2.统计学习方法/210.png)![211](2.统计学习方法/211.png)![212](2.统计学习方法/212.png)![213](2.统计学习方法/213.png) |
|            | **第11章总结**          |                                          | ![214](2.统计学习方法/214.png)![215](2.统计学习方法/215.png) |
|            | **第12章总结**          |                                          | ![216](2.统计学习方法/216.png)![217](2.统计学习方法/217.png)![218](2.统计学习方法/218.png)![219](2.统计学习方法/219.png)![220](2.统计学习方法/220.png) |